{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.379982\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Потому , что суммируется в знаменателе по десяти элементам, каждый элемент колеблется вокруг единицы, и в числителе один такой элемент*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.811954 analytic: 2.811954, relative error: 2.233023e-08\n",
      "numerical: 0.496897 analytic: 0.496897, relative error: 4.743155e-08\n",
      "numerical: -0.382682 analytic: -0.382682, relative error: 1.506414e-07\n",
      "numerical: 0.173486 analytic: 0.173486, relative error: 2.653206e-07\n",
      "numerical: -0.169956 analytic: -0.169956, relative error: 1.527938e-07\n",
      "numerical: 1.275406 analytic: 1.275406, relative error: 1.666857e-08\n",
      "numerical: 1.675813 analytic: 1.675813, relative error: 5.345433e-08\n",
      "numerical: -0.933433 analytic: -0.933434, relative error: 5.256180e-08\n",
      "numerical: -1.683991 analytic: -1.683991, relative error: 2.754301e-08\n",
      "numerical: -2.585578 analytic: -2.585578, relative error: 1.665419e-08\n",
      "numerical: -0.909095 analytic: -0.909095, relative error: 2.053602e-08\n",
      "numerical: -2.982040 analytic: -2.982040, relative error: 8.630636e-09\n",
      "numerical: -0.735915 analytic: -0.735915, relative error: 3.322682e-08\n",
      "numerical: 1.926708 analytic: 1.926708, relative error: 2.002045e-08\n",
      "numerical: 0.422399 analytic: 0.422399, relative error: 1.339482e-07\n",
      "numerical: 1.937762 analytic: 1.937762, relative error: 2.276890e-08\n",
      "numerical: 1.506623 analytic: 1.506623, relative error: 3.435116e-08\n",
      "numerical: 0.018067 analytic: 0.018067, relative error: 1.072295e-06\n",
      "numerical: -0.742579 analytic: -0.742579, relative error: 2.850503e-08\n",
      "numerical: 2.526563 analytic: 2.526563, relative error: 2.406686e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.379982e+00 computed in 0.098512s\n",
      "vectorized loss: 2.379982e+00 computed in 0.051423s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 383.340834\n",
      "iteration 100 / 1500: loss 231.623893\n",
      "iteration 200 / 1500: loss 140.657698\n",
      "iteration 300 / 1500: loss 85.837244\n",
      "iteration 400 / 1500: loss 52.706482\n",
      "iteration 500 / 1500: loss 32.608224\n",
      "iteration 600 / 1500: loss 20.439354\n",
      "iteration 700 / 1500: loss 13.292053\n",
      "iteration 800 / 1500: loss 8.822213\n",
      "iteration 900 / 1500: loss 6.105501\n",
      "iteration 1000 / 1500: loss 4.477316\n",
      "iteration 1100 / 1500: loss 3.536708\n",
      "iteration 1200 / 1500: loss 2.888719\n",
      "iteration 1300 / 1500: loss 2.562437\n",
      "iteration 1400 / 1500: loss 2.279374\n",
      "That took 17.099160s\n",
      "iteration 0 / 1500: loss 435.676449\n",
      "iteration 100 / 1500: loss 249.046948\n",
      "iteration 200 / 1500: loss 143.106183\n",
      "iteration 300 / 1500: loss 82.862984\n",
      "iteration 400 / 1500: loss 48.191392\n",
      "iteration 500 / 1500: loss 28.487827\n",
      "iteration 600 / 1500: loss 17.160268\n",
      "iteration 700 / 1500: loss 10.658470\n",
      "iteration 800 / 1500: loss 7.024320\n",
      "iteration 900 / 1500: loss 4.827428\n",
      "iteration 1000 / 1500: loss 3.595412\n",
      "iteration 1100 / 1500: loss 2.921513\n",
      "iteration 1200 / 1500: loss 2.569899\n",
      "iteration 1300 / 1500: loss 2.358801\n",
      "iteration 1400 / 1500: loss 2.162859\n",
      "That took 16.947654s\n",
      "iteration 0 / 1500: loss 478.142999\n",
      "iteration 100 / 1500: loss 258.173017\n",
      "iteration 200 / 1500: loss 140.623853\n",
      "iteration 300 / 1500: loss 77.003906\n",
      "iteration 400 / 1500: loss 42.501369\n",
      "iteration 500 / 1500: loss 24.016893\n",
      "iteration 600 / 1500: loss 13.905511\n",
      "iteration 700 / 1500: loss 8.454204\n",
      "iteration 800 / 1500: loss 5.517885\n",
      "iteration 900 / 1500: loss 3.926904\n",
      "iteration 1000 / 1500: loss 3.038597\n",
      "iteration 1100 / 1500: loss 2.702919\n",
      "iteration 1200 / 1500: loss 2.317647\n",
      "iteration 1300 / 1500: loss 2.213544\n",
      "iteration 1400 / 1500: loss 2.144869\n",
      "That took 17.143801s\n",
      "iteration 0 / 1500: loss 516.274550\n",
      "iteration 100 / 1500: loss 264.294707\n",
      "iteration 200 / 1500: loss 136.206363\n",
      "iteration 300 / 1500: loss 70.576607\n",
      "iteration 400 / 1500: loss 37.055757\n",
      "iteration 500 / 1500: loss 19.960725\n",
      "iteration 600 / 1500: loss 11.238829\n",
      "iteration 700 / 1500: loss 6.774906\n",
      "iteration 800 / 1500: loss 4.473293\n",
      "iteration 900 / 1500: loss 3.153484\n",
      "iteration 1000 / 1500: loss 2.642714\n",
      "iteration 1100 / 1500: loss 2.348693\n",
      "iteration 1200 / 1500: loss 2.207842\n",
      "iteration 1300 / 1500: loss 2.133835\n",
      "iteration 1400 / 1500: loss 2.100435\n",
      "That took 17.345200s\n",
      "iteration 0 / 1500: loss 562.734782\n",
      "iteration 100 / 1500: loss 272.139973\n",
      "iteration 200 / 1500: loss 132.805165\n",
      "iteration 300 / 1500: loss 65.173761\n",
      "iteration 400 / 1500: loss 32.606346\n",
      "iteration 500 / 1500: loss 16.809290\n",
      "iteration 600 / 1500: loss 9.203194\n",
      "iteration 700 / 1500: loss 5.472504\n",
      "iteration 800 / 1500: loss 3.763147\n",
      "iteration 900 / 1500: loss 2.879790\n",
      "iteration 1000 / 1500: loss 2.470398\n",
      "iteration 1100 / 1500: loss 2.229592\n",
      "iteration 1200 / 1500: loss 2.161174\n",
      "iteration 1300 / 1500: loss 2.115785\n",
      "iteration 1400 / 1500: loss 2.103020\n",
      "That took 17.110937s\n",
      "iteration 0 / 1500: loss 603.119314\n",
      "iteration 100 / 1500: loss 276.155813\n",
      "iteration 200 / 1500: loss 127.493073\n",
      "iteration 300 / 1500: loss 59.516610\n",
      "iteration 400 / 1500: loss 28.292143\n",
      "iteration 500 / 1500: loss 14.101713\n",
      "iteration 600 / 1500: loss 7.543334\n",
      "iteration 700 / 1500: loss 4.611103\n",
      "iteration 800 / 1500: loss 3.201005\n",
      "iteration 900 / 1500: loss 2.656547\n",
      "iteration 1000 / 1500: loss 2.340607\n",
      "iteration 1100 / 1500: loss 2.109413\n",
      "iteration 1200 / 1500: loss 2.125020\n",
      "iteration 1300 / 1500: loss 2.088523\n",
      "iteration 1400 / 1500: loss 2.107805\n",
      "That took 17.160463s\n",
      "iteration 0 / 1500: loss 647.890813\n",
      "iteration 100 / 1500: loss 281.115881\n",
      "iteration 200 / 1500: loss 122.747000\n",
      "iteration 300 / 1500: loss 54.263853\n",
      "iteration 400 / 1500: loss 24.718056\n",
      "iteration 500 / 1500: loss 11.768614\n",
      "iteration 600 / 1500: loss 6.263842\n",
      "iteration 700 / 1500: loss 3.931075\n",
      "iteration 800 / 1500: loss 2.854821\n",
      "iteration 900 / 1500: loss 2.457062\n",
      "iteration 1000 / 1500: loss 2.247034\n",
      "iteration 1100 / 1500: loss 2.155350\n",
      "iteration 1200 / 1500: loss 2.066351\n",
      "iteration 1300 / 1500: loss 2.070401\n",
      "iteration 1400 / 1500: loss 2.043913\n",
      "That took 17.107771s\n",
      "iteration 0 / 1500: loss 687.478099\n",
      "iteration 100 / 1500: loss 282.160706\n",
      "iteration 200 / 1500: loss 116.700199\n",
      "iteration 300 / 1500: loss 48.904702\n",
      "iteration 400 / 1500: loss 21.316284\n",
      "iteration 500 / 1500: loss 9.946295\n",
      "iteration 600 / 1500: loss 5.334285\n",
      "iteration 700 / 1500: loss 3.372396\n",
      "iteration 800 / 1500: loss 2.647830\n",
      "iteration 900 / 1500: loss 2.364226\n",
      "iteration 1000 / 1500: loss 2.135588\n",
      "iteration 1100 / 1500: loss 2.076999\n",
      "iteration 1200 / 1500: loss 2.044405\n",
      "iteration 1300 / 1500: loss 2.060312\n",
      "iteration 1400 / 1500: loss 2.066451\n",
      "That took 17.057921s\n",
      "iteration 0 / 1500: loss 732.983301\n",
      "iteration 100 / 1500: loss 284.463681\n",
      "iteration 200 / 1500: loss 111.384312\n",
      "iteration 300 / 1500: loss 44.299773\n",
      "iteration 400 / 1500: loss 18.440743\n",
      "iteration 500 / 1500: loss 8.428042\n",
      "iteration 600 / 1500: loss 4.541929\n",
      "iteration 700 / 1500: loss 2.996095\n",
      "iteration 800 / 1500: loss 2.458576\n",
      "iteration 900 / 1500: loss 2.173748\n",
      "iteration 1000 / 1500: loss 2.134068\n",
      "iteration 1100 / 1500: loss 2.078220\n",
      "iteration 1200 / 1500: loss 2.097224\n",
      "iteration 1300 / 1500: loss 2.090271\n",
      "iteration 1400 / 1500: loss 2.077976\n",
      "That took 17.232180s\n",
      "iteration 0 / 1500: loss 777.538842\n",
      "iteration 100 / 1500: loss 285.645421\n",
      "iteration 200 / 1500: loss 105.834074\n",
      "iteration 300 / 1500: loss 40.041271\n",
      "iteration 400 / 1500: loss 16.018518\n",
      "iteration 500 / 1500: loss 7.168527\n",
      "iteration 600 / 1500: loss 3.934384\n",
      "iteration 700 / 1500: loss 2.734653\n",
      "iteration 800 / 1500: loss 2.367181\n",
      "iteration 900 / 1500: loss 2.188511\n",
      "iteration 1000 / 1500: loss 2.161545\n",
      "iteration 1100 / 1500: loss 2.063187\n",
      "iteration 1200 / 1500: loss 2.066016\n",
      "iteration 1300 / 1500: loss 2.032386\n",
      "iteration 1400 / 1500: loss 2.061951\n",
      "That took 17.271658s\n",
      "iteration 0 / 1500: loss 392.037666\n",
      "iteration 100 / 1500: loss 189.334962\n",
      "iteration 200 / 1500: loss 92.549085\n",
      "iteration 300 / 1500: loss 45.775365\n",
      "iteration 400 / 1500: loss 23.221684\n",
      "iteration 500 / 1500: loss 12.287900\n",
      "iteration 600 / 1500: loss 6.985989\n",
      "iteration 700 / 1500: loss 4.469323\n",
      "iteration 800 / 1500: loss 3.089869\n",
      "iteration 900 / 1500: loss 2.575882\n",
      "iteration 1000 / 1500: loss 2.319484\n",
      "iteration 1100 / 1500: loss 2.180436\n",
      "iteration 1200 / 1500: loss 2.030966\n",
      "iteration 1300 / 1500: loss 2.077483\n",
      "iteration 1400 / 1500: loss 2.042238\n",
      "That took 17.103252s\n",
      "iteration 0 / 1500: loss 432.229189\n",
      "iteration 100 / 1500: loss 193.214041\n",
      "iteration 200 / 1500: loss 87.166566\n",
      "iteration 300 / 1500: loss 40.059378\n",
      "iteration 400 / 1500: loss 19.135124\n",
      "iteration 500 / 1500: loss 9.627654\n",
      "iteration 600 / 1500: loss 5.433446\n",
      "iteration 700 / 1500: loss 3.552579\n",
      "iteration 800 / 1500: loss 2.711528\n",
      "iteration 900 / 1500: loss 2.304505\n",
      "iteration 1000 / 1500: loss 2.126043\n",
      "iteration 1100 / 1500: loss 2.053831\n",
      "iteration 1200 / 1500: loss 2.080470\n",
      "iteration 1300 / 1500: loss 2.073647\n",
      "iteration 1400 / 1500: loss 2.050796\n",
      "That took 16.949343s\n",
      "iteration 0 / 1500: loss 471.912030\n",
      "iteration 100 / 1500: loss 194.609771\n",
      "iteration 200 / 1500: loss 81.347226\n",
      "iteration 300 / 1500: loss 34.649219\n",
      "iteration 400 / 1500: loss 15.452068\n",
      "iteration 500 / 1500: loss 7.567101\n",
      "iteration 600 / 1500: loss 4.348598\n",
      "iteration 700 / 1500: loss 3.014417\n",
      "iteration 800 / 1500: loss 2.384621\n",
      "iteration 900 / 1500: loss 2.222728\n",
      "iteration 1000 / 1500: loss 2.088524\n",
      "iteration 1100 / 1500: loss 2.052258\n",
      "iteration 1200 / 1500: loss 2.080791\n",
      "iteration 1300 / 1500: loss 2.029985\n",
      "iteration 1400 / 1500: loss 2.036566\n",
      "That took 17.022985s\n",
      "iteration 0 / 1500: loss 515.635587\n",
      "iteration 100 / 1500: loss 196.382945\n",
      "iteration 200 / 1500: loss 75.772402\n",
      "iteration 300 / 1500: loss 30.062142\n",
      "iteration 400 / 1500: loss 12.678094\n",
      "iteration 500 / 1500: loss 6.102060\n",
      "iteration 600 / 1500: loss 3.660716\n",
      "iteration 700 / 1500: loss 2.716367\n",
      "iteration 800 / 1500: loss 2.276238\n",
      "iteration 900 / 1500: loss 2.143408\n",
      "iteration 1000 / 1500: loss 2.097793\n",
      "iteration 1100 / 1500: loss 2.095845\n",
      "iteration 1200 / 1500: loss 1.955691\n",
      "iteration 1300 / 1500: loss 1.993426\n",
      "iteration 1400 / 1500: loss 1.980728\n",
      "That took 17.149136s\n",
      "iteration 0 / 1500: loss 564.657533\n",
      "iteration 100 / 1500: loss 198.232745\n",
      "iteration 200 / 1500: loss 70.617835\n",
      "iteration 300 / 1500: loss 26.055118\n",
      "iteration 400 / 1500: loss 10.488073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 1500: loss 5.057707\n",
      "iteration 600 / 1500: loss 3.082423\n",
      "iteration 700 / 1500: loss 2.503345\n",
      "iteration 800 / 1500: loss 2.183216\n",
      "iteration 900 / 1500: loss 2.125319\n",
      "iteration 1000 / 1500: loss 2.125485\n",
      "iteration 1100 / 1500: loss 2.029439\n",
      "iteration 1200 / 1500: loss 2.126679\n",
      "iteration 1300 / 1500: loss 2.113170\n",
      "iteration 1400 / 1500: loss 2.095711\n",
      "That took 18.225684s\n",
      "iteration 0 / 1500: loss 598.518101\n",
      "iteration 100 / 1500: loss 194.273260\n",
      "iteration 200 / 1500: loss 64.089222\n",
      "iteration 300 / 1500: loss 22.118435\n",
      "iteration 400 / 1500: loss 8.573558\n",
      "iteration 500 / 1500: loss 4.123740\n",
      "iteration 600 / 1500: loss 2.722610\n",
      "iteration 700 / 1500: loss 2.271272\n",
      "iteration 800 / 1500: loss 2.154826\n",
      "iteration 900 / 1500: loss 2.151730\n",
      "iteration 1000 / 1500: loss 2.091728\n",
      "iteration 1100 / 1500: loss 2.042166\n",
      "iteration 1200 / 1500: loss 2.079564\n",
      "iteration 1300 / 1500: loss 2.113911\n",
      "iteration 1400 / 1500: loss 2.030598\n",
      "That took 17.012388s\n",
      "iteration 0 / 1500: loss 641.728154\n",
      "iteration 100 / 1500: loss 191.976088\n",
      "iteration 200 / 1500: loss 58.608476\n",
      "iteration 300 / 1500: loss 18.915204\n",
      "iteration 400 / 1500: loss 7.098629\n",
      "iteration 500 / 1500: loss 3.490764\n",
      "iteration 600 / 1500: loss 2.516014\n",
      "iteration 700 / 1500: loss 2.211177\n",
      "iteration 800 / 1500: loss 2.133654\n",
      "iteration 900 / 1500: loss 2.048133\n",
      "iteration 1000 / 1500: loss 2.070029\n",
      "iteration 1100 / 1500: loss 2.113246\n",
      "iteration 1200 / 1500: loss 2.041387\n",
      "iteration 1300 / 1500: loss 2.024206\n",
      "iteration 1400 / 1500: loss 2.096524\n",
      "That took 17.069138s\n",
      "iteration 0 / 1500: loss 689.791345\n",
      "iteration 100 / 1500: loss 190.017799\n",
      "iteration 200 / 1500: loss 53.683477\n",
      "iteration 300 / 1500: loss 16.271434\n",
      "iteration 400 / 1500: loss 5.967833\n",
      "iteration 500 / 1500: loss 3.164748\n",
      "iteration 600 / 1500: loss 2.334109\n",
      "iteration 700 / 1500: loss 2.142078\n",
      "iteration 800 / 1500: loss 2.099194\n",
      "iteration 900 / 1500: loss 2.077489\n",
      "iteration 1000 / 1500: loss 2.107940\n",
      "iteration 1100 / 1500: loss 2.060860\n",
      "iteration 1200 / 1500: loss 2.095770\n",
      "iteration 1300 / 1500: loss 2.042398\n",
      "iteration 1400 / 1500: loss 2.044910\n",
      "That took 17.127431s\n",
      "iteration 0 / 1500: loss 744.254137\n",
      "iteration 100 / 1500: loss 189.683673\n",
      "iteration 200 / 1500: loss 49.589885\n",
      "iteration 300 / 1500: loss 14.159954\n",
      "iteration 400 / 1500: loss 5.130726\n",
      "iteration 500 / 1500: loss 2.848772\n",
      "iteration 600 / 1500: loss 2.302000\n",
      "iteration 700 / 1500: loss 2.124540\n",
      "iteration 800 / 1500: loss 2.048069\n",
      "iteration 900 / 1500: loss 2.040357\n",
      "iteration 1000 / 1500: loss 2.122522\n",
      "iteration 1100 / 1500: loss 2.042135\n",
      "iteration 1200 / 1500: loss 2.084480\n",
      "iteration 1300 / 1500: loss 2.032644\n",
      "iteration 1400 / 1500: loss 2.028966\n",
      "That took 17.157584s\n",
      "iteration 0 / 1500: loss 790.004761\n",
      "iteration 100 / 1500: loss 185.966269\n",
      "iteration 200 / 1500: loss 45.054181\n",
      "iteration 300 / 1500: loss 12.161157\n",
      "iteration 400 / 1500: loss 4.461208\n",
      "iteration 500 / 1500: loss 2.645571\n",
      "iteration 600 / 1500: loss 2.221988\n",
      "iteration 700 / 1500: loss 2.025686\n",
      "iteration 800 / 1500: loss 2.127312\n",
      "iteration 900 / 1500: loss 2.060533\n",
      "iteration 1000 / 1500: loss 2.124482\n",
      "iteration 1100 / 1500: loss 2.031198\n",
      "iteration 1200 / 1500: loss 2.086226\n",
      "iteration 1300 / 1500: loss 2.133735\n",
      "iteration 1400 / 1500: loss 2.137474\n",
      "That took 16.900201s\n",
      "iteration 0 / 1500: loss 388.605194\n",
      "iteration 100 / 1500: loss 150.726730\n",
      "iteration 200 / 1500: loss 59.399159\n",
      "iteration 300 / 1500: loss 24.199530\n",
      "iteration 400 / 1500: loss 10.650193\n",
      "iteration 500 / 1500: loss 5.431945\n",
      "iteration 600 / 1500: loss 3.288143\n",
      "iteration 700 / 1500: loss 2.579622\n",
      "iteration 800 / 1500: loss 2.228403\n",
      "iteration 900 / 1500: loss 2.110029\n",
      "iteration 1000 / 1500: loss 2.045620\n",
      "iteration 1100 / 1500: loss 2.119531\n",
      "iteration 1200 / 1500: loss 2.051559\n",
      "iteration 1300 / 1500: loss 2.129416\n",
      "iteration 1400 / 1500: loss 2.085413\n",
      "That took 16.996046s\n",
      "iteration 0 / 1500: loss 428.821529\n",
      "iteration 100 / 1500: loss 150.066841\n",
      "iteration 200 / 1500: loss 53.399921\n",
      "iteration 300 / 1500: loss 19.959217\n",
      "iteration 400 / 1500: loss 8.240577\n",
      "iteration 500 / 1500: loss 4.188823\n",
      "iteration 600 / 1500: loss 2.790141\n",
      "iteration 700 / 1500: loss 2.298909\n",
      "iteration 800 / 1500: loss 2.062901\n",
      "iteration 900 / 1500: loss 2.064153\n",
      "iteration 1000 / 1500: loss 2.015336\n",
      "iteration 1100 / 1500: loss 2.013731\n",
      "iteration 1200 / 1500: loss 2.065679\n",
      "iteration 1300 / 1500: loss 1.975827\n",
      "iteration 1400 / 1500: loss 2.173640\n",
      "That took 17.216834s\n",
      "iteration 0 / 1500: loss 472.143489\n",
      "iteration 100 / 1500: loss 148.457417\n",
      "iteration 200 / 1500: loss 47.834336\n",
      "iteration 300 / 1500: loss 16.439573\n",
      "iteration 400 / 1500: loss 6.491145\n",
      "iteration 500 / 1500: loss 3.471804\n",
      "iteration 600 / 1500: loss 2.417745\n",
      "iteration 700 / 1500: loss 2.145086\n",
      "iteration 800 / 1500: loss 2.135597\n",
      "iteration 900 / 1500: loss 2.032617\n",
      "iteration 1000 / 1500: loss 2.049158\n",
      "iteration 1100 / 1500: loss 1.963875\n",
      "iteration 1200 / 1500: loss 2.011954\n",
      "iteration 1300 / 1500: loss 2.020039\n",
      "iteration 1400 / 1500: loss 2.101060\n",
      "That took 17.081742s\n",
      "iteration 0 / 1500: loss 524.439572\n",
      "iteration 100 / 1500: loss 148.451324\n",
      "iteration 200 / 1500: loss 43.151409\n",
      "iteration 300 / 1500: loss 13.598246\n",
      "iteration 400 / 1500: loss 5.284353\n",
      "iteration 500 / 1500: loss 2.985520\n",
      "iteration 600 / 1500: loss 2.314795\n",
      "iteration 700 / 1500: loss 2.048760\n",
      "iteration 800 / 1500: loss 2.061659\n",
      "iteration 900 / 1500: loss 2.155803\n",
      "iteration 1000 / 1500: loss 2.094165\n",
      "iteration 1100 / 1500: loss 2.066344\n",
      "iteration 1200 / 1500: loss 2.048438\n",
      "iteration 1300 / 1500: loss 2.093154\n",
      "iteration 1400 / 1500: loss 2.032255\n",
      "That took 17.047201s\n",
      "iteration 0 / 1500: loss 558.357991\n",
      "iteration 100 / 1500: loss 142.600042\n",
      "iteration 200 / 1500: loss 37.602641\n",
      "iteration 300 / 1500: loss 11.024299\n",
      "iteration 400 / 1500: loss 4.326795\n",
      "iteration 500 / 1500: loss 2.578462\n",
      "iteration 600 / 1500: loss 2.234751\n",
      "iteration 700 / 1500: loss 2.019773\n",
      "iteration 800 / 1500: loss 2.042927\n",
      "iteration 900 / 1500: loss 2.063427\n",
      "iteration 1000 / 1500: loss 2.098345\n",
      "iteration 1100 / 1500: loss 2.006443\n",
      "iteration 1200 / 1500: loss 2.046842\n",
      "iteration 1300 / 1500: loss 2.106134\n",
      "iteration 1400 / 1500: loss 2.085345\n",
      "That took 16.933164s\n",
      "iteration 0 / 1500: loss 606.918386\n",
      "iteration 100 / 1500: loss 139.420759\n",
      "iteration 200 / 1500: loss 33.441089\n",
      "iteration 300 / 1500: loss 9.236269\n",
      "iteration 400 / 1500: loss 3.728955\n",
      "iteration 500 / 1500: loss 2.401852\n",
      "iteration 600 / 1500: loss 2.137741\n",
      "iteration 700 / 1500: loss 2.065982\n",
      "iteration 800 / 1500: loss 2.033317\n",
      "iteration 900 / 1500: loss 2.041778\n",
      "iteration 1000 / 1500: loss 2.123839\n",
      "iteration 1100 / 1500: loss 2.025843\n",
      "iteration 1200 / 1500: loss 2.108798\n",
      "iteration 1300 / 1500: loss 2.077527\n",
      "iteration 1400 / 1500: loss 2.078070\n",
      "That took 17.017786s\n",
      "iteration 0 / 1500: loss 647.121663\n",
      "iteration 100 / 1500: loss 133.325043\n",
      "iteration 200 / 1500: loss 28.960941\n",
      "iteration 300 / 1500: loss 7.611765\n",
      "iteration 400 / 1500: loss 3.255479\n",
      "iteration 500 / 1500: loss 2.386750\n",
      "iteration 600 / 1500: loss 2.175726\n",
      "iteration 700 / 1500: loss 2.119020\n",
      "iteration 800 / 1500: loss 2.065535\n",
      "iteration 900 / 1500: loss 2.128325\n",
      "iteration 1000 / 1500: loss 2.056950\n",
      "iteration 1100 / 1500: loss 2.127375\n",
      "iteration 1200 / 1500: loss 2.062685\n",
      "iteration 1300 / 1500: loss 2.098491\n",
      "iteration 1400 / 1500: loss 2.086580\n",
      "That took 17.136963s\n",
      "iteration 0 / 1500: loss 688.838899\n",
      "iteration 100 / 1500: loss 128.195112\n",
      "iteration 200 / 1500: loss 25.352121\n",
      "iteration 300 / 1500: loss 6.432724\n",
      "iteration 400 / 1500: loss 2.897260\n",
      "iteration 500 / 1500: loss 2.193768\n",
      "iteration 600 / 1500: loss 2.110844\n",
      "iteration 700 / 1500: loss 2.125780\n",
      "iteration 800 / 1500: loss 2.075159\n",
      "iteration 900 / 1500: loss 1.992626\n",
      "iteration 1000 / 1500: loss 2.056591\n",
      "iteration 1100 / 1500: loss 2.067934\n",
      "iteration 1200 / 1500: loss 2.027845\n",
      "iteration 1300 / 1500: loss 2.010692\n",
      "iteration 1400 / 1500: loss 2.052590\n",
      "That took 16.940122s\n",
      "iteration 0 / 1500: loss 732.056914\n",
      "iteration 100 / 1500: loss 122.704284\n",
      "iteration 200 / 1500: loss 22.133505\n",
      "iteration 300 / 1500: loss 5.401796\n",
      "iteration 400 / 1500: loss 2.677756\n",
      "iteration 500 / 1500: loss 2.149201\n",
      "iteration 600 / 1500: loss 2.120916\n",
      "iteration 700 / 1500: loss 2.097868\n",
      "iteration 800 / 1500: loss 2.097004\n",
      "iteration 900 / 1500: loss 2.092617\n",
      "iteration 1000 / 1500: loss 2.130178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 2.070176\n",
      "iteration 1200 / 1500: loss 2.063629\n",
      "iteration 1300 / 1500: loss 2.069647\n",
      "iteration 1400 / 1500: loss 2.081864\n",
      "That took 16.974370s\n",
      "iteration 0 / 1500: loss 771.007270\n",
      "iteration 100 / 1500: loss 116.330989\n",
      "iteration 200 / 1500: loss 19.104097\n",
      "iteration 300 / 1500: loss 4.657408\n",
      "iteration 400 / 1500: loss 2.470514\n",
      "iteration 500 / 1500: loss 2.158232\n",
      "iteration 600 / 1500: loss 2.111338\n",
      "iteration 700 / 1500: loss 2.099724\n",
      "iteration 800 / 1500: loss 2.053813\n",
      "iteration 900 / 1500: loss 2.148816\n",
      "iteration 1000 / 1500: loss 2.092416\n",
      "iteration 1100 / 1500: loss 2.051434\n",
      "iteration 1200 / 1500: loss 2.090977\n",
      "iteration 1300 / 1500: loss 2.074986\n",
      "iteration 1400 / 1500: loss 2.096535\n",
      "That took 16.999033s\n",
      "iteration 0 / 1500: loss 390.526696\n",
      "iteration 100 / 1500: loss 121.435167\n",
      "iteration 200 / 1500: loss 38.967236\n",
      "iteration 300 / 1500: loss 13.385917\n",
      "iteration 400 / 1500: loss 5.537419\n",
      "iteration 500 / 1500: loss 3.153137\n",
      "iteration 600 / 1500: loss 2.349604\n",
      "iteration 700 / 1500: loss 2.054312\n",
      "iteration 800 / 1500: loss 2.040837\n",
      "iteration 900 / 1500: loss 2.096066\n",
      "iteration 1000 / 1500: loss 2.041360\n",
      "iteration 1100 / 1500: loss 2.081564\n",
      "iteration 1200 / 1500: loss 2.006811\n",
      "iteration 1300 / 1500: loss 2.021612\n",
      "iteration 1400 / 1500: loss 2.017203\n",
      "That took 16.992393s\n",
      "iteration 0 / 1500: loss 436.341795\n",
      "iteration 100 / 1500: loss 119.271049\n",
      "iteration 200 / 1500: loss 33.745345\n",
      "iteration 300 / 1500: loss 10.573788\n",
      "iteration 400 / 1500: loss 4.392126\n",
      "iteration 500 / 1500: loss 2.628099\n",
      "iteration 600 / 1500: loss 2.257747\n",
      "iteration 700 / 1500: loss 2.135704\n",
      "iteration 800 / 1500: loss 2.038508\n",
      "iteration 900 / 1500: loss 2.058617\n",
      "iteration 1000 / 1500: loss 2.064410\n",
      "iteration 1100 / 1500: loss 2.027655\n",
      "iteration 1200 / 1500: loss 2.066426\n",
      "iteration 1300 / 1500: loss 2.058476\n",
      "iteration 1400 / 1500: loss 2.002784\n",
      "That took 17.129900s\n",
      "iteration 0 / 1500: loss 473.707714\n",
      "iteration 100 / 1500: loss 113.738330\n",
      "iteration 200 / 1500: loss 28.537083\n",
      "iteration 300 / 1500: loss 8.325778\n",
      "iteration 400 / 1500: loss 3.542846\n",
      "iteration 500 / 1500: loss 2.437308\n",
      "iteration 600 / 1500: loss 2.112281\n",
      "iteration 700 / 1500: loss 2.148698\n",
      "iteration 800 / 1500: loss 2.113180\n",
      "iteration 900 / 1500: loss 2.111160\n",
      "iteration 1000 / 1500: loss 2.063109\n",
      "iteration 1100 / 1500: loss 2.031872\n",
      "iteration 1200 / 1500: loss 2.087102\n",
      "iteration 1300 / 1500: loss 2.068766\n",
      "iteration 1400 / 1500: loss 1.994447\n",
      "That took 18.232510s\n",
      "iteration 0 / 1500: loss 524.612255\n",
      "iteration 100 / 1500: loss 110.717633\n",
      "iteration 200 / 1500: loss 24.730138\n",
      "iteration 300 / 1500: loss 6.773226\n",
      "iteration 400 / 1500: loss 2.995551\n",
      "iteration 500 / 1500: loss 2.258175\n",
      "iteration 600 / 1500: loss 2.128293\n",
      "iteration 700 / 1500: loss 2.063329\n",
      "iteration 800 / 1500: loss 2.056804\n",
      "iteration 900 / 1500: loss 2.025825\n",
      "iteration 1000 / 1500: loss 2.024935\n",
      "iteration 1100 / 1500: loss 2.019588\n",
      "iteration 1200 / 1500: loss 2.117714\n",
      "iteration 1300 / 1500: loss 2.077525\n",
      "iteration 1400 / 1500: loss 2.004349\n",
      "That took 17.193618s\n",
      "iteration 0 / 1500: loss 575.221111\n",
      "iteration 100 / 1500: loss 106.643613\n",
      "iteration 200 / 1500: loss 21.134806\n",
      "iteration 300 / 1500: loss 5.568384\n",
      "iteration 400 / 1500: loss 2.741581\n",
      "iteration 500 / 1500: loss 2.168222\n",
      "iteration 600 / 1500: loss 2.083447\n",
      "iteration 700 / 1500: loss 2.025321\n",
      "iteration 800 / 1500: loss 2.058999\n",
      "iteration 900 / 1500: loss 2.069558\n",
      "iteration 1000 / 1500: loss 2.042203\n",
      "iteration 1100 / 1500: loss 2.059429\n",
      "iteration 1200 / 1500: loss 2.065196\n",
      "iteration 1300 / 1500: loss 2.036525\n",
      "iteration 1400 / 1500: loss 2.096095\n",
      "That took 17.042109s\n",
      "iteration 0 / 1500: loss 608.834829\n",
      "iteration 100 / 1500: loss 99.109059\n",
      "iteration 200 / 1500: loss 17.551819\n",
      "iteration 300 / 1500: loss 4.531788\n",
      "iteration 400 / 1500: loss 2.465191\n",
      "iteration 500 / 1500: loss 2.116363\n",
      "iteration 600 / 1500: loss 2.060077\n",
      "iteration 700 / 1500: loss 2.012614\n",
      "iteration 800 / 1500: loss 2.042319\n",
      "iteration 900 / 1500: loss 2.020704\n",
      "iteration 1000 / 1500: loss 2.033471\n",
      "iteration 1100 / 1500: loss 2.102335\n",
      "iteration 1200 / 1500: loss 2.025995\n",
      "iteration 1300 / 1500: loss 2.079756\n",
      "iteration 1400 / 1500: loss 2.008189\n",
      "That took 17.059482s\n",
      "iteration 0 / 1500: loss 643.234220\n",
      "iteration 100 / 1500: loss 92.166209\n",
      "iteration 200 / 1500: loss 14.789300\n",
      "iteration 300 / 1500: loss 3.858436\n",
      "iteration 400 / 1500: loss 2.335284\n",
      "iteration 500 / 1500: loss 2.102291\n",
      "iteration 600 / 1500: loss 2.110477\n",
      "iteration 700 / 1500: loss 2.132583\n",
      "iteration 800 / 1500: loss 2.096186\n",
      "iteration 900 / 1500: loss 2.075821\n",
      "iteration 1000 / 1500: loss 2.009604\n",
      "iteration 1100 / 1500: loss 2.069619\n",
      "iteration 1200 / 1500: loss 2.109259\n",
      "iteration 1300 / 1500: loss 2.109634\n",
      "iteration 1400 / 1500: loss 2.043683\n",
      "That took 17.127792s\n",
      "iteration 0 / 1500: loss 687.273299\n",
      "iteration 100 / 1500: loss 86.645724\n",
      "iteration 200 / 1500: loss 12.584948\n",
      "iteration 300 / 1500: loss 3.375012\n",
      "iteration 400 / 1500: loss 2.216371\n",
      "iteration 500 / 1500: loss 2.038108\n",
      "iteration 600 / 1500: loss 2.100339\n",
      "iteration 700 / 1500: loss 2.081070\n",
      "iteration 800 / 1500: loss 1.982607\n",
      "iteration 900 / 1500: loss 2.025105\n",
      "iteration 1000 / 1500: loss 2.056084\n",
      "iteration 1100 / 1500: loss 2.085008\n",
      "iteration 1200 / 1500: loss 2.065915\n",
      "iteration 1300 / 1500: loss 2.100121\n",
      "iteration 1400 / 1500: loss 2.041923\n",
      "That took 17.127550s\n",
      "iteration 0 / 1500: loss 733.875728\n",
      "iteration 100 / 1500: loss 81.189813\n",
      "iteration 200 / 1500: loss 10.727039\n",
      "iteration 300 / 1500: loss 2.995944\n",
      "iteration 400 / 1500: loss 2.169579\n",
      "iteration 500 / 1500: loss 2.100148\n",
      "iteration 600 / 1500: loss 2.135086\n",
      "iteration 700 / 1500: loss 2.039165\n",
      "iteration 800 / 1500: loss 2.057031\n",
      "iteration 900 / 1500: loss 2.096958\n",
      "iteration 1000 / 1500: loss 2.075487\n",
      "iteration 1100 / 1500: loss 2.145405\n",
      "iteration 1200 / 1500: loss 2.112804\n",
      "iteration 1300 / 1500: loss 2.105453\n",
      "iteration 1400 / 1500: loss 2.135988\n",
      "That took 17.527988s\n",
      "iteration 0 / 1500: loss 770.371416\n",
      "iteration 100 / 1500: loss 75.068943\n",
      "iteration 200 / 1500: loss 8.934664\n",
      "iteration 300 / 1500: loss 2.714141\n",
      "iteration 400 / 1500: loss 2.169391\n",
      "iteration 500 / 1500: loss 2.102668\n",
      "iteration 600 / 1500: loss 2.081653\n",
      "iteration 700 / 1500: loss 2.037748\n",
      "iteration 800 / 1500: loss 2.001554\n",
      "iteration 900 / 1500: loss 2.121820\n",
      "iteration 1000 / 1500: loss 2.097689\n",
      "iteration 1100 / 1500: loss 2.094199\n",
      "iteration 1200 / 1500: loss 2.120424\n",
      "iteration 1300 / 1500: loss 2.107973\n",
      "iteration 1400 / 1500: loss 2.059077\n",
      "That took 17.435351s\n",
      "iteration 0 / 1500: loss 390.416576\n",
      "iteration 100 / 1500: loss 97.303404\n",
      "iteration 200 / 1500: loss 25.533250\n",
      "iteration 300 / 1500: loss 7.825236\n",
      "iteration 400 / 1500: loss 3.450515\n",
      "iteration 500 / 1500: loss 2.376406\n",
      "iteration 600 / 1500: loss 2.152746\n",
      "iteration 700 / 1500: loss 2.089275\n",
      "iteration 800 / 1500: loss 2.089835\n",
      "iteration 900 / 1500: loss 2.073849\n",
      "iteration 1000 / 1500: loss 2.062143\n",
      "iteration 1100 / 1500: loss 2.099002\n",
      "iteration 1200 / 1500: loss 1.998394\n",
      "iteration 1300 / 1500: loss 1.956897\n",
      "iteration 1400 / 1500: loss 2.008843\n",
      "That took 17.173220s\n",
      "iteration 0 / 1500: loss 439.612575\n",
      "iteration 100 / 1500: loss 94.361276\n",
      "iteration 200 / 1500: loss 21.432226\n",
      "iteration 300 / 1500: loss 6.238499\n",
      "iteration 400 / 1500: loss 2.856222\n",
      "iteration 500 / 1500: loss 2.287144\n",
      "iteration 600 / 1500: loss 2.135777\n",
      "iteration 700 / 1500: loss 2.017676\n",
      "iteration 800 / 1500: loss 2.099166\n",
      "iteration 900 / 1500: loss 2.040124\n",
      "iteration 1000 / 1500: loss 2.040628\n",
      "iteration 1100 / 1500: loss 2.097879\n",
      "iteration 1200 / 1500: loss 2.077080\n",
      "iteration 1300 / 1500: loss 2.049811\n",
      "iteration 1400 / 1500: loss 2.104976\n",
      "That took 16.906327s\n",
      "iteration 0 / 1500: loss 473.690737\n",
      "iteration 100 / 1500: loss 86.986935\n",
      "iteration 200 / 1500: loss 17.383794\n",
      "iteration 300 / 1500: loss 4.834820\n",
      "iteration 400 / 1500: loss 2.650163\n",
      "iteration 500 / 1500: loss 2.037055\n",
      "iteration 600 / 1500: loss 2.085838\n",
      "iteration 700 / 1500: loss 2.082799\n",
      "iteration 800 / 1500: loss 2.063529\n",
      "iteration 900 / 1500: loss 2.025231\n",
      "iteration 1000 / 1500: loss 2.015833\n",
      "iteration 1100 / 1500: loss 2.003977\n",
      "iteration 1200 / 1500: loss 2.046988\n",
      "iteration 1300 / 1500: loss 2.043125\n",
      "iteration 1400 / 1500: loss 2.036376\n",
      "That took 17.186200s\n",
      "iteration 0 / 1500: loss 512.718226\n",
      "iteration 100 / 1500: loss 80.485518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 14.182925\n",
      "iteration 300 / 1500: loss 3.947084\n",
      "iteration 400 / 1500: loss 2.370492\n",
      "iteration 500 / 1500: loss 2.023779\n",
      "iteration 600 / 1500: loss 2.117659\n",
      "iteration 700 / 1500: loss 1.996512\n",
      "iteration 800 / 1500: loss 2.105200\n",
      "iteration 900 / 1500: loss 2.150687\n",
      "iteration 1000 / 1500: loss 2.001264\n",
      "iteration 1100 / 1500: loss 2.056913\n",
      "iteration 1200 / 1500: loss 2.081047\n",
      "iteration 1300 / 1500: loss 2.109383\n",
      "iteration 1400 / 1500: loss 2.071281\n",
      "That took 17.267890s\n",
      "iteration 0 / 1500: loss 556.730467\n",
      "iteration 100 / 1500: loss 75.123505\n",
      "iteration 200 / 1500: loss 11.731708\n",
      "iteration 300 / 1500: loss 3.320287\n",
      "iteration 400 / 1500: loss 2.221894\n",
      "iteration 500 / 1500: loss 2.053317\n",
      "iteration 600 / 1500: loss 2.088516\n",
      "iteration 700 / 1500: loss 2.065710\n",
      "iteration 800 / 1500: loss 2.058120\n",
      "iteration 900 / 1500: loss 2.048289\n",
      "iteration 1000 / 1500: loss 2.089480\n",
      "iteration 1100 / 1500: loss 2.082444\n",
      "iteration 1200 / 1500: loss 2.105912\n",
      "iteration 1300 / 1500: loss 2.052755\n",
      "iteration 1400 / 1500: loss 2.066239\n",
      "That took 17.179241s\n",
      "iteration 0 / 1500: loss 605.849628\n",
      "iteration 100 / 1500: loss 69.929862\n",
      "iteration 200 / 1500: loss 9.730638\n",
      "iteration 300 / 1500: loss 2.900176\n",
      "iteration 400 / 1500: loss 2.148117\n",
      "iteration 500 / 1500: loss 2.055016\n",
      "iteration 600 / 1500: loss 2.018259\n",
      "iteration 700 / 1500: loss 2.005170\n",
      "iteration 800 / 1500: loss 2.049550\n",
      "iteration 900 / 1500: loss 2.053074\n",
      "iteration 1000 / 1500: loss 2.010468\n",
      "iteration 1100 / 1500: loss 2.101961\n",
      "iteration 1200 / 1500: loss 2.078826\n",
      "iteration 1300 / 1500: loss 2.058017\n",
      "iteration 1400 / 1500: loss 2.056132\n",
      "That took 17.338230s\n",
      "iteration 0 / 1500: loss 644.518707\n",
      "iteration 100 / 1500: loss 64.059714\n",
      "iteration 200 / 1500: loss 8.115297\n",
      "iteration 300 / 1500: loss 2.617910\n",
      "iteration 400 / 1500: loss 2.101295\n",
      "iteration 500 / 1500: loss 2.091941\n",
      "iteration 600 / 1500: loss 2.088108\n",
      "iteration 700 / 1500: loss 2.024431\n",
      "iteration 800 / 1500: loss 2.064822\n",
      "iteration 900 / 1500: loss 2.081347\n",
      "iteration 1000 / 1500: loss 2.065809\n",
      "iteration 1100 / 1500: loss 2.091706\n",
      "iteration 1200 / 1500: loss 2.121327\n",
      "iteration 1300 / 1500: loss 2.047928\n",
      "iteration 1400 / 1500: loss 2.105068\n",
      "That took 17.164308s\n",
      "iteration 0 / 1500: loss 681.295178\n",
      "iteration 100 / 1500: loss 58.131319\n",
      "iteration 200 / 1500: loss 6.671359\n",
      "iteration 300 / 1500: loss 2.491427\n",
      "iteration 400 / 1500: loss 2.072721\n",
      "iteration 500 / 1500: loss 2.079358\n",
      "iteration 600 / 1500: loss 2.080304\n",
      "iteration 700 / 1500: loss 2.097220\n",
      "iteration 800 / 1500: loss 2.098578\n",
      "iteration 900 / 1500: loss 2.048685\n",
      "iteration 1000 / 1500: loss 2.057500\n",
      "iteration 1100 / 1500: loss 2.013951\n",
      "iteration 1200 / 1500: loss 2.101362\n",
      "iteration 1300 / 1500: loss 2.074108\n",
      "iteration 1400 / 1500: loss 2.056816\n",
      "That took 17.157287s\n",
      "iteration 0 / 1500: loss 735.887406\n",
      "iteration 100 / 1500: loss 53.883234\n",
      "iteration 200 / 1500: loss 5.668875\n",
      "iteration 300 / 1500: loss 2.317499\n",
      "iteration 400 / 1500: loss 2.089576\n",
      "iteration 500 / 1500: loss 2.108388\n",
      "iteration 600 / 1500: loss 2.146619\n",
      "iteration 700 / 1500: loss 2.107050\n",
      "iteration 800 / 1500: loss 2.105058\n",
      "iteration 900 / 1500: loss 2.062987\n",
      "iteration 1000 / 1500: loss 2.063833\n",
      "iteration 1100 / 1500: loss 2.113235\n",
      "iteration 1200 / 1500: loss 2.052384\n",
      "iteration 1300 / 1500: loss 2.074487\n",
      "iteration 1400 / 1500: loss 2.092315\n",
      "That took 16.995709s\n",
      "iteration 0 / 1500: loss 778.912571\n",
      "iteration 100 / 1500: loss 49.012568\n",
      "iteration 200 / 1500: loss 4.895358\n",
      "iteration 300 / 1500: loss 2.243495\n",
      "iteration 400 / 1500: loss 2.092333\n",
      "iteration 500 / 1500: loss 2.029273\n",
      "iteration 600 / 1500: loss 2.107934\n",
      "iteration 700 / 1500: loss 2.096097\n",
      "iteration 800 / 1500: loss 2.033708\n",
      "iteration 900 / 1500: loss 2.104836\n",
      "iteration 1000 / 1500: loss 2.039093\n",
      "iteration 1100 / 1500: loss 2.143541\n",
      "iteration 1200 / 1500: loss 2.112621\n",
      "iteration 1300 / 1500: loss 2.068916\n",
      "iteration 1400 / 1500: loss 2.062616\n",
      "That took 17.217707s\n",
      "iteration 0 / 1500: loss 388.560898\n",
      "iteration 100 / 1500: loss 77.570079\n",
      "iteration 200 / 1500: loss 16.905500\n",
      "iteration 300 / 1500: loss 4.982721\n",
      "iteration 400 / 1500: loss 2.595667\n",
      "iteration 500 / 1500: loss 2.159726\n",
      "iteration 600 / 1500: loss 2.046282\n",
      "iteration 700 / 1500: loss 2.062682\n",
      "iteration 800 / 1500: loss 2.050354\n",
      "iteration 900 / 1500: loss 1.970096\n",
      "iteration 1000 / 1500: loss 2.120965\n",
      "iteration 1100 / 1500: loss 2.013874\n",
      "iteration 1200 / 1500: loss 2.080641\n",
      "iteration 1300 / 1500: loss 2.075337\n",
      "iteration 1400 / 1500: loss 1.978548\n",
      "That took 18.425131s\n",
      "iteration 0 / 1500: loss 431.678266\n",
      "iteration 100 / 1500: loss 72.226509\n",
      "iteration 200 / 1500: loss 13.587997\n",
      "iteration 300 / 1500: loss 3.911417\n",
      "iteration 400 / 1500: loss 2.329158\n",
      "iteration 500 / 1500: loss 2.001005\n",
      "iteration 600 / 1500: loss 2.096686\n",
      "iteration 700 / 1500: loss 2.013059\n",
      "iteration 800 / 1500: loss 1.980673\n",
      "iteration 900 / 1500: loss 2.029435\n",
      "iteration 1000 / 1500: loss 2.030903\n",
      "iteration 1100 / 1500: loss 2.066053\n",
      "iteration 1200 / 1500: loss 2.035939\n",
      "iteration 1300 / 1500: loss 2.067748\n",
      "iteration 1400 / 1500: loss 2.043702\n",
      "That took 17.160118s\n",
      "iteration 0 / 1500: loss 479.469233\n",
      "iteration 100 / 1500: loss 67.429203\n",
      "iteration 200 / 1500: loss 11.044174\n",
      "iteration 300 / 1500: loss 3.259692\n",
      "iteration 400 / 1500: loss 2.294848\n",
      "iteration 500 / 1500: loss 2.084463\n",
      "iteration 600 / 1500: loss 2.020042\n",
      "iteration 700 / 1500: loss 2.085897\n",
      "iteration 800 / 1500: loss 1.985715\n",
      "iteration 900 / 1500: loss 2.086879\n",
      "iteration 1000 / 1500: loss 1.991457\n",
      "iteration 1100 / 1500: loss 2.042696\n",
      "iteration 1200 / 1500: loss 2.051045\n",
      "iteration 1300 / 1500: loss 2.040046\n",
      "iteration 1400 / 1500: loss 2.024724\n",
      "That took 17.098509s\n",
      "iteration 0 / 1500: loss 509.947555\n",
      "iteration 100 / 1500: loss 60.072707\n",
      "iteration 200 / 1500: loss 8.703525\n",
      "iteration 300 / 1500: loss 2.787365\n",
      "iteration 400 / 1500: loss 2.107524\n",
      "iteration 500 / 1500: loss 2.060711\n",
      "iteration 600 / 1500: loss 2.122979\n",
      "iteration 700 / 1500: loss 2.040252\n",
      "iteration 800 / 1500: loss 2.046719\n",
      "iteration 900 / 1500: loss 2.074308\n",
      "iteration 1000 / 1500: loss 2.060778\n",
      "iteration 1100 / 1500: loss 2.061649\n",
      "iteration 1200 / 1500: loss 2.049606\n",
      "iteration 1300 / 1500: loss 2.016143\n",
      "iteration 1400 / 1500: loss 2.051304\n",
      "That took 17.272361s\n",
      "iteration 0 / 1500: loss 552.651220\n",
      "iteration 100 / 1500: loss 54.446266\n",
      "iteration 200 / 1500: loss 7.010775\n",
      "iteration 300 / 1500: loss 2.527289\n",
      "iteration 400 / 1500: loss 2.080640\n",
      "iteration 500 / 1500: loss 2.064407\n",
      "iteration 600 / 1500: loss 2.079398\n",
      "iteration 700 / 1500: loss 2.054356\n",
      "iteration 800 / 1500: loss 2.046462\n",
      "iteration 900 / 1500: loss 2.059784\n",
      "iteration 1000 / 1500: loss 2.081435\n",
      "iteration 1100 / 1500: loss 2.104058\n",
      "iteration 1200 / 1500: loss 2.045434\n",
      "iteration 1300 / 1500: loss 2.069714\n",
      "iteration 1400 / 1500: loss 2.092574\n",
      "That took 16.921028s\n",
      "iteration 0 / 1500: loss 600.635293\n",
      "iteration 100 / 1500: loss 49.595365\n",
      "iteration 200 / 1500: loss 5.773490\n",
      "iteration 300 / 1500: loss 2.390700\n",
      "iteration 400 / 1500: loss 2.077905\n",
      "iteration 500 / 1500: loss 2.009549\n",
      "iteration 600 / 1500: loss 2.064407\n",
      "iteration 700 / 1500: loss 2.058655\n",
      "iteration 800 / 1500: loss 2.049886\n",
      "iteration 900 / 1500: loss 2.069287\n",
      "iteration 1000 / 1500: loss 2.079078\n",
      "iteration 1100 / 1500: loss 2.131234\n",
      "iteration 1200 / 1500: loss 2.077099\n",
      "iteration 1300 / 1500: loss 2.064350\n",
      "iteration 1400 / 1500: loss 2.083847\n",
      "That took 17.017200s\n",
      "iteration 0 / 1500: loss 643.635647\n",
      "iteration 100 / 1500: loss 44.478511\n",
      "iteration 200 / 1500: loss 4.882005\n",
      "iteration 300 / 1500: loss 2.259273\n",
      "iteration 400 / 1500: loss 2.013840\n",
      "iteration 500 / 1500: loss 2.056040\n",
      "iteration 600 / 1500: loss 2.036551\n",
      "iteration 700 / 1500: loss 2.122533\n",
      "iteration 800 / 1500: loss 2.053912\n",
      "iteration 900 / 1500: loss 2.063889\n",
      "iteration 1000 / 1500: loss 2.015271\n",
      "iteration 1100 / 1500: loss 2.092600\n",
      "iteration 1200 / 1500: loss 2.109012\n",
      "iteration 1300 / 1500: loss 2.078357\n",
      "iteration 1400 / 1500: loss 2.095270\n",
      "That took 17.187876s\n",
      "iteration 0 / 1500: loss 684.578808\n",
      "iteration 100 / 1500: loss 39.778658\n",
      "iteration 200 / 1500: loss 4.213404\n",
      "iteration 300 / 1500: loss 2.174526\n",
      "iteration 400 / 1500: loss 2.052517\n",
      "iteration 500 / 1500: loss 2.080632\n",
      "iteration 600 / 1500: loss 2.123123\n",
      "iteration 700 / 1500: loss 2.129183\n",
      "iteration 800 / 1500: loss 2.106810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 2.058490\n",
      "iteration 1000 / 1500: loss 2.054377\n",
      "iteration 1100 / 1500: loss 2.104375\n",
      "iteration 1200 / 1500: loss 2.058574\n",
      "iteration 1300 / 1500: loss 2.093199\n",
      "iteration 1400 / 1500: loss 2.062568\n",
      "That took 17.283249s\n",
      "iteration 0 / 1500: loss 717.905963\n",
      "iteration 100 / 1500: loss 35.070922\n",
      "iteration 200 / 1500: loss 3.625027\n",
      "iteration 300 / 1500: loss 2.179946\n",
      "iteration 400 / 1500: loss 2.114269\n",
      "iteration 500 / 1500: loss 2.082914\n",
      "iteration 600 / 1500: loss 2.056653\n",
      "iteration 700 / 1500: loss 2.116433\n",
      "iteration 800 / 1500: loss 2.192252\n",
      "iteration 900 / 1500: loss 2.105366\n",
      "iteration 1000 / 1500: loss 2.058825\n",
      "iteration 1100 / 1500: loss 2.057108\n",
      "iteration 1200 / 1500: loss 2.100446\n",
      "iteration 1300 / 1500: loss 2.117360\n",
      "iteration 1400 / 1500: loss 2.062221\n",
      "That took 17.152236s\n",
      "iteration 0 / 1500: loss 774.683325\n",
      "iteration 100 / 1500: loss 31.882568\n",
      "iteration 200 / 1500: loss 3.245534\n",
      "iteration 300 / 1500: loss 2.172437\n",
      "iteration 400 / 1500: loss 2.048672\n",
      "iteration 500 / 1500: loss 2.120048\n",
      "iteration 600 / 1500: loss 2.093718\n",
      "iteration 700 / 1500: loss 2.136126\n",
      "iteration 800 / 1500: loss 2.062874\n",
      "iteration 900 / 1500: loss 2.073570\n",
      "iteration 1000 / 1500: loss 2.076162\n",
      "iteration 1100 / 1500: loss 2.069041\n",
      "iteration 1200 / 1500: loss 2.069220\n",
      "iteration 1300 / 1500: loss 2.088649\n",
      "iteration 1400 / 1500: loss 2.063441\n",
      "That took 17.205478s\n",
      "iteration 0 / 1500: loss 388.774295\n",
      "iteration 100 / 1500: loss 62.572607\n",
      "iteration 200 / 1500: loss 11.593125\n",
      "iteration 300 / 1500: loss 3.526986\n",
      "iteration 400 / 1500: loss 2.331505\n",
      "iteration 500 / 1500: loss 2.098876\n",
      "iteration 600 / 1500: loss 2.047115\n",
      "iteration 700 / 1500: loss 2.008972\n",
      "iteration 800 / 1500: loss 2.050642\n",
      "iteration 900 / 1500: loss 2.066762\n",
      "iteration 1000 / 1500: loss 2.047632\n",
      "iteration 1100 / 1500: loss 1.995622\n",
      "iteration 1200 / 1500: loss 2.033152\n",
      "iteration 1300 / 1500: loss 2.024457\n",
      "iteration 1400 / 1500: loss 2.022397\n",
      "That took 17.298886s\n",
      "iteration 0 / 1500: loss 432.815494\n",
      "iteration 100 / 1500: loss 56.736037\n",
      "iteration 200 / 1500: loss 9.069029\n",
      "iteration 300 / 1500: loss 2.930766\n",
      "iteration 400 / 1500: loss 2.169948\n",
      "iteration 500 / 1500: loss 2.070632\n",
      "iteration 600 / 1500: loss 2.061838\n",
      "iteration 700 / 1500: loss 2.054475\n",
      "iteration 800 / 1500: loss 2.037566\n",
      "iteration 900 / 1500: loss 2.069662\n",
      "iteration 1000 / 1500: loss 2.015268\n",
      "iteration 1100 / 1500: loss 2.047938\n",
      "iteration 1200 / 1500: loss 2.017328\n",
      "iteration 1300 / 1500: loss 2.031126\n",
      "iteration 1400 / 1500: loss 2.009240\n",
      "That took 17.105018s\n",
      "iteration 0 / 1500: loss 476.349258\n",
      "iteration 100 / 1500: loss 51.156051\n",
      "iteration 200 / 1500: loss 7.139943\n",
      "iteration 300 / 1500: loss 2.628651\n",
      "iteration 400 / 1500: loss 2.154671\n",
      "iteration 500 / 1500: loss 2.084829\n",
      "iteration 600 / 1500: loss 2.057087\n",
      "iteration 700 / 1500: loss 2.047725\n",
      "iteration 800 / 1500: loss 2.044491\n",
      "iteration 900 / 1500: loss 2.088227\n",
      "iteration 1000 / 1500: loss 2.047658\n",
      "iteration 1100 / 1500: loss 2.017596\n",
      "iteration 1200 / 1500: loss 2.052995\n",
      "iteration 1300 / 1500: loss 2.007144\n",
      "iteration 1400 / 1500: loss 2.113588\n",
      "That took 17.126372s\n",
      "iteration 0 / 1500: loss 519.317668\n",
      "iteration 100 / 1500: loss 45.704289\n",
      "iteration 200 / 1500: loss 5.736296\n",
      "iteration 300 / 1500: loss 2.389896\n",
      "iteration 400 / 1500: loss 2.048508\n",
      "iteration 500 / 1500: loss 2.085985\n",
      "iteration 600 / 1500: loss 2.043942\n",
      "iteration 700 / 1500: loss 2.048385\n",
      "iteration 800 / 1500: loss 2.055654\n",
      "iteration 900 / 1500: loss 2.130271\n",
      "iteration 1000 / 1500: loss 2.055268\n",
      "iteration 1100 / 1500: loss 2.005937\n",
      "iteration 1200 / 1500: loss 2.042154\n",
      "iteration 1300 / 1500: loss 2.056826\n",
      "iteration 1400 / 1500: loss 2.070753\n",
      "That took 17.305415s\n",
      "iteration 0 / 1500: loss 557.314113\n",
      "iteration 100 / 1500: loss 40.005699\n",
      "iteration 200 / 1500: loss 4.703727\n",
      "iteration 300 / 1500: loss 2.225215\n",
      "iteration 400 / 1500: loss 2.110234\n",
      "iteration 500 / 1500: loss 2.004286\n",
      "iteration 600 / 1500: loss 2.051948\n",
      "iteration 700 / 1500: loss 2.032634\n",
      "iteration 800 / 1500: loss 2.035770\n",
      "iteration 900 / 1500: loss 2.059804\n",
      "iteration 1000 / 1500: loss 2.054719\n",
      "iteration 1100 / 1500: loss 2.027607\n",
      "iteration 1200 / 1500: loss 2.057137\n",
      "iteration 1300 / 1500: loss 2.068662\n",
      "iteration 1400 / 1500: loss 2.110741\n",
      "That took 17.423480s\n",
      "iteration 0 / 1500: loss 602.669514\n",
      "iteration 100 / 1500: loss 35.794991\n",
      "iteration 200 / 1500: loss 3.958181\n",
      "iteration 300 / 1500: loss 2.153247\n",
      "iteration 400 / 1500: loss 2.069906\n",
      "iteration 500 / 1500: loss 2.019160\n",
      "iteration 600 / 1500: loss 2.088686\n",
      "iteration 700 / 1500: loss 2.069481\n",
      "iteration 800 / 1500: loss 2.097964\n",
      "iteration 900 / 1500: loss 2.048791\n",
      "iteration 1000 / 1500: loss 2.071288\n",
      "iteration 1100 / 1500: loss 2.124265\n",
      "iteration 1200 / 1500: loss 2.043946\n",
      "iteration 1300 / 1500: loss 2.096537\n",
      "iteration 1400 / 1500: loss 2.086323\n",
      "That took 17.441611s\n",
      "iteration 0 / 1500: loss 648.460072\n",
      "iteration 100 / 1500: loss 31.348736\n",
      "iteration 200 / 1500: loss 3.456383\n",
      "iteration 300 / 1500: loss 2.105523\n",
      "iteration 400 / 1500: loss 1.995486\n",
      "iteration 500 / 1500: loss 2.063323\n",
      "iteration 600 / 1500: loss 2.046721\n",
      "iteration 700 / 1500: loss 2.066012\n",
      "iteration 800 / 1500: loss 2.098845\n",
      "iteration 900 / 1500: loss 2.067491\n",
      "iteration 1000 / 1500: loss 2.052146\n",
      "iteration 1100 / 1500: loss 2.077706\n",
      "iteration 1200 / 1500: loss 2.105897\n",
      "iteration 1300 / 1500: loss 2.110719\n",
      "iteration 1400 / 1500: loss 2.051257\n",
      "That took 17.319272s\n",
      "iteration 0 / 1500: loss 682.273300\n",
      "iteration 100 / 1500: loss 27.255325\n",
      "iteration 200 / 1500: loss 3.067713\n",
      "iteration 300 / 1500: loss 2.174953\n",
      "iteration 400 / 1500: loss 2.053396\n",
      "iteration 500 / 1500: loss 2.061555\n",
      "iteration 600 / 1500: loss 2.050321\n",
      "iteration 700 / 1500: loss 2.079005\n",
      "iteration 800 / 1500: loss 2.062010\n",
      "iteration 900 / 1500: loss 2.017658\n",
      "iteration 1000 / 1500: loss 2.058966\n",
      "iteration 1100 / 1500: loss 2.120047\n",
      "iteration 1200 / 1500: loss 2.084118\n",
      "iteration 1300 / 1500: loss 2.067821\n",
      "iteration 1400 / 1500: loss 2.096830\n",
      "That took 17.305976s\n",
      "iteration 0 / 1500: loss 734.835622\n",
      "iteration 100 / 1500: loss 24.014390\n",
      "iteration 200 / 1500: loss 2.755878\n",
      "iteration 300 / 1500: loss 2.160080\n",
      "iteration 400 / 1500: loss 2.082280\n",
      "iteration 500 / 1500: loss 2.115459\n",
      "iteration 600 / 1500: loss 2.108466\n",
      "iteration 700 / 1500: loss 2.091890\n",
      "iteration 800 / 1500: loss 2.083655\n",
      "iteration 900 / 1500: loss 2.110055\n",
      "iteration 1000 / 1500: loss 2.032679\n",
      "iteration 1100 / 1500: loss 2.122446\n",
      "iteration 1200 / 1500: loss 2.075512\n",
      "iteration 1300 / 1500: loss 2.162473\n",
      "iteration 1400 / 1500: loss 2.065388\n",
      "That took 18.359808s\n",
      "iteration 0 / 1500: loss 773.070469\n",
      "iteration 100 / 1500: loss 20.907919\n",
      "iteration 200 / 1500: loss 2.590598\n",
      "iteration 300 / 1500: loss 2.147281\n",
      "iteration 400 / 1500: loss 2.026369\n",
      "iteration 500 / 1500: loss 2.077516\n",
      "iteration 600 / 1500: loss 2.053856\n",
      "iteration 700 / 1500: loss 2.118030\n",
      "iteration 800 / 1500: loss 2.101222\n",
      "iteration 900 / 1500: loss 2.061865\n",
      "iteration 1000 / 1500: loss 2.068034\n",
      "iteration 1100 / 1500: loss 2.061841\n",
      "iteration 1200 / 1500: loss 2.112635\n",
      "iteration 1300 / 1500: loss 2.118881\n",
      "iteration 1400 / 1500: loss 2.092539\n",
      "That took 17.349749s\n",
      "iteration 0 / 1500: loss 386.637160\n",
      "iteration 100 / 1500: loss 49.889861\n",
      "iteration 200 / 1500: loss 8.039919\n",
      "iteration 300 / 1500: loss 2.779462\n",
      "iteration 400 / 1500: loss 2.032245\n",
      "iteration 500 / 1500: loss 2.060936\n",
      "iteration 600 / 1500: loss 2.030795\n",
      "iteration 700 / 1500: loss 2.009181\n",
      "iteration 800 / 1500: loss 2.061891\n",
      "iteration 900 / 1500: loss 2.026650\n",
      "iteration 1000 / 1500: loss 2.000707\n",
      "iteration 1100 / 1500: loss 2.027729\n",
      "iteration 1200 / 1500: loss 1.992132\n",
      "iteration 1300 / 1500: loss 2.002091\n",
      "iteration 1400 / 1500: loss 1.980484\n",
      "That took 17.223058s\n",
      "iteration 0 / 1500: loss 430.236167\n",
      "iteration 100 / 1500: loss 44.401220\n",
      "iteration 200 / 1500: loss 6.318503\n",
      "iteration 300 / 1500: loss 2.452734\n",
      "iteration 400 / 1500: loss 2.032143\n",
      "iteration 500 / 1500: loss 2.058319\n",
      "iteration 600 / 1500: loss 1.994249\n",
      "iteration 700 / 1500: loss 2.085075\n",
      "iteration 800 / 1500: loss 2.021996\n",
      "iteration 900 / 1500: loss 1.968229\n",
      "iteration 1000 / 1500: loss 2.034726\n",
      "iteration 1100 / 1500: loss 2.091249\n",
      "iteration 1200 / 1500: loss 2.084608\n",
      "iteration 1300 / 1500: loss 1.987046\n",
      "iteration 1400 / 1500: loss 2.001920\n",
      "That took 17.222758s\n",
      "iteration 0 / 1500: loss 473.309484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 39.086022\n",
      "iteration 200 / 1500: loss 4.961222\n",
      "iteration 300 / 1500: loss 2.361031\n",
      "iteration 400 / 1500: loss 2.065030\n",
      "iteration 500 / 1500: loss 2.083881\n",
      "iteration 600 / 1500: loss 2.045966\n",
      "iteration 700 / 1500: loss 1.966322\n",
      "iteration 800 / 1500: loss 2.056831\n",
      "iteration 900 / 1500: loss 1.973383\n",
      "iteration 1000 / 1500: loss 2.086074\n",
      "iteration 1100 / 1500: loss 2.024770\n",
      "iteration 1200 / 1500: loss 2.034779\n",
      "iteration 1300 / 1500: loss 1.981704\n",
      "iteration 1400 / 1500: loss 2.022771\n",
      "That took 17.201611s\n",
      "iteration 0 / 1500: loss 512.601107\n",
      "iteration 100 / 1500: loss 34.008692\n",
      "iteration 200 / 1500: loss 4.010987\n",
      "iteration 300 / 1500: loss 2.148862\n",
      "iteration 400 / 1500: loss 2.121666\n",
      "iteration 500 / 1500: loss 2.069514\n",
      "iteration 600 / 1500: loss 2.072771\n",
      "iteration 700 / 1500: loss 2.030304\n",
      "iteration 800 / 1500: loss 2.070978\n",
      "iteration 900 / 1500: loss 2.094741\n",
      "iteration 1000 / 1500: loss 1.984718\n",
      "iteration 1100 / 1500: loss 2.071597\n",
      "iteration 1200 / 1500: loss 2.042430\n",
      "iteration 1300 / 1500: loss 1.988481\n",
      "iteration 1400 / 1500: loss 2.153296\n",
      "That took 17.085683s\n",
      "iteration 0 / 1500: loss 557.859211\n",
      "iteration 100 / 1500: loss 29.708386\n",
      "iteration 200 / 1500: loss 3.509510\n",
      "iteration 300 / 1500: loss 2.086606\n",
      "iteration 400 / 1500: loss 2.010540\n",
      "iteration 500 / 1500: loss 2.022305\n",
      "iteration 600 / 1500: loss 2.085711\n",
      "iteration 700 / 1500: loss 2.060889\n",
      "iteration 800 / 1500: loss 2.077249\n",
      "iteration 900 / 1500: loss 2.032399\n",
      "iteration 1000 / 1500: loss 2.133587\n",
      "iteration 1100 / 1500: loss 2.015433\n",
      "iteration 1200 / 1500: loss 2.066039\n",
      "iteration 1300 / 1500: loss 2.088515\n",
      "iteration 1400 / 1500: loss 2.026905\n",
      "That took 17.110370s\n",
      "iteration 0 / 1500: loss 602.287084\n",
      "iteration 100 / 1500: loss 25.645113\n",
      "iteration 200 / 1500: loss 2.979064\n",
      "iteration 300 / 1500: loss 2.141468\n",
      "iteration 400 / 1500: loss 2.078490\n",
      "iteration 500 / 1500: loss 2.065527\n",
      "iteration 600 / 1500: loss 1.991089\n",
      "iteration 700 / 1500: loss 2.063303\n",
      "iteration 800 / 1500: loss 2.053271\n",
      "iteration 900 / 1500: loss 2.040056\n",
      "iteration 1000 / 1500: loss 2.050118\n",
      "iteration 1100 / 1500: loss 2.061587\n",
      "iteration 1200 / 1500: loss 2.054586\n",
      "iteration 1300 / 1500: loss 2.043371\n",
      "iteration 1400 / 1500: loss 2.084993\n",
      "That took 17.164073s\n",
      "iteration 0 / 1500: loss 638.628011\n",
      "iteration 100 / 1500: loss 21.875524\n",
      "iteration 200 / 1500: loss 2.726557\n",
      "iteration 300 / 1500: loss 2.076926\n",
      "iteration 400 / 1500: loss 1.986713\n",
      "iteration 500 / 1500: loss 2.080918\n",
      "iteration 600 / 1500: loss 2.074661\n",
      "iteration 700 / 1500: loss 2.050176\n",
      "iteration 800 / 1500: loss 2.065666\n",
      "iteration 900 / 1500: loss 2.162842\n",
      "iteration 1000 / 1500: loss 2.034609\n",
      "iteration 1100 / 1500: loss 2.070661\n",
      "iteration 1200 / 1500: loss 2.111110\n",
      "iteration 1300 / 1500: loss 2.055898\n",
      "iteration 1400 / 1500: loss 2.073501\n",
      "That took 17.357079s\n",
      "iteration 0 / 1500: loss 690.367217\n",
      "iteration 100 / 1500: loss 19.131951\n",
      "iteration 200 / 1500: loss 2.464461\n",
      "iteration 300 / 1500: loss 2.150891\n",
      "iteration 400 / 1500: loss 2.084034\n",
      "iteration 500 / 1500: loss 2.042063\n",
      "iteration 600 / 1500: loss 2.048941\n",
      "iteration 700 / 1500: loss 2.075576\n",
      "iteration 800 / 1500: loss 2.084561\n",
      "iteration 900 / 1500: loss 2.088613\n",
      "iteration 1000 / 1500: loss 2.101110\n",
      "iteration 1100 / 1500: loss 2.060841\n",
      "iteration 1200 / 1500: loss 2.059275\n",
      "iteration 1300 / 1500: loss 2.002589\n",
      "iteration 1400 / 1500: loss 2.121734\n",
      "That took 17.033590s\n",
      "iteration 0 / 1500: loss 732.319393\n",
      "iteration 100 / 1500: loss 16.338836\n",
      "iteration 200 / 1500: loss 2.395431\n",
      "iteration 300 / 1500: loss 2.043715\n",
      "iteration 400 / 1500: loss 2.102915\n",
      "iteration 500 / 1500: loss 2.076013\n",
      "iteration 600 / 1500: loss 2.088387\n",
      "iteration 700 / 1500: loss 2.099314\n",
      "iteration 800 / 1500: loss 2.122596\n",
      "iteration 900 / 1500: loss 2.051843\n",
      "iteration 1000 / 1500: loss 2.120057\n",
      "iteration 1100 / 1500: loss 2.093814\n",
      "iteration 1200 / 1500: loss 2.135698\n",
      "iteration 1300 / 1500: loss 2.073291\n",
      "iteration 1400 / 1500: loss 2.033269\n",
      "That took 17.089206s\n",
      "iteration 0 / 1500: loss 783.803453\n",
      "iteration 100 / 1500: loss 14.221904\n",
      "iteration 200 / 1500: loss 2.320772\n",
      "iteration 300 / 1500: loss 2.061100\n",
      "iteration 400 / 1500: loss 2.096142\n",
      "iteration 500 / 1500: loss 2.136850\n",
      "iteration 600 / 1500: loss 2.104536\n",
      "iteration 700 / 1500: loss 2.091919\n",
      "iteration 800 / 1500: loss 2.067018\n",
      "iteration 900 / 1500: loss 2.097726\n",
      "iteration 1000 / 1500: loss 2.135776\n",
      "iteration 1100 / 1500: loss 2.082024\n",
      "iteration 1200 / 1500: loss 2.049606\n",
      "iteration 1300 / 1500: loss 2.024096\n",
      "iteration 1400 / 1500: loss 2.113573\n",
      "That took 17.099234s\n",
      "iteration 0 / 1500: loss 393.571222\n",
      "iteration 100 / 1500: loss 40.967786\n",
      "iteration 200 / 1500: loss 5.967237\n",
      "iteration 300 / 1500: loss 2.389460\n",
      "iteration 400 / 1500: loss 2.066076\n",
      "iteration 500 / 1500: loss 2.052615\n",
      "iteration 600 / 1500: loss 2.010916\n",
      "iteration 700 / 1500: loss 2.052293\n",
      "iteration 800 / 1500: loss 1.990976\n",
      "iteration 900 / 1500: loss 1.998682\n",
      "iteration 1000 / 1500: loss 1.992472\n",
      "iteration 1100 / 1500: loss 1.993189\n",
      "iteration 1200 / 1500: loss 2.034533\n",
      "iteration 1300 / 1500: loss 2.091434\n",
      "iteration 1400 / 1500: loss 2.047699\n",
      "That took 17.102452s\n",
      "iteration 0 / 1500: loss 429.766494\n",
      "iteration 100 / 1500: loss 35.061357\n",
      "iteration 200 / 1500: loss 4.615775\n",
      "iteration 300 / 1500: loss 2.293061\n",
      "iteration 400 / 1500: loss 2.056287\n",
      "iteration 500 / 1500: loss 2.042747\n",
      "iteration 600 / 1500: loss 2.050384\n",
      "iteration 700 / 1500: loss 2.029994\n",
      "iteration 800 / 1500: loss 2.020829\n",
      "iteration 900 / 1500: loss 2.036777\n",
      "iteration 1000 / 1500: loss 2.060344\n",
      "iteration 1100 / 1500: loss 2.027751\n",
      "iteration 1200 / 1500: loss 2.116829\n",
      "iteration 1300 / 1500: loss 2.064093\n",
      "iteration 1400 / 1500: loss 2.022956\n",
      "That took 17.286159s\n",
      "iteration 0 / 1500: loss 473.316116\n",
      "iteration 100 / 1500: loss 30.170555\n",
      "iteration 200 / 1500: loss 3.754713\n",
      "iteration 300 / 1500: loss 2.131738\n",
      "iteration 400 / 1500: loss 2.053344\n",
      "iteration 500 / 1500: loss 1.991623\n",
      "iteration 600 / 1500: loss 2.006771\n",
      "iteration 700 / 1500: loss 2.056632\n",
      "iteration 800 / 1500: loss 2.046916\n",
      "iteration 900 / 1500: loss 2.034214\n",
      "iteration 1000 / 1500: loss 2.092708\n",
      "iteration 1100 / 1500: loss 2.065846\n",
      "iteration 1200 / 1500: loss 2.134468\n",
      "iteration 1300 / 1500: loss 2.041421\n",
      "iteration 1400 / 1500: loss 2.015780\n",
      "That took 17.037875s\n",
      "iteration 0 / 1500: loss 522.216541\n",
      "iteration 100 / 1500: loss 26.167173\n",
      "iteration 200 / 1500: loss 3.237319\n",
      "iteration 300 / 1500: loss 2.114649\n",
      "iteration 400 / 1500: loss 2.129730\n",
      "iteration 500 / 1500: loss 1.995493\n",
      "iteration 600 / 1500: loss 2.131154\n",
      "iteration 700 / 1500: loss 2.041989\n",
      "iteration 800 / 1500: loss 2.046953\n",
      "iteration 900 / 1500: loss 2.129378\n",
      "iteration 1000 / 1500: loss 2.047617\n",
      "iteration 1100 / 1500: loss 2.065352\n",
      "iteration 1200 / 1500: loss 2.117498\n",
      "iteration 1300 / 1500: loss 2.052042\n",
      "iteration 1400 / 1500: loss 2.028817\n",
      "That took 17.166993s\n",
      "iteration 0 / 1500: loss 557.073006\n",
      "iteration 100 / 1500: loss 21.864057\n",
      "iteration 200 / 1500: loss 2.773760\n",
      "iteration 300 / 1500: loss 2.058618\n",
      "iteration 400 / 1500: loss 2.051900\n",
      "iteration 500 / 1500: loss 2.013044\n",
      "iteration 600 / 1500: loss 2.093256\n",
      "iteration 700 / 1500: loss 2.038428\n",
      "iteration 800 / 1500: loss 2.083536\n",
      "iteration 900 / 1500: loss 2.060125\n",
      "iteration 1000 / 1500: loss 2.069822\n",
      "iteration 1100 / 1500: loss 2.084874\n",
      "iteration 1200 / 1500: loss 2.100711\n",
      "iteration 1300 / 1500: loss 2.122479\n",
      "iteration 1400 / 1500: loss 2.048562\n",
      "That took 16.927262s\n",
      "iteration 0 / 1500: loss 605.823332\n",
      "iteration 100 / 1500: loss 18.728090\n",
      "iteration 200 / 1500: loss 2.549743\n",
      "iteration 300 / 1500: loss 2.117613\n",
      "iteration 400 / 1500: loss 2.019634\n",
      "iteration 500 / 1500: loss 2.082731\n",
      "iteration 600 / 1500: loss 2.054889\n",
      "iteration 700 / 1500: loss 2.085884\n",
      "iteration 800 / 1500: loss 2.048441\n",
      "iteration 900 / 1500: loss 2.084363\n",
      "iteration 1000 / 1500: loss 2.108746\n",
      "iteration 1100 / 1500: loss 2.065211\n",
      "iteration 1200 / 1500: loss 2.045633\n",
      "iteration 1300 / 1500: loss 2.049511\n",
      "iteration 1400 / 1500: loss 2.031136\n",
      "That took 17.204362s\n",
      "iteration 0 / 1500: loss 642.614601\n",
      "iteration 100 / 1500: loss 15.722124\n",
      "iteration 200 / 1500: loss 2.306397\n",
      "iteration 300 / 1500: loss 2.059238\n",
      "iteration 400 / 1500: loss 2.096697\n",
      "iteration 500 / 1500: loss 2.116200\n",
      "iteration 600 / 1500: loss 2.102567\n",
      "iteration 700 / 1500: loss 2.125324\n",
      "iteration 800 / 1500: loss 2.152992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 2.078855\n",
      "iteration 1000 / 1500: loss 2.138943\n",
      "iteration 1100 / 1500: loss 2.073667\n",
      "iteration 1200 / 1500: loss 2.167827\n",
      "iteration 1300 / 1500: loss 2.008757\n",
      "iteration 1400 / 1500: loss 2.093605\n",
      "That took 18.176790s\n",
      "iteration 0 / 1500: loss 695.157750\n",
      "iteration 100 / 1500: loss 13.519224\n",
      "iteration 200 / 1500: loss 2.316686\n",
      "iteration 300 / 1500: loss 2.060240\n",
      "iteration 400 / 1500: loss 2.121219\n",
      "iteration 500 / 1500: loss 2.074434\n",
      "iteration 600 / 1500: loss 2.047458\n",
      "iteration 700 / 1500: loss 2.090288\n",
      "iteration 800 / 1500: loss 2.035088\n",
      "iteration 900 / 1500: loss 2.095237\n",
      "iteration 1000 / 1500: loss 2.033276\n",
      "iteration 1100 / 1500: loss 2.057381\n",
      "iteration 1200 / 1500: loss 2.110923\n",
      "iteration 1300 / 1500: loss 2.074614\n",
      "iteration 1400 / 1500: loss 2.017032\n",
      "That took 17.256773s\n",
      "iteration 0 / 1500: loss 736.531845\n",
      "iteration 100 / 1500: loss 11.470761\n",
      "iteration 200 / 1500: loss 2.223686\n",
      "iteration 300 / 1500: loss 2.147489\n",
      "iteration 400 / 1500: loss 2.098595\n",
      "iteration 500 / 1500: loss 2.136286\n",
      "iteration 600 / 1500: loss 2.042494\n",
      "iteration 700 / 1500: loss 2.135083\n",
      "iteration 800 / 1500: loss 2.057039\n",
      "iteration 900 / 1500: loss 2.089839\n",
      "iteration 1000 / 1500: loss 2.156601\n",
      "iteration 1100 / 1500: loss 2.136873\n",
      "iteration 1200 / 1500: loss 2.135915\n",
      "iteration 1300 / 1500: loss 2.104197\n",
      "iteration 1400 / 1500: loss 2.055780\n",
      "That took 16.901526s\n",
      "iteration 0 / 1500: loss 768.857149\n",
      "iteration 100 / 1500: loss 9.668069\n",
      "iteration 200 / 1500: loss 2.164491\n",
      "iteration 300 / 1500: loss 2.038526\n",
      "iteration 400 / 1500: loss 2.095058\n",
      "iteration 500 / 1500: loss 2.092895\n",
      "iteration 600 / 1500: loss 2.079967\n",
      "iteration 700 / 1500: loss 2.088124\n",
      "iteration 800 / 1500: loss 2.088324\n",
      "iteration 900 / 1500: loss 2.097538\n",
      "iteration 1000 / 1500: loss 2.038041\n",
      "iteration 1100 / 1500: loss 2.089447\n",
      "iteration 1200 / 1500: loss 2.108183\n",
      "iteration 1300 / 1500: loss 2.174489\n",
      "iteration 1400 / 1500: loss 2.111473\n",
      "That took 16.848460s\n",
      "iteration 0 / 1500: loss 385.924256\n",
      "iteration 100 / 1500: loss 32.548778\n",
      "iteration 200 / 1500: loss 4.454510\n",
      "iteration 300 / 1500: loss 2.213039\n",
      "iteration 400 / 1500: loss 2.009940\n",
      "iteration 500 / 1500: loss 1.982174\n",
      "iteration 600 / 1500: loss 2.049230\n",
      "iteration 700 / 1500: loss 2.052117\n",
      "iteration 800 / 1500: loss 2.014737\n",
      "iteration 900 / 1500: loss 1.996910\n",
      "iteration 1000 / 1500: loss 2.054233\n",
      "iteration 1100 / 1500: loss 2.020529\n",
      "iteration 1200 / 1500: loss 2.134020\n",
      "iteration 1300 / 1500: loss 2.055583\n",
      "iteration 1400 / 1500: loss 2.037162\n",
      "That took 17.033369s\n",
      "iteration 0 / 1500: loss 428.664509\n",
      "iteration 100 / 1500: loss 27.642197\n",
      "iteration 200 / 1500: loss 3.581947\n",
      "iteration 300 / 1500: loss 2.093869\n",
      "iteration 400 / 1500: loss 2.095363\n",
      "iteration 500 / 1500: loss 2.072644\n",
      "iteration 600 / 1500: loss 2.068407\n",
      "iteration 700 / 1500: loss 2.021451\n",
      "iteration 800 / 1500: loss 2.063160\n",
      "iteration 900 / 1500: loss 2.013956\n",
      "iteration 1000 / 1500: loss 2.024996\n",
      "iteration 1100 / 1500: loss 2.067412\n",
      "iteration 1200 / 1500: loss 2.041523\n",
      "iteration 1300 / 1500: loss 2.008835\n",
      "iteration 1400 / 1500: loss 2.061515\n",
      "That took 17.155291s\n",
      "iteration 0 / 1500: loss 478.397974\n",
      "iteration 100 / 1500: loss 23.568882\n",
      "iteration 200 / 1500: loss 3.032367\n",
      "iteration 300 / 1500: loss 2.065511\n",
      "iteration 400 / 1500: loss 2.032715\n",
      "iteration 500 / 1500: loss 2.004498\n",
      "iteration 600 / 1500: loss 2.016162\n",
      "iteration 700 / 1500: loss 2.003083\n",
      "iteration 800 / 1500: loss 2.063465\n",
      "iteration 900 / 1500: loss 2.039415\n",
      "iteration 1000 / 1500: loss 2.040364\n",
      "iteration 1100 / 1500: loss 2.029027\n",
      "iteration 1200 / 1500: loss 2.094964\n",
      "iteration 1300 / 1500: loss 2.073147\n",
      "iteration 1400 / 1500: loss 2.043398\n",
      "That took 17.216380s\n",
      "iteration 0 / 1500: loss 515.578743\n",
      "iteration 100 / 1500: loss 19.550956\n",
      "iteration 200 / 1500: loss 2.573962\n",
      "iteration 300 / 1500: loss 2.104333\n",
      "iteration 400 / 1500: loss 2.073316\n",
      "iteration 500 / 1500: loss 2.085580\n",
      "iteration 600 / 1500: loss 2.059877\n",
      "iteration 700 / 1500: loss 2.012306\n",
      "iteration 800 / 1500: loss 1.993328\n",
      "iteration 900 / 1500: loss 1.974057\n",
      "iteration 1000 / 1500: loss 1.974920\n",
      "iteration 1100 / 1500: loss 2.055276\n",
      "iteration 1200 / 1500: loss 2.138737\n",
      "iteration 1300 / 1500: loss 2.077210\n",
      "iteration 1400 / 1500: loss 2.055996\n",
      "That took 17.308757s\n",
      "iteration 0 / 1500: loss 562.084161\n",
      "iteration 100 / 1500: loss 16.446964\n",
      "iteration 200 / 1500: loss 2.420017\n",
      "iteration 300 / 1500: loss 2.079636\n",
      "iteration 400 / 1500: loss 2.004812\n",
      "iteration 500 / 1500: loss 2.098712\n",
      "iteration 600 / 1500: loss 1.985092\n",
      "iteration 700 / 1500: loss 2.101969\n",
      "iteration 800 / 1500: loss 2.134017\n",
      "iteration 900 / 1500: loss 2.045557\n",
      "iteration 1000 / 1500: loss 2.019962\n",
      "iteration 1100 / 1500: loss 2.085102\n",
      "iteration 1200 / 1500: loss 2.091596\n",
      "iteration 1300 / 1500: loss 2.041729\n",
      "iteration 1400 / 1500: loss 2.062721\n",
      "That took 17.291094s\n",
      "iteration 0 / 1500: loss 596.196022\n",
      "iteration 100 / 1500: loss 13.627160\n",
      "iteration 200 / 1500: loss 2.351565\n",
      "iteration 300 / 1500: loss 2.100215\n",
      "iteration 400 / 1500: loss 2.074127\n",
      "iteration 500 / 1500: loss 2.065516\n",
      "iteration 600 / 1500: loss 2.086015\n",
      "iteration 700 / 1500: loss 2.076797\n",
      "iteration 800 / 1500: loss 2.132985\n",
      "iteration 900 / 1500: loss 2.027710\n",
      "iteration 1000 / 1500: loss 2.101018\n",
      "iteration 1100 / 1500: loss 2.007663\n",
      "iteration 1200 / 1500: loss 1.995193\n",
      "iteration 1300 / 1500: loss 2.026594\n",
      "iteration 1400 / 1500: loss 2.055012\n",
      "That took 17.060923s\n",
      "iteration 0 / 1500: loss 664.767423\n",
      "iteration 100 / 1500: loss 11.809011\n",
      "iteration 200 / 1500: loss 2.304150\n",
      "iteration 300 / 1500: loss 2.086757\n",
      "iteration 400 / 1500: loss 2.128582\n",
      "iteration 500 / 1500: loss 2.062130\n",
      "iteration 600 / 1500: loss 2.040809\n",
      "iteration 700 / 1500: loss 2.089079\n",
      "iteration 800 / 1500: loss 2.091100\n",
      "iteration 900 / 1500: loss 2.063613\n",
      "iteration 1000 / 1500: loss 2.089128\n",
      "iteration 1100 / 1500: loss 2.005478\n",
      "iteration 1200 / 1500: loss 2.072428\n",
      "iteration 1300 / 1500: loss 1.985529\n",
      "iteration 1400 / 1500: loss 2.047470\n",
      "That took 16.975060s\n",
      "iteration 0 / 1500: loss 687.521505\n",
      "iteration 100 / 1500: loss 9.604739\n",
      "iteration 200 / 1500: loss 2.170623\n",
      "iteration 300 / 1500: loss 2.076134\n",
      "iteration 400 / 1500: loss 2.038761\n",
      "iteration 500 / 1500: loss 2.100661\n",
      "iteration 600 / 1500: loss 2.056260\n",
      "iteration 700 / 1500: loss 2.033376\n",
      "iteration 800 / 1500: loss 2.048292\n",
      "iteration 900 / 1500: loss 2.088556\n",
      "iteration 1000 / 1500: loss 2.093840\n",
      "iteration 1100 / 1500: loss 2.094476\n",
      "iteration 1200 / 1500: loss 2.005033\n",
      "iteration 1300 / 1500: loss 2.080544\n",
      "iteration 1400 / 1500: loss 2.086437\n",
      "That took 17.057583s\n",
      "iteration 0 / 1500: loss 737.590643\n",
      "iteration 100 / 1500: loss 8.180885\n",
      "iteration 200 / 1500: loss 2.131245\n",
      "iteration 300 / 1500: loss 2.068239\n",
      "iteration 400 / 1500: loss 2.046568\n",
      "iteration 500 / 1500: loss 2.066765\n",
      "iteration 600 / 1500: loss 2.059194\n",
      "iteration 700 / 1500: loss 2.071985\n",
      "iteration 800 / 1500: loss 2.054735\n",
      "iteration 900 / 1500: loss 2.074893\n",
      "iteration 1000 / 1500: loss 2.030363\n",
      "iteration 1100 / 1500: loss 2.080616\n",
      "iteration 1200 / 1500: loss 2.102124\n",
      "iteration 1300 / 1500: loss 2.108053\n",
      "iteration 1400 / 1500: loss 2.112698\n",
      "That took 17.078434s\n",
      "iteration 0 / 1500: loss 773.598076\n",
      "iteration 100 / 1500: loss 6.869981\n",
      "iteration 200 / 1500: loss 2.089070\n",
      "iteration 300 / 1500: loss 2.110709\n",
      "iteration 400 / 1500: loss 2.010369\n",
      "iteration 500 / 1500: loss 2.036641\n",
      "iteration 600 / 1500: loss 2.087806\n",
      "iteration 700 / 1500: loss 2.117625\n",
      "iteration 800 / 1500: loss 2.056973\n",
      "iteration 900 / 1500: loss 2.114536\n",
      "iteration 1000 / 1500: loss 2.078361\n",
      "iteration 1100 / 1500: loss 2.088189\n",
      "iteration 1200 / 1500: loss 2.095986\n",
      "iteration 1300 / 1500: loss 2.110022\n",
      "iteration 1400 / 1500: loss 2.125400\n",
      "That took 17.088622s\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.350694 val accuracy: 0.359000\n",
      "lr 1.000000e-07 reg 2.777778e+04 train accuracy: 0.348776 val accuracy: 0.364000\n",
      "lr 1.000000e-07 reg 3.055556e+04 train accuracy: 0.348000 val accuracy: 0.361000\n",
      "lr 1.000000e-07 reg 3.333333e+04 train accuracy: 0.337469 val accuracy: 0.351000\n",
      "lr 1.000000e-07 reg 3.611111e+04 train accuracy: 0.338531 val accuracy: 0.358000\n",
      "lr 1.000000e-07 reg 3.888889e+04 train accuracy: 0.335571 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 4.166667e+04 train accuracy: 0.335020 val accuracy: 0.348000\n",
      "lr 1.000000e-07 reg 4.444444e+04 train accuracy: 0.335796 val accuracy: 0.353000\n",
      "lr 1.000000e-07 reg 4.722222e+04 train accuracy: 0.328082 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.326286 val accuracy: 0.338000\n",
      "lr 1.444444e-07 reg 2.500000e+04 train accuracy: 0.351837 val accuracy: 0.364000\n",
      "lr 1.444444e-07 reg 2.777778e+04 train accuracy: 0.347592 val accuracy: 0.360000\n",
      "lr 1.444444e-07 reg 3.055556e+04 train accuracy: 0.350469 val accuracy: 0.362000\n",
      "lr 1.444444e-07 reg 3.333333e+04 train accuracy: 0.346204 val accuracy: 0.363000\n",
      "lr 1.444444e-07 reg 3.611111e+04 train accuracy: 0.338694 val accuracy: 0.345000\n",
      "lr 1.444444e-07 reg 3.888889e+04 train accuracy: 0.332510 val accuracy: 0.343000\n",
      "lr 1.444444e-07 reg 4.166667e+04 train accuracy: 0.330143 val accuracy: 0.347000\n",
      "lr 1.444444e-07 reg 4.444444e+04 train accuracy: 0.332959 val accuracy: 0.351000\n",
      "lr 1.444444e-07 reg 4.722222e+04 train accuracy: 0.326735 val accuracy: 0.344000\n",
      "lr 1.444444e-07 reg 5.000000e+04 train accuracy: 0.325776 val accuracy: 0.342000\n",
      "lr 1.888889e-07 reg 2.500000e+04 train accuracy: 0.349020 val accuracy: 0.362000\n",
      "lr 1.888889e-07 reg 2.777778e+04 train accuracy: 0.346204 val accuracy: 0.366000\n",
      "lr 1.888889e-07 reg 3.055556e+04 train accuracy: 0.345653 val accuracy: 0.355000\n",
      "lr 1.888889e-07 reg 3.333333e+04 train accuracy: 0.345612 val accuracy: 0.361000\n",
      "lr 1.888889e-07 reg 3.611111e+04 train accuracy: 0.334633 val accuracy: 0.349000\n",
      "lr 1.888889e-07 reg 3.888889e+04 train accuracy: 0.333429 val accuracy: 0.359000\n",
      "lr 1.888889e-07 reg 4.166667e+04 train accuracy: 0.328714 val accuracy: 0.350000\n",
      "lr 1.888889e-07 reg 4.444444e+04 train accuracy: 0.330837 val accuracy: 0.345000\n",
      "lr 1.888889e-07 reg 4.722222e+04 train accuracy: 0.330347 val accuracy: 0.347000\n",
      "lr 1.888889e-07 reg 5.000000e+04 train accuracy: 0.330551 val accuracy: 0.342000\n",
      "lr 2.333333e-07 reg 2.500000e+04 train accuracy: 0.354796 val accuracy: 0.375000\n",
      "lr 2.333333e-07 reg 2.777778e+04 train accuracy: 0.350061 val accuracy: 0.362000\n",
      "lr 2.333333e-07 reg 3.055556e+04 train accuracy: 0.344163 val accuracy: 0.358000\n",
      "lr 2.333333e-07 reg 3.333333e+04 train accuracy: 0.337939 val accuracy: 0.355000\n",
      "lr 2.333333e-07 reg 3.611111e+04 train accuracy: 0.340673 val accuracy: 0.360000\n",
      "lr 2.333333e-07 reg 3.888889e+04 train accuracy: 0.338102 val accuracy: 0.357000\n",
      "lr 2.333333e-07 reg 4.166667e+04 train accuracy: 0.342408 val accuracy: 0.351000\n",
      "lr 2.333333e-07 reg 4.444444e+04 train accuracy: 0.331653 val accuracy: 0.342000\n",
      "lr 2.333333e-07 reg 4.722222e+04 train accuracy: 0.335959 val accuracy: 0.336000\n",
      "lr 2.333333e-07 reg 5.000000e+04 train accuracy: 0.327306 val accuracy: 0.340000\n",
      "lr 2.777778e-07 reg 2.500000e+04 train accuracy: 0.341939 val accuracy: 0.359000\n",
      "lr 2.777778e-07 reg 2.777778e+04 train accuracy: 0.345816 val accuracy: 0.365000\n",
      "lr 2.777778e-07 reg 3.055556e+04 train accuracy: 0.340429 val accuracy: 0.358000\n",
      "lr 2.777778e-07 reg 3.333333e+04 train accuracy: 0.344245 val accuracy: 0.361000\n",
      "lr 2.777778e-07 reg 3.611111e+04 train accuracy: 0.334224 val accuracy: 0.346000\n",
      "lr 2.777778e-07 reg 3.888889e+04 train accuracy: 0.327857 val accuracy: 0.338000\n",
      "lr 2.777778e-07 reg 4.166667e+04 train accuracy: 0.334429 val accuracy: 0.358000\n",
      "lr 2.777778e-07 reg 4.444444e+04 train accuracy: 0.329449 val accuracy: 0.344000\n",
      "lr 2.777778e-07 reg 4.722222e+04 train accuracy: 0.331939 val accuracy: 0.342000\n",
      "lr 2.777778e-07 reg 5.000000e+04 train accuracy: 0.326857 val accuracy: 0.341000\n",
      "lr 3.222222e-07 reg 2.500000e+04 train accuracy: 0.343531 val accuracy: 0.356000\n",
      "lr 3.222222e-07 reg 2.777778e+04 train accuracy: 0.347388 val accuracy: 0.355000\n",
      "lr 3.222222e-07 reg 3.055556e+04 train accuracy: 0.339224 val accuracy: 0.353000\n",
      "lr 3.222222e-07 reg 3.333333e+04 train accuracy: 0.338388 val accuracy: 0.346000\n",
      "lr 3.222222e-07 reg 3.611111e+04 train accuracy: 0.338327 val accuracy: 0.355000\n",
      "lr 3.222222e-07 reg 3.888889e+04 train accuracy: 0.325449 val accuracy: 0.334000\n",
      "lr 3.222222e-07 reg 4.166667e+04 train accuracy: 0.329327 val accuracy: 0.339000\n",
      "lr 3.222222e-07 reg 4.444444e+04 train accuracy: 0.328347 val accuracy: 0.356000\n",
      "lr 3.222222e-07 reg 4.722222e+04 train accuracy: 0.332102 val accuracy: 0.346000\n",
      "lr 3.222222e-07 reg 5.000000e+04 train accuracy: 0.324857 val accuracy: 0.348000\n",
      "lr 3.666667e-07 reg 2.500000e+04 train accuracy: 0.339163 val accuracy: 0.355000\n",
      "lr 3.666667e-07 reg 2.777778e+04 train accuracy: 0.346102 val accuracy: 0.363000\n",
      "lr 3.666667e-07 reg 3.055556e+04 train accuracy: 0.340694 val accuracy: 0.360000\n",
      "lr 3.666667e-07 reg 3.333333e+04 train accuracy: 0.341408 val accuracy: 0.361000\n",
      "lr 3.666667e-07 reg 3.611111e+04 train accuracy: 0.338429 val accuracy: 0.338000\n",
      "lr 3.666667e-07 reg 3.888889e+04 train accuracy: 0.335490 val accuracy: 0.345000\n",
      "lr 3.666667e-07 reg 4.166667e+04 train accuracy: 0.325653 val accuracy: 0.344000\n",
      "lr 3.666667e-07 reg 4.444444e+04 train accuracy: 0.330898 val accuracy: 0.337000\n",
      "lr 3.666667e-07 reg 4.722222e+04 train accuracy: 0.317429 val accuracy: 0.332000\n",
      "lr 3.666667e-07 reg 5.000000e+04 train accuracy: 0.330327 val accuracy: 0.347000\n",
      "lr 4.111111e-07 reg 2.500000e+04 train accuracy: 0.349898 val accuracy: 0.364000\n",
      "lr 4.111111e-07 reg 2.777778e+04 train accuracy: 0.347980 val accuracy: 0.351000\n",
      "lr 4.111111e-07 reg 3.055556e+04 train accuracy: 0.337816 val accuracy: 0.353000\n",
      "lr 4.111111e-07 reg 3.333333e+04 train accuracy: 0.333286 val accuracy: 0.346000\n",
      "lr 4.111111e-07 reg 3.611111e+04 train accuracy: 0.340592 val accuracy: 0.347000\n",
      "lr 4.111111e-07 reg 3.888889e+04 train accuracy: 0.332592 val accuracy: 0.358000\n",
      "lr 4.111111e-07 reg 4.166667e+04 train accuracy: 0.334837 val accuracy: 0.354000\n",
      "lr 4.111111e-07 reg 4.444444e+04 train accuracy: 0.325286 val accuracy: 0.342000\n",
      "lr 4.111111e-07 reg 4.722222e+04 train accuracy: 0.335592 val accuracy: 0.353000\n",
      "lr 4.111111e-07 reg 5.000000e+04 train accuracy: 0.322061 val accuracy: 0.340000\n",
      "lr 4.555556e-07 reg 2.500000e+04 train accuracy: 0.347531 val accuracy: 0.361000\n",
      "lr 4.555556e-07 reg 2.777778e+04 train accuracy: 0.341571 val accuracy: 0.349000\n",
      "lr 4.555556e-07 reg 3.055556e+04 train accuracy: 0.344204 val accuracy: 0.357000\n",
      "lr 4.555556e-07 reg 3.333333e+04 train accuracy: 0.340898 val accuracy: 0.355000\n",
      "lr 4.555556e-07 reg 3.611111e+04 train accuracy: 0.332898 val accuracy: 0.350000\n",
      "lr 4.555556e-07 reg 3.888889e+04 train accuracy: 0.332898 val accuracy: 0.351000\n",
      "lr 4.555556e-07 reg 4.166667e+04 train accuracy: 0.335429 val accuracy: 0.349000\n",
      "lr 4.555556e-07 reg 4.444444e+04 train accuracy: 0.323388 val accuracy: 0.340000\n",
      "lr 4.555556e-07 reg 4.722222e+04 train accuracy: 0.328959 val accuracy: 0.343000\n",
      "lr 4.555556e-07 reg 5.000000e+04 train accuracy: 0.318204 val accuracy: 0.339000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.349388 val accuracy: 0.361000\n",
      "lr 5.000000e-07 reg 2.777778e+04 train accuracy: 0.344102 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 3.055556e+04 train accuracy: 0.347592 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 3.333333e+04 train accuracy: 0.346408 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 3.611111e+04 train accuracy: 0.329041 val accuracy: 0.347000\n",
      "lr 5.000000e-07 reg 3.888889e+04 train accuracy: 0.329653 val accuracy: 0.329000\n",
      "lr 5.000000e-07 reg 4.166667e+04 train accuracy: 0.330673 val accuracy: 0.340000\n",
      "lr 5.000000e-07 reg 4.444444e+04 train accuracy: 0.340837 val accuracy: 0.347000\n",
      "lr 5.000000e-07 reg 4.722222e+04 train accuracy: 0.332408 val accuracy: 0.338000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.319551 val accuracy: 0.326000\n",
      "best validation accuracy achieved during cross-validation: 0.375000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "range_lern_rate = np.linspace(learning_rates[0],learning_rates[1],10)\n",
    "range_reg = np.linspace(regularization_strengths[0],regularization_strengths[1],10)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "for i in range_lern_rate:\n",
    "    for j in range_reg:\n",
    "        softmax = Softmax()\n",
    "        tic = time.time()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=i, reg=j,num_iters=1500, verbose=True)\n",
    "        toc = time.time()\n",
    "        print('That took %fs' % (toc - tic))\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_accur = np.mean(y_train == y_train_pred)   \n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_accur = np.mean(y_val == y_val_pred)\n",
    "        results[(i, j)]=(train_accur, val_accur)\n",
    "        \n",
    "        if val_accur>best_val:\n",
    "            best_val = val_accur\n",
    "            best_softmax = softmax    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.346000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:Да\n",
    "\n",
    "*Your explanation*:В SVM если добавить точку, score которой будет за пределами margin от правильного класса, она никак не повлияет на loss, однако в SVM даже точка, очень сильно удалённая по score от правильного класса, повлияет на loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
