{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.413385\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Потому , что суммируется в знаменателе по десяти элементам, каждый элемент колеблется вокруг единицы, и в числителе один такой элемент*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.144071 analytic: -2.144071, relative error: 5.685411e-09\n",
      "numerical: -2.605971 analytic: -2.605971, relative error: 1.030905e-08\n",
      "numerical: 4.451479 analytic: 4.451479, relative error: 3.709216e-09\n",
      "numerical: 3.451371 analytic: 3.451371, relative error: 2.462136e-08\n",
      "numerical: 0.189206 analytic: 0.189206, relative error: 1.972662e-07\n",
      "numerical: 0.787074 analytic: 0.787074, relative error: 5.287779e-09\n",
      "numerical: 3.971139 analytic: 3.971139, relative error: 1.739027e-08\n",
      "numerical: -5.614336 analytic: -5.614336, relative error: 9.930122e-09\n",
      "numerical: 1.162398 analytic: 1.162398, relative error: 4.623967e-08\n",
      "numerical: 1.735740 analytic: 1.735740, relative error: 1.103260e-08\n",
      "numerical: 1.021648 analytic: 1.022439, relative error: 3.869002e-04\n",
      "numerical: -0.456456 analytic: -0.472140, relative error: 1.689017e-02\n",
      "numerical: -0.072218 analytic: -0.064738, relative error: 5.461599e-02\n",
      "numerical: -1.225014 analytic: -1.222357, relative error: 1.085495e-03\n",
      "numerical: -0.980034 analytic: -0.969788, relative error: 5.254720e-03\n",
      "numerical: -7.270839 analytic: -7.276138, relative error: 3.642312e-04\n",
      "numerical: -4.636137 analytic: -4.646445, relative error: 1.110557e-03\n",
      "numerical: -4.665094 analytic: -4.668036, relative error: 3.152189e-04\n",
      "numerical: 0.122438 analytic: 0.128677, relative error: 2.484351e-02\n",
      "numerical: -5.205317 analytic: -5.211965, relative error: 6.382154e-04\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.413385e+00 computed in 0.098423s\n",
      "vectorized loss: 2.413385e+00 computed in 0.004409s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 392.911252\n",
      "iteration 100 / 1500: loss 237.722584\n",
      "iteration 200 / 1500: loss 144.621967\n",
      "iteration 300 / 1500: loss 88.205732\n",
      "iteration 400 / 1500: loss 54.070178\n",
      "iteration 500 / 1500: loss 33.486344\n",
      "iteration 600 / 1500: loss 21.071358\n",
      "iteration 700 / 1500: loss 13.603166\n",
      "iteration 800 / 1500: loss 8.978432\n",
      "iteration 900 / 1500: loss 6.179748\n",
      "iteration 1000 / 1500: loss 4.513044\n",
      "iteration 1100 / 1500: loss 3.475400\n",
      "iteration 1200 / 1500: loss 3.008592\n",
      "iteration 1300 / 1500: loss 2.558985\n",
      "iteration 1400 / 1500: loss 2.371883\n",
      "That took 17.623913s\n",
      "iteration 0 / 1500: loss 428.586523\n",
      "iteration 100 / 1500: loss 245.399918\n",
      "iteration 200 / 1500: loss 141.116425\n",
      "iteration 300 / 1500: loss 81.534202\n",
      "iteration 400 / 1500: loss 47.485530\n",
      "iteration 500 / 1500: loss 28.002894\n",
      "iteration 600 / 1500: loss 16.968556\n",
      "iteration 700 / 1500: loss 10.496080\n",
      "iteration 800 / 1500: loss 6.956401\n",
      "iteration 900 / 1500: loss 4.868119\n",
      "iteration 1000 / 1500: loss 3.650567\n",
      "iteration 1100 / 1500: loss 2.919103\n",
      "iteration 1200 / 1500: loss 2.521864\n",
      "iteration 1300 / 1500: loss 2.377428\n",
      "iteration 1400 / 1500: loss 2.235673\n",
      "That took 17.814124s\n",
      "iteration 0 / 1500: loss 476.201921\n",
      "iteration 100 / 1500: loss 257.292039\n",
      "iteration 200 / 1500: loss 140.119604\n",
      "iteration 300 / 1500: loss 76.743831\n",
      "iteration 400 / 1500: loss 42.488515\n",
      "iteration 500 / 1500: loss 23.882960\n",
      "iteration 600 / 1500: loss 13.873027\n",
      "iteration 700 / 1500: loss 8.421981\n",
      "iteration 800 / 1500: loss 5.493672\n",
      "iteration 900 / 1500: loss 3.916260\n",
      "iteration 1000 / 1500: loss 3.061863\n",
      "iteration 1100 / 1500: loss 2.623935\n",
      "iteration 1200 / 1500: loss 2.395066\n",
      "iteration 1300 / 1500: loss 2.211217\n",
      "iteration 1400 / 1500: loss 2.158488\n",
      "That took 18.794374s\n",
      "iteration 0 / 1500: loss 524.412242\n",
      "iteration 100 / 1500: loss 268.541164\n",
      "iteration 200 / 1500: loss 138.154739\n",
      "iteration 300 / 1500: loss 71.770235\n",
      "iteration 400 / 1500: loss 37.690508\n",
      "iteration 500 / 1500: loss 20.325329\n",
      "iteration 600 / 1500: loss 11.365910\n",
      "iteration 700 / 1500: loss 6.740748\n",
      "iteration 800 / 1500: loss 4.489120\n",
      "iteration 900 / 1500: loss 3.242390\n",
      "iteration 1000 / 1500: loss 2.733709\n",
      "iteration 1100 / 1500: loss 2.346888\n",
      "iteration 1200 / 1500: loss 2.185375\n",
      "iteration 1300 / 1500: loss 2.228706\n",
      "iteration 1400 / 1500: loss 2.050755\n",
      "That took 18.143227s\n",
      "iteration 0 / 1500: loss 562.406671\n",
      "iteration 100 / 1500: loss 272.779626\n",
      "iteration 200 / 1500: loss 133.138487\n",
      "iteration 300 / 1500: loss 65.410875\n",
      "iteration 400 / 1500: loss 32.762391\n",
      "iteration 500 / 1500: loss 16.892858\n",
      "iteration 600 / 1500: loss 9.202845\n",
      "iteration 700 / 1500: loss 5.486772\n",
      "iteration 800 / 1500: loss 3.769955\n",
      "iteration 900 / 1500: loss 2.862238\n",
      "iteration 1000 / 1500: loss 2.422602\n",
      "iteration 1100 / 1500: loss 2.231215\n",
      "iteration 1200 / 1500: loss 2.080974\n",
      "iteration 1300 / 1500: loss 2.051046\n",
      "iteration 1400 / 1500: loss 2.138586\n",
      "That took 17.824186s\n",
      "iteration 0 / 1500: loss 603.480983\n",
      "iteration 100 / 1500: loss 276.391394\n",
      "iteration 200 / 1500: loss 127.651535\n",
      "iteration 300 / 1500: loss 59.574943\n",
      "iteration 400 / 1500: loss 28.371250\n",
      "iteration 500 / 1500: loss 14.098119\n",
      "iteration 600 / 1500: loss 7.615310\n",
      "iteration 700 / 1500: loss 4.510769\n",
      "iteration 800 / 1500: loss 3.144719\n",
      "iteration 900 / 1500: loss 2.574667\n",
      "iteration 1000 / 1500: loss 2.345893\n",
      "iteration 1100 / 1500: loss 2.136595\n",
      "iteration 1200 / 1500: loss 2.072417\n",
      "iteration 1300 / 1500: loss 2.127517\n",
      "iteration 1400 / 1500: loss 2.037093\n",
      "That took 17.797909s\n",
      "iteration 0 / 1500: loss 643.279898\n",
      "iteration 100 / 1500: loss 279.111458\n",
      "iteration 200 / 1500: loss 121.930627\n",
      "iteration 300 / 1500: loss 53.819286\n",
      "iteration 400 / 1500: loss 24.518130\n",
      "iteration 500 / 1500: loss 11.781897\n",
      "iteration 600 / 1500: loss 6.311195\n",
      "iteration 700 / 1500: loss 3.900959\n",
      "iteration 800 / 1500: loss 2.849815\n",
      "iteration 900 / 1500: loss 2.392265\n",
      "iteration 1000 / 1500: loss 2.228372\n",
      "iteration 1100 / 1500: loss 2.165892\n",
      "iteration 1200 / 1500: loss 2.093223\n",
      "iteration 1300 / 1500: loss 2.031914\n",
      "iteration 1400 / 1500: loss 2.048963\n",
      "That took 17.740973s\n",
      "iteration 0 / 1500: loss 698.270556\n",
      "iteration 100 / 1500: loss 286.851004\n",
      "iteration 200 / 1500: loss 118.436335\n",
      "iteration 300 / 1500: loss 49.728652\n",
      "iteration 400 / 1500: loss 21.531109\n",
      "iteration 500 / 1500: loss 10.000823\n",
      "iteration 600 / 1500: loss 5.337691\n",
      "iteration 700 / 1500: loss 3.311406\n",
      "iteration 800 / 1500: loss 2.584788\n",
      "iteration 900 / 1500: loss 2.266613\n",
      "iteration 1000 / 1500: loss 2.157818\n",
      "iteration 1100 / 1500: loss 2.133558\n",
      "iteration 1200 / 1500: loss 2.026783\n",
      "iteration 1300 / 1500: loss 2.098128\n",
      "iteration 1400 / 1500: loss 2.014678\n",
      "That took 17.905931s\n",
      "iteration 0 / 1500: loss 733.166137\n",
      "iteration 100 / 1500: loss 284.144628\n",
      "iteration 200 / 1500: loss 111.038246\n",
      "iteration 300 / 1500: loss 44.206311\n",
      "iteration 400 / 1500: loss 18.362257\n",
      "iteration 500 / 1500: loss 8.386978\n",
      "iteration 600 / 1500: loss 4.503336\n",
      "iteration 700 / 1500: loss 3.020253\n",
      "iteration 800 / 1500: loss 2.415089\n",
      "iteration 900 / 1500: loss 2.153286\n",
      "iteration 1000 / 1500: loss 2.155170\n",
      "iteration 1100 / 1500: loss 2.091233\n",
      "iteration 1200 / 1500: loss 2.060848\n",
      "iteration 1300 / 1500: loss 1.996205\n",
      "iteration 1400 / 1500: loss 2.007850\n",
      "That took 17.862048s\n",
      "iteration 0 / 1500: loss 779.677061\n",
      "iteration 100 / 1500: loss 286.285072\n",
      "iteration 200 / 1500: loss 106.240497\n",
      "iteration 300 / 1500: loss 40.112998\n",
      "iteration 400 / 1500: loss 16.019787\n",
      "iteration 500 / 1500: loss 7.235799\n",
      "iteration 600 / 1500: loss 3.993574\n",
      "iteration 700 / 1500: loss 2.772500\n",
      "iteration 800 / 1500: loss 2.350693\n",
      "iteration 900 / 1500: loss 2.210025\n",
      "iteration 1000 / 1500: loss 2.136923\n",
      "iteration 1100 / 1500: loss 2.113777\n",
      "iteration 1200 / 1500: loss 2.044093\n",
      "iteration 1300 / 1500: loss 2.116139\n",
      "iteration 1400 / 1500: loss 2.064346\n",
      "That took 17.895114s\n",
      "iteration 0 / 1500: loss 384.848337\n",
      "iteration 100 / 1500: loss 186.468706\n",
      "iteration 200 / 1500: loss 91.130658\n",
      "iteration 300 / 1500: loss 45.060905\n",
      "iteration 400 / 1500: loss 22.904719\n",
      "iteration 500 / 1500: loss 12.143177\n",
      "iteration 600 / 1500: loss 6.817959\n",
      "iteration 700 / 1500: loss 4.393036\n",
      "iteration 800 / 1500: loss 3.223949\n",
      "iteration 900 / 1500: loss 2.588207\n",
      "iteration 1000 / 1500: loss 2.300082\n",
      "iteration 1100 / 1500: loss 2.148964\n",
      "iteration 1200 / 1500: loss 2.092100\n",
      "iteration 1300 / 1500: loss 2.049094\n",
      "iteration 1400 / 1500: loss 2.030604\n",
      "That took 17.870997s\n",
      "iteration 0 / 1500: loss 435.925713\n",
      "iteration 100 / 1500: loss 194.390067\n",
      "iteration 200 / 1500: loss 87.783863\n",
      "iteration 300 / 1500: loss 40.195197\n",
      "iteration 400 / 1500: loss 19.069288\n",
      "iteration 500 / 1500: loss 9.592678\n",
      "iteration 600 / 1500: loss 5.459275\n",
      "iteration 700 / 1500: loss 3.588863\n",
      "iteration 800 / 1500: loss 2.668011\n",
      "iteration 900 / 1500: loss 2.366681\n",
      "iteration 1000 / 1500: loss 2.172274\n",
      "iteration 1100 / 1500: loss 2.147027\n",
      "iteration 1200 / 1500: loss 2.030017\n",
      "iteration 1300 / 1500: loss 2.086761\n",
      "iteration 1400 / 1500: loss 2.035491\n",
      "That took 17.914416s\n",
      "iteration 0 / 1500: loss 470.864909\n",
      "iteration 100 / 1500: loss 194.405226\n",
      "iteration 200 / 1500: loss 81.010456\n",
      "iteration 300 / 1500: loss 34.447839\n",
      "iteration 400 / 1500: loss 15.422861\n",
      "iteration 500 / 1500: loss 7.505602\n",
      "iteration 600 / 1500: loss 4.315755\n",
      "iteration 700 / 1500: loss 3.002446\n",
      "iteration 800 / 1500: loss 2.429963\n",
      "iteration 900 / 1500: loss 2.156401\n",
      "iteration 1000 / 1500: loss 2.061799\n",
      "iteration 1100 / 1500: loss 2.103018\n",
      "iteration 1200 / 1500: loss 2.000096\n",
      "iteration 1300 / 1500: loss 2.038024\n",
      "iteration 1400 / 1500: loss 2.096966\n",
      "That took 17.867575s\n",
      "iteration 0 / 1500: loss 515.895997\n",
      "iteration 100 / 1500: loss 196.470788\n",
      "iteration 200 / 1500: loss 75.646456\n",
      "iteration 300 / 1500: loss 29.904322\n",
      "iteration 400 / 1500: loss 12.671602\n",
      "iteration 500 / 1500: loss 6.143778\n",
      "iteration 600 / 1500: loss 3.616621\n",
      "iteration 700 / 1500: loss 2.647699\n",
      "iteration 800 / 1500: loss 2.260683\n",
      "iteration 900 / 1500: loss 2.166086\n",
      "iteration 1000 / 1500: loss 2.064445\n",
      "iteration 1100 / 1500: loss 2.082225\n",
      "iteration 1200 / 1500: loss 2.087919\n",
      "iteration 1300 / 1500: loss 2.052588\n",
      "iteration 1400 / 1500: loss 2.068443\n",
      "That took 17.732048s\n",
      "iteration 0 / 1500: loss 564.490043\n",
      "iteration 100 / 1500: loss 198.368146\n",
      "iteration 200 / 1500: loss 70.673392\n",
      "iteration 300 / 1500: loss 26.055140\n",
      "iteration 400 / 1500: loss 10.467516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 1500: loss 4.960097\n",
      "iteration 600 / 1500: loss 3.075999\n",
      "iteration 700 / 1500: loss 2.451116\n",
      "iteration 800 / 1500: loss 2.200932\n",
      "iteration 900 / 1500: loss 2.146495\n",
      "iteration 1000 / 1500: loss 2.032191\n",
      "iteration 1100 / 1500: loss 2.066782\n",
      "iteration 1200 / 1500: loss 2.055791\n",
      "iteration 1300 / 1500: loss 1.949469\n",
      "iteration 1400 / 1500: loss 2.062019\n",
      "That took 17.766830s\n",
      "iteration 0 / 1500: loss 605.367926\n",
      "iteration 100 / 1500: loss 196.278284\n",
      "iteration 200 / 1500: loss 64.716893\n",
      "iteration 300 / 1500: loss 22.318762\n",
      "iteration 400 / 1500: loss 8.658961\n",
      "iteration 500 / 1500: loss 4.260769\n",
      "iteration 600 / 1500: loss 2.701903\n",
      "iteration 700 / 1500: loss 2.248914\n",
      "iteration 800 / 1500: loss 2.148097\n",
      "iteration 900 / 1500: loss 2.101291\n",
      "iteration 1000 / 1500: loss 2.031444\n",
      "iteration 1100 / 1500: loss 2.013360\n",
      "iteration 1200 / 1500: loss 2.083216\n",
      "iteration 1300 / 1500: loss 2.098141\n",
      "iteration 1400 / 1500: loss 2.063387\n",
      "That took 17.740958s\n",
      "iteration 0 / 1500: loss 644.534281\n",
      "iteration 100 / 1500: loss 193.249045\n",
      "iteration 200 / 1500: loss 58.949356\n",
      "iteration 300 / 1500: loss 19.055152\n",
      "iteration 400 / 1500: loss 7.129899\n",
      "iteration 500 / 1500: loss 3.625377\n",
      "iteration 600 / 1500: loss 2.517944\n",
      "iteration 700 / 1500: loss 2.173727\n",
      "iteration 800 / 1500: loss 2.126060\n",
      "iteration 900 / 1500: loss 2.068997\n",
      "iteration 1000 / 1500: loss 2.124375\n",
      "iteration 1100 / 1500: loss 2.097187\n",
      "iteration 1200 / 1500: loss 2.061375\n",
      "iteration 1300 / 1500: loss 2.092661\n",
      "iteration 1400 / 1500: loss 2.058279\n",
      "That took 17.881010s\n",
      "iteration 0 / 1500: loss 683.799714\n",
      "iteration 100 / 1500: loss 189.203444\n",
      "iteration 200 / 1500: loss 53.556031\n",
      "iteration 300 / 1500: loss 16.226278\n",
      "iteration 400 / 1500: loss 5.981232\n",
      "iteration 500 / 1500: loss 3.144070\n",
      "iteration 600 / 1500: loss 2.414352\n",
      "iteration 700 / 1500: loss 2.150329\n",
      "iteration 800 / 1500: loss 2.096770\n",
      "iteration 900 / 1500: loss 2.086691\n",
      "iteration 1000 / 1500: loss 2.039992\n",
      "iteration 1100 / 1500: loss 2.056650\n",
      "iteration 1200 / 1500: loss 2.073675\n",
      "iteration 1300 / 1500: loss 2.070515\n",
      "iteration 1400 / 1500: loss 2.038239\n",
      "That took 17.869113s\n",
      "iteration 0 / 1500: loss 741.507734\n",
      "iteration 100 / 1500: loss 188.926023\n",
      "iteration 200 / 1500: loss 49.461581\n",
      "iteration 300 / 1500: loss 14.063719\n",
      "iteration 400 / 1500: loss 5.075806\n",
      "iteration 500 / 1500: loss 2.857607\n",
      "iteration 600 / 1500: loss 2.391344\n",
      "iteration 700 / 1500: loss 2.145828\n",
      "iteration 800 / 1500: loss 2.113008\n",
      "iteration 900 / 1500: loss 2.102694\n",
      "iteration 1000 / 1500: loss 2.098797\n",
      "iteration 1100 / 1500: loss 2.049516\n",
      "iteration 1200 / 1500: loss 2.081181\n",
      "iteration 1300 / 1500: loss 2.043668\n",
      "iteration 1400 / 1500: loss 2.085902\n",
      "That took 17.898613s\n",
      "iteration 0 / 1500: loss 776.130468\n",
      "iteration 100 / 1500: loss 182.884964\n",
      "iteration 200 / 1500: loss 44.342946\n",
      "iteration 300 / 1500: loss 11.992854\n",
      "iteration 400 / 1500: loss 4.406070\n",
      "iteration 500 / 1500: loss 2.606144\n",
      "iteration 600 / 1500: loss 2.218436\n",
      "iteration 700 / 1500: loss 2.102793\n",
      "iteration 800 / 1500: loss 2.071767\n",
      "iteration 900 / 1500: loss 2.042452\n",
      "iteration 1000 / 1500: loss 2.122495\n",
      "iteration 1100 / 1500: loss 2.127707\n",
      "iteration 1200 / 1500: loss 2.080410\n",
      "iteration 1300 / 1500: loss 2.124552\n",
      "iteration 1400 / 1500: loss 2.087514\n",
      "That took 18.500362s\n",
      "iteration 0 / 1500: loss 386.662740\n",
      "iteration 100 / 1500: loss 150.028885\n",
      "iteration 200 / 1500: loss 59.191801\n",
      "iteration 300 / 1500: loss 24.173987\n",
      "iteration 400 / 1500: loss 10.606283\n",
      "iteration 500 / 1500: loss 5.298445\n",
      "iteration 600 / 1500: loss 3.374793\n",
      "iteration 700 / 1500: loss 2.551082\n",
      "iteration 800 / 1500: loss 2.230131\n",
      "iteration 900 / 1500: loss 2.050658\n",
      "iteration 1000 / 1500: loss 2.051278\n",
      "iteration 1100 / 1500: loss 1.984169\n",
      "iteration 1200 / 1500: loss 2.046515\n",
      "iteration 1300 / 1500: loss 2.001159\n",
      "iteration 1400 / 1500: loss 2.052878\n",
      "That took 18.479385s\n",
      "iteration 0 / 1500: loss 425.524656\n",
      "iteration 100 / 1500: loss 148.513943\n",
      "iteration 200 / 1500: loss 53.013288\n",
      "iteration 300 / 1500: loss 19.763668\n",
      "iteration 400 / 1500: loss 8.218600\n",
      "iteration 500 / 1500: loss 4.154940\n",
      "iteration 600 / 1500: loss 2.754510\n",
      "iteration 700 / 1500: loss 2.226882\n",
      "iteration 800 / 1500: loss 2.108002\n",
      "iteration 900 / 1500: loss 2.017248\n",
      "iteration 1000 / 1500: loss 2.015003\n",
      "iteration 1100 / 1500: loss 2.109527\n",
      "iteration 1200 / 1500: loss 2.051453\n",
      "iteration 1300 / 1500: loss 2.045592\n",
      "iteration 1400 / 1500: loss 2.032469\n",
      "That took 17.821826s\n",
      "iteration 0 / 1500: loss 479.644728\n",
      "iteration 100 / 1500: loss 150.800007\n",
      "iteration 200 / 1500: loss 48.527646\n",
      "iteration 300 / 1500: loss 16.604437\n",
      "iteration 400 / 1500: loss 6.514541\n",
      "iteration 500 / 1500: loss 3.461735\n",
      "iteration 600 / 1500: loss 2.489451\n",
      "iteration 700 / 1500: loss 2.175233\n",
      "iteration 800 / 1500: loss 2.108492\n",
      "iteration 900 / 1500: loss 2.003432\n",
      "iteration 1000 / 1500: loss 2.061986\n",
      "iteration 1100 / 1500: loss 2.081404\n",
      "iteration 1200 / 1500: loss 2.009125\n",
      "iteration 1300 / 1500: loss 2.024103\n",
      "iteration 1400 / 1500: loss 2.039788\n",
      "That took 17.587664s\n",
      "iteration 0 / 1500: loss 520.362171\n",
      "iteration 100 / 1500: loss 147.504483\n",
      "iteration 200 / 1500: loss 42.901549\n",
      "iteration 300 / 1500: loss 13.575358\n",
      "iteration 400 / 1500: loss 5.273935\n",
      "iteration 500 / 1500: loss 3.004566\n",
      "iteration 600 / 1500: loss 2.295401\n",
      "iteration 700 / 1500: loss 2.115584\n",
      "iteration 800 / 1500: loss 2.038322\n",
      "iteration 900 / 1500: loss 2.108557\n",
      "iteration 1000 / 1500: loss 1.954796\n",
      "iteration 1100 / 1500: loss 1.979696\n",
      "iteration 1200 / 1500: loss 2.028972\n",
      "iteration 1300 / 1500: loss 2.052003\n",
      "iteration 1400 / 1500: loss 2.037078\n",
      "That took 17.525535s\n",
      "iteration 0 / 1500: loss 557.832878\n",
      "iteration 100 / 1500: loss 142.234184\n",
      "iteration 200 / 1500: loss 37.550923\n",
      "iteration 300 / 1500: loss 11.035794\n",
      "iteration 400 / 1500: loss 4.348404\n",
      "iteration 500 / 1500: loss 2.594179\n",
      "iteration 600 / 1500: loss 2.169035\n",
      "iteration 700 / 1500: loss 2.080925\n",
      "iteration 800 / 1500: loss 2.010947\n",
      "iteration 900 / 1500: loss 2.116869\n",
      "iteration 1000 / 1500: loss 2.083861\n",
      "iteration 1100 / 1500: loss 2.058804\n",
      "iteration 1200 / 1500: loss 2.042837\n",
      "iteration 1300 / 1500: loss 2.035030\n",
      "iteration 1400 / 1500: loss 2.013217\n",
      "That took 17.622092s\n",
      "iteration 0 / 1500: loss 606.730792\n",
      "iteration 100 / 1500: loss 139.359899\n",
      "iteration 200 / 1500: loss 33.387990\n",
      "iteration 300 / 1500: loss 9.155650\n",
      "iteration 400 / 1500: loss 3.663052\n",
      "iteration 500 / 1500: loss 2.449358\n",
      "iteration 600 / 1500: loss 2.122671\n",
      "iteration 700 / 1500: loss 2.047908\n",
      "iteration 800 / 1500: loss 1.996207\n",
      "iteration 900 / 1500: loss 2.067240\n",
      "iteration 1000 / 1500: loss 2.045246\n",
      "iteration 1100 / 1500: loss 2.066360\n",
      "iteration 1200 / 1500: loss 2.019931\n",
      "iteration 1300 / 1500: loss 2.083719\n",
      "iteration 1400 / 1500: loss 2.068439\n",
      "That took 17.396676s\n",
      "iteration 0 / 1500: loss 648.920632\n",
      "iteration 100 / 1500: loss 134.323369\n",
      "iteration 200 / 1500: loss 29.125247\n",
      "iteration 300 / 1500: loss 7.622632\n",
      "iteration 400 / 1500: loss 3.233456\n",
      "iteration 500 / 1500: loss 2.285055\n",
      "iteration 600 / 1500: loss 2.162952\n",
      "iteration 700 / 1500: loss 2.050058\n",
      "iteration 800 / 1500: loss 2.041771\n",
      "iteration 900 / 1500: loss 2.128717\n",
      "iteration 1000 / 1500: loss 2.038810\n",
      "iteration 1100 / 1500: loss 2.046573\n",
      "iteration 1200 / 1500: loss 2.062619\n",
      "iteration 1300 / 1500: loss 2.040861\n",
      "iteration 1400 / 1500: loss 2.050711\n",
      "That took 17.658457s\n",
      "iteration 0 / 1500: loss 684.912992\n",
      "iteration 100 / 1500: loss 127.625871\n",
      "iteration 200 / 1500: loss 25.215043\n",
      "iteration 300 / 1500: loss 6.294176\n",
      "iteration 400 / 1500: loss 2.849777\n",
      "iteration 500 / 1500: loss 2.234989\n",
      "iteration 600 / 1500: loss 2.089211\n",
      "iteration 700 / 1500: loss 2.081456\n",
      "iteration 800 / 1500: loss 2.084908\n",
      "iteration 900 / 1500: loss 2.063465\n",
      "iteration 1000 / 1500: loss 2.091690\n",
      "iteration 1100 / 1500: loss 2.077968\n",
      "iteration 1200 / 1500: loss 2.060260\n",
      "iteration 1300 / 1500: loss 2.091321\n",
      "iteration 1400 / 1500: loss 1.997857\n",
      "That took 17.834906s\n",
      "iteration 0 / 1500: loss 734.051910\n",
      "iteration 100 / 1500: loss 123.113107\n",
      "iteration 200 / 1500: loss 22.216399\n",
      "iteration 300 / 1500: loss 5.450667\n",
      "iteration 400 / 1500: loss 2.678919\n",
      "iteration 500 / 1500: loss 2.148031\n",
      "iteration 600 / 1500: loss 2.126690\n",
      "iteration 700 / 1500: loss 2.111012\n",
      "iteration 800 / 1500: loss 2.064992\n",
      "iteration 900 / 1500: loss 2.043610\n",
      "iteration 1000 / 1500: loss 2.082630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 2.022532\n",
      "iteration 1200 / 1500: loss 2.055910\n",
      "iteration 1300 / 1500: loss 2.118863\n",
      "iteration 1400 / 1500: loss 2.106020\n",
      "That took 17.423862s\n",
      "iteration 0 / 1500: loss 772.946914\n",
      "iteration 100 / 1500: loss 116.843589\n",
      "iteration 200 / 1500: loss 19.191583\n",
      "iteration 300 / 1500: loss 4.651046\n",
      "iteration 400 / 1500: loss 2.484887\n",
      "iteration 500 / 1500: loss 2.142160\n",
      "iteration 600 / 1500: loss 2.078054\n",
      "iteration 700 / 1500: loss 2.043982\n",
      "iteration 800 / 1500: loss 2.065069\n",
      "iteration 900 / 1500: loss 2.058414\n",
      "iteration 1000 / 1500: loss 2.065668\n",
      "iteration 1100 / 1500: loss 2.116362\n",
      "iteration 1200 / 1500: loss 2.051560\n",
      "iteration 1300 / 1500: loss 2.111298\n",
      "iteration 1400 / 1500: loss 2.106965\n",
      "That took 17.770364s\n",
      "iteration 0 / 1500: loss 395.450298\n",
      "iteration 100 / 1500: loss 122.675275\n",
      "iteration 200 / 1500: loss 39.288047\n",
      "iteration 300 / 1500: loss 13.540185\n",
      "iteration 400 / 1500: loss 5.596484\n",
      "iteration 500 / 1500: loss 3.146875\n",
      "iteration 600 / 1500: loss 2.409617\n",
      "iteration 700 / 1500: loss 2.125235\n",
      "iteration 800 / 1500: loss 2.087775\n",
      "iteration 900 / 1500: loss 2.047420\n",
      "iteration 1000 / 1500: loss 2.012016\n",
      "iteration 1100 / 1500: loss 2.020928\n",
      "iteration 1200 / 1500: loss 2.062526\n",
      "iteration 1300 / 1500: loss 2.017034\n",
      "iteration 1400 / 1500: loss 2.031287\n",
      "That took 17.314094s\n",
      "iteration 0 / 1500: loss 437.633803\n",
      "iteration 100 / 1500: loss 119.614743\n",
      "iteration 200 / 1500: loss 33.895088\n",
      "iteration 300 / 1500: loss 10.749374\n",
      "iteration 400 / 1500: loss 4.345204\n",
      "iteration 500 / 1500: loss 2.695419\n",
      "iteration 600 / 1500: loss 2.243488\n",
      "iteration 700 / 1500: loss 2.092645\n",
      "iteration 800 / 1500: loss 2.072999\n",
      "iteration 900 / 1500: loss 2.038316\n",
      "iteration 1000 / 1500: loss 2.017608\n",
      "iteration 1100 / 1500: loss 1.994796\n",
      "iteration 1200 / 1500: loss 2.006267\n",
      "iteration 1300 / 1500: loss 2.018324\n",
      "iteration 1400 / 1500: loss 2.047820\n",
      "That took 17.504737s\n",
      "iteration 0 / 1500: loss 478.800027\n",
      "iteration 100 / 1500: loss 114.553061\n",
      "iteration 200 / 1500: loss 28.760673\n",
      "iteration 300 / 1500: loss 8.412921\n",
      "iteration 400 / 1500: loss 3.513455\n",
      "iteration 500 / 1500: loss 2.312339\n",
      "iteration 600 / 1500: loss 2.162932\n",
      "iteration 700 / 1500: loss 2.084644\n",
      "iteration 800 / 1500: loss 2.085293\n",
      "iteration 900 / 1500: loss 2.132430\n",
      "iteration 1000 / 1500: loss 2.068105\n",
      "iteration 1100 / 1500: loss 2.054079\n",
      "iteration 1200 / 1500: loss 2.079118\n",
      "iteration 1300 / 1500: loss 2.037744\n",
      "iteration 1400 / 1500: loss 2.101680\n",
      "That took 17.492472s\n",
      "iteration 0 / 1500: loss 521.713640\n",
      "iteration 100 / 1500: loss 110.068296\n",
      "iteration 200 / 1500: loss 24.612910\n",
      "iteration 300 / 1500: loss 6.756452\n",
      "iteration 400 / 1500: loss 3.007423\n",
      "iteration 500 / 1500: loss 2.262224\n",
      "iteration 600 / 1500: loss 2.116219\n",
      "iteration 700 / 1500: loss 2.075142\n",
      "iteration 800 / 1500: loss 2.019684\n",
      "iteration 900 / 1500: loss 2.003801\n",
      "iteration 1000 / 1500: loss 2.012710\n",
      "iteration 1100 / 1500: loss 2.096986\n",
      "iteration 1200 / 1500: loss 1.995131\n",
      "iteration 1300 / 1500: loss 2.041670\n",
      "iteration 1400 / 1500: loss 2.075057\n",
      "That took 17.382797s\n",
      "iteration 0 / 1500: loss 551.627224\n",
      "iteration 100 / 1500: loss 102.311418\n",
      "iteration 200 / 1500: loss 20.371311\n",
      "iteration 300 / 1500: loss 5.376078\n",
      "iteration 400 / 1500: loss 2.637373\n",
      "iteration 500 / 1500: loss 2.135724\n",
      "iteration 600 / 1500: loss 2.111365\n",
      "iteration 700 / 1500: loss 2.061352\n",
      "iteration 800 / 1500: loss 2.044961\n",
      "iteration 900 / 1500: loss 2.076603\n",
      "iteration 1000 / 1500: loss 2.065068\n",
      "iteration 1100 / 1500: loss 2.077941\n",
      "iteration 1200 / 1500: loss 2.081602\n",
      "iteration 1300 / 1500: loss 2.066971\n",
      "iteration 1400 / 1500: loss 2.088602\n",
      "That took 17.309734s\n",
      "iteration 0 / 1500: loss 600.859424\n",
      "iteration 100 / 1500: loss 97.740853\n",
      "iteration 200 / 1500: loss 17.439128\n",
      "iteration 300 / 1500: loss 4.530301\n",
      "iteration 400 / 1500: loss 2.463405\n",
      "iteration 500 / 1500: loss 2.151645\n",
      "iteration 600 / 1500: loss 2.115809\n",
      "iteration 700 / 1500: loss 2.125242\n",
      "iteration 800 / 1500: loss 2.057720\n",
      "iteration 900 / 1500: loss 2.040993\n",
      "iteration 1000 / 1500: loss 2.045803\n",
      "iteration 1100 / 1500: loss 2.036493\n",
      "iteration 1200 / 1500: loss 2.001317\n",
      "iteration 1300 / 1500: loss 2.077112\n",
      "iteration 1400 / 1500: loss 2.080656\n",
      "That took 17.209578s\n",
      "iteration 0 / 1500: loss 632.942889\n",
      "iteration 100 / 1500: loss 90.273155\n",
      "iteration 200 / 1500: loss 14.490042\n",
      "iteration 300 / 1500: loss 3.846387\n",
      "iteration 400 / 1500: loss 2.291905\n",
      "iteration 500 / 1500: loss 2.080443\n",
      "iteration 600 / 1500: loss 2.054834\n",
      "iteration 700 / 1500: loss 2.087351\n",
      "iteration 800 / 1500: loss 2.152015\n",
      "iteration 900 / 1500: loss 2.096249\n",
      "iteration 1000 / 1500: loss 2.043984\n",
      "iteration 1100 / 1500: loss 2.047934\n",
      "iteration 1200 / 1500: loss 2.085211\n",
      "iteration 1300 / 1500: loss 2.089245\n",
      "iteration 1400 / 1500: loss 2.045934\n",
      "That took 17.414118s\n",
      "iteration 0 / 1500: loss 691.952632\n",
      "iteration 100 / 1500: loss 87.143352\n",
      "iteration 200 / 1500: loss 12.601465\n",
      "iteration 300 / 1500: loss 3.433813\n",
      "iteration 400 / 1500: loss 2.196927\n",
      "iteration 500 / 1500: loss 2.090402\n",
      "iteration 600 / 1500: loss 2.073562\n",
      "iteration 700 / 1500: loss 2.091361\n",
      "iteration 800 / 1500: loss 2.087617\n",
      "iteration 900 / 1500: loss 2.061176\n",
      "iteration 1000 / 1500: loss 2.028659\n",
      "iteration 1100 / 1500: loss 2.066618\n",
      "iteration 1200 / 1500: loss 2.033778\n",
      "iteration 1300 / 1500: loss 2.114869\n",
      "iteration 1400 / 1500: loss 2.095652\n",
      "That took 18.531015s\n",
      "iteration 0 / 1500: loss 722.625347\n",
      "iteration 100 / 1500: loss 79.963988\n",
      "iteration 200 / 1500: loss 10.533035\n",
      "iteration 300 / 1500: loss 3.004416\n",
      "iteration 400 / 1500: loss 2.149024\n",
      "iteration 500 / 1500: loss 2.081523\n",
      "iteration 600 / 1500: loss 1.993143\n",
      "iteration 700 / 1500: loss 2.009053\n",
      "iteration 800 / 1500: loss 2.021196\n",
      "iteration 900 / 1500: loss 2.120432\n",
      "iteration 1000 / 1500: loss 2.136704\n",
      "iteration 1100 / 1500: loss 2.107162\n",
      "iteration 1200 / 1500: loss 2.071220\n",
      "iteration 1300 / 1500: loss 2.028279\n",
      "iteration 1400 / 1500: loss 2.097087\n",
      "That took 17.539564s\n",
      "iteration 0 / 1500: loss 779.160190\n",
      "iteration 100 / 1500: loss 75.734708\n",
      "iteration 200 / 1500: loss 9.083326\n",
      "iteration 300 / 1500: loss 2.784203\n",
      "iteration 400 / 1500: loss 2.157977\n",
      "iteration 500 / 1500: loss 2.058319\n",
      "iteration 600 / 1500: loss 2.074883\n",
      "iteration 700 / 1500: loss 2.048975\n",
      "iteration 800 / 1500: loss 2.062948\n",
      "iteration 900 / 1500: loss 2.087479\n",
      "iteration 1000 / 1500: loss 2.173777\n",
      "iteration 1100 / 1500: loss 2.119745\n",
      "iteration 1200 / 1500: loss 2.089885\n",
      "iteration 1300 / 1500: loss 2.069902\n",
      "iteration 1400 / 1500: loss 2.144834\n",
      "That took 18.058061s\n",
      "iteration 0 / 1500: loss 394.445780\n",
      "iteration 100 / 1500: loss 98.452504\n",
      "iteration 200 / 1500: loss 25.734894\n",
      "iteration 300 / 1500: loss 7.911860\n",
      "iteration 400 / 1500: loss 3.477474\n",
      "iteration 500 / 1500: loss 2.353284\n",
      "iteration 600 / 1500: loss 2.100916\n",
      "iteration 700 / 1500: loss 2.049239\n",
      "iteration 800 / 1500: loss 2.044711\n",
      "iteration 900 / 1500: loss 2.115420\n",
      "iteration 1000 / 1500: loss 2.021624\n",
      "iteration 1100 / 1500: loss 1.992295\n",
      "iteration 1200 / 1500: loss 2.005480\n",
      "iteration 1300 / 1500: loss 2.059375\n",
      "iteration 1400 / 1500: loss 1.970001\n",
      "That took 17.821380s\n",
      "iteration 0 / 1500: loss 424.596733\n",
      "iteration 100 / 1500: loss 90.824833\n",
      "iteration 200 / 1500: loss 20.734482\n",
      "iteration 300 / 1500: loss 6.020626\n",
      "iteration 400 / 1500: loss 2.894318\n",
      "iteration 500 / 1500: loss 2.156988\n",
      "iteration 600 / 1500: loss 2.062345\n",
      "iteration 700 / 1500: loss 2.081264\n",
      "iteration 800 / 1500: loss 2.064896\n",
      "iteration 900 / 1500: loss 2.028473\n",
      "iteration 1000 / 1500: loss 2.045479\n",
      "iteration 1100 / 1500: loss 1.968888\n",
      "iteration 1200 / 1500: loss 2.094455\n",
      "iteration 1300 / 1500: loss 2.058170\n",
      "iteration 1400 / 1500: loss 2.019528\n",
      "That took 17.855730s\n",
      "iteration 0 / 1500: loss 474.358598\n",
      "iteration 100 / 1500: loss 86.991870\n",
      "iteration 200 / 1500: loss 17.382565\n",
      "iteration 300 / 1500: loss 4.786837\n",
      "iteration 400 / 1500: loss 2.639264\n",
      "iteration 500 / 1500: loss 2.170787\n",
      "iteration 600 / 1500: loss 2.084549\n",
      "iteration 700 / 1500: loss 2.062332\n",
      "iteration 800 / 1500: loss 2.102587\n",
      "iteration 900 / 1500: loss 2.005972\n",
      "iteration 1000 / 1500: loss 2.038601\n",
      "iteration 1100 / 1500: loss 2.088059\n",
      "iteration 1200 / 1500: loss 2.023267\n",
      "iteration 1300 / 1500: loss 2.044092\n",
      "iteration 1400 / 1500: loss 1.980273\n",
      "That took 17.905238s\n",
      "iteration 0 / 1500: loss 518.963078\n",
      "iteration 100 / 1500: loss 81.452040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 14.384040\n",
      "iteration 300 / 1500: loss 3.996710\n",
      "iteration 400 / 1500: loss 2.346282\n",
      "iteration 500 / 1500: loss 2.101132\n",
      "iteration 600 / 1500: loss 2.083036\n",
      "iteration 700 / 1500: loss 2.068241\n",
      "iteration 800 / 1500: loss 2.040203\n",
      "iteration 900 / 1500: loss 2.010004\n",
      "iteration 1000 / 1500: loss 1.983601\n",
      "iteration 1100 / 1500: loss 2.025666\n",
      "iteration 1200 / 1500: loss 2.032896\n",
      "iteration 1300 / 1500: loss 2.077109\n",
      "iteration 1400 / 1500: loss 2.071358\n",
      "That took 17.657768s\n",
      "iteration 0 / 1500: loss 556.830219\n",
      "iteration 100 / 1500: loss 75.043705\n",
      "iteration 200 / 1500: loss 11.714847\n",
      "iteration 300 / 1500: loss 3.319196\n",
      "iteration 400 / 1500: loss 2.246104\n",
      "iteration 500 / 1500: loss 2.077773\n",
      "iteration 600 / 1500: loss 2.106828\n",
      "iteration 700 / 1500: loss 2.028881\n",
      "iteration 800 / 1500: loss 2.051328\n",
      "iteration 900 / 1500: loss 2.027927\n",
      "iteration 1000 / 1500: loss 2.053456\n",
      "iteration 1100 / 1500: loss 2.064397\n",
      "iteration 1200 / 1500: loss 2.031849\n",
      "iteration 1300 / 1500: loss 2.105839\n",
      "iteration 1400 / 1500: loss 2.054620\n",
      "That took 17.936420s\n",
      "iteration 0 / 1500: loss 591.983621\n",
      "iteration 100 / 1500: loss 68.491591\n",
      "iteration 200 / 1500: loss 9.584301\n",
      "iteration 300 / 1500: loss 2.897145\n",
      "iteration 400 / 1500: loss 2.186647\n",
      "iteration 500 / 1500: loss 2.120088\n",
      "iteration 600 / 1500: loss 2.033162\n",
      "iteration 700 / 1500: loss 2.067999\n",
      "iteration 800 / 1500: loss 2.084300\n",
      "iteration 900 / 1500: loss 2.009677\n",
      "iteration 1000 / 1500: loss 2.037548\n",
      "iteration 1100 / 1500: loss 2.085405\n",
      "iteration 1200 / 1500: loss 2.089741\n",
      "iteration 1300 / 1500: loss 2.074136\n",
      "iteration 1400 / 1500: loss 2.035730\n",
      "That took 17.900970s\n",
      "iteration 0 / 1500: loss 647.024716\n",
      "iteration 100 / 1500: loss 64.425221\n",
      "iteration 200 / 1500: loss 8.076916\n",
      "iteration 300 / 1500: loss 2.678185\n",
      "iteration 400 / 1500: loss 2.128458\n",
      "iteration 500 / 1500: loss 2.131207\n",
      "iteration 600 / 1500: loss 2.084242\n",
      "iteration 700 / 1500: loss 2.024430\n",
      "iteration 800 / 1500: loss 2.108290\n",
      "iteration 900 / 1500: loss 2.067641\n",
      "iteration 1000 / 1500: loss 2.082159\n",
      "iteration 1100 / 1500: loss 1.964582\n",
      "iteration 1200 / 1500: loss 2.089223\n",
      "iteration 1300 / 1500: loss 2.099962\n",
      "iteration 1400 / 1500: loss 2.182397\n",
      "That took 17.805356s\n",
      "iteration 0 / 1500: loss 684.843294\n",
      "iteration 100 / 1500: loss 58.478497\n",
      "iteration 200 / 1500: loss 6.795513\n",
      "iteration 300 / 1500: loss 2.418671\n",
      "iteration 400 / 1500: loss 2.049906\n",
      "iteration 500 / 1500: loss 2.068392\n",
      "iteration 600 / 1500: loss 2.122002\n",
      "iteration 700 / 1500: loss 2.090191\n",
      "iteration 800 / 1500: loss 2.083263\n",
      "iteration 900 / 1500: loss 2.124723\n",
      "iteration 1000 / 1500: loss 2.084251\n",
      "iteration 1100 / 1500: loss 2.082213\n",
      "iteration 1200 / 1500: loss 2.128657\n",
      "iteration 1300 / 1500: loss 2.120677\n",
      "iteration 1400 / 1500: loss 2.077620\n",
      "That took 17.957992s\n",
      "iteration 0 / 1500: loss 729.355046\n",
      "iteration 100 / 1500: loss 53.373730\n",
      "iteration 200 / 1500: loss 5.741916\n",
      "iteration 300 / 1500: loss 2.363507\n",
      "iteration 400 / 1500: loss 2.120657\n",
      "iteration 500 / 1500: loss 2.076679\n",
      "iteration 600 / 1500: loss 2.124407\n",
      "iteration 700 / 1500: loss 2.019816\n",
      "iteration 800 / 1500: loss 2.120688\n",
      "iteration 900 / 1500: loss 2.141205\n",
      "iteration 1000 / 1500: loss 2.148526\n",
      "iteration 1100 / 1500: loss 2.108102\n",
      "iteration 1200 / 1500: loss 1.999970\n",
      "iteration 1300 / 1500: loss 2.082730\n",
      "iteration 1400 / 1500: loss 2.055154\n",
      "That took 17.952478s\n",
      "iteration 0 / 1500: loss 763.399920\n",
      "iteration 100 / 1500: loss 48.254316\n",
      "iteration 200 / 1500: loss 4.874889\n",
      "iteration 300 / 1500: loss 2.267476\n",
      "iteration 400 / 1500: loss 2.071595\n",
      "iteration 500 / 1500: loss 2.026439\n",
      "iteration 600 / 1500: loss 2.085426\n",
      "iteration 700 / 1500: loss 2.047179\n",
      "iteration 800 / 1500: loss 2.016575\n",
      "iteration 900 / 1500: loss 2.110313\n",
      "iteration 1000 / 1500: loss 2.104060\n",
      "iteration 1100 / 1500: loss 2.046959\n",
      "iteration 1200 / 1500: loss 2.078854\n",
      "iteration 1300 / 1500: loss 2.122729\n",
      "iteration 1400 / 1500: loss 2.050996\n",
      "That took 18.081736s\n",
      "iteration 0 / 1500: loss 389.244842\n",
      "iteration 100 / 1500: loss 78.051634\n",
      "iteration 200 / 1500: loss 16.986396\n",
      "iteration 300 / 1500: loss 4.988213\n",
      "iteration 400 / 1500: loss 2.527357\n",
      "iteration 500 / 1500: loss 2.176831\n",
      "iteration 600 / 1500: loss 2.091991\n",
      "iteration 700 / 1500: loss 2.025508\n",
      "iteration 800 / 1500: loss 2.053712\n",
      "iteration 900 / 1500: loss 2.005125\n",
      "iteration 1000 / 1500: loss 1.980441\n",
      "iteration 1100 / 1500: loss 2.033892\n",
      "iteration 1200 / 1500: loss 2.071094\n",
      "iteration 1300 / 1500: loss 2.013113\n",
      "iteration 1400 / 1500: loss 1.976405\n",
      "That took 17.875298s\n",
      "iteration 0 / 1500: loss 432.629867\n",
      "iteration 100 / 1500: loss 72.468538\n",
      "iteration 200 / 1500: loss 13.623708\n",
      "iteration 300 / 1500: loss 3.962950\n",
      "iteration 400 / 1500: loss 2.324197\n",
      "iteration 500 / 1500: loss 2.156171\n",
      "iteration 600 / 1500: loss 2.048103\n",
      "iteration 700 / 1500: loss 2.051473\n",
      "iteration 800 / 1500: loss 2.040214\n",
      "iteration 900 / 1500: loss 2.015082\n",
      "iteration 1000 / 1500: loss 2.032040\n",
      "iteration 1100 / 1500: loss 2.044221\n",
      "iteration 1200 / 1500: loss 2.009543\n",
      "iteration 1300 / 1500: loss 2.031707\n",
      "iteration 1400 / 1500: loss 1.989352\n",
      "That took 17.864570s\n",
      "iteration 0 / 1500: loss 475.067647\n",
      "iteration 100 / 1500: loss 66.558495\n",
      "iteration 200 / 1500: loss 10.931024\n",
      "iteration 300 / 1500: loss 3.354186\n",
      "iteration 400 / 1500: loss 2.265040\n",
      "iteration 500 / 1500: loss 2.005138\n",
      "iteration 600 / 1500: loss 2.054517\n",
      "iteration 700 / 1500: loss 2.027933\n",
      "iteration 800 / 1500: loss 2.025967\n",
      "iteration 900 / 1500: loss 2.013160\n",
      "iteration 1000 / 1500: loss 2.006388\n",
      "iteration 1100 / 1500: loss 2.050035\n",
      "iteration 1200 / 1500: loss 2.071428\n",
      "iteration 1300 / 1500: loss 2.037334\n",
      "iteration 1400 / 1500: loss 2.073896\n",
      "That took 17.939062s\n",
      "iteration 0 / 1500: loss 520.571620\n",
      "iteration 100 / 1500: loss 61.069540\n",
      "iteration 200 / 1500: loss 8.807892\n",
      "iteration 300 / 1500: loss 2.844098\n",
      "iteration 400 / 1500: loss 2.174727\n",
      "iteration 500 / 1500: loss 1.980213\n",
      "iteration 600 / 1500: loss 2.008341\n",
      "iteration 700 / 1500: loss 2.038190\n",
      "iteration 800 / 1500: loss 1.994422\n",
      "iteration 900 / 1500: loss 1.940298\n",
      "iteration 1000 / 1500: loss 2.020637\n",
      "iteration 1100 / 1500: loss 2.028111\n",
      "iteration 1200 / 1500: loss 2.103480\n",
      "iteration 1300 / 1500: loss 2.040240\n",
      "iteration 1400 / 1500: loss 2.103421\n",
      "That took 17.925374s\n",
      "iteration 0 / 1500: loss 561.269788\n",
      "iteration 100 / 1500: loss 55.344126\n",
      "iteration 200 / 1500: loss 7.123214\n",
      "iteration 300 / 1500: loss 2.530343\n",
      "iteration 400 / 1500: loss 2.081920\n",
      "iteration 500 / 1500: loss 2.092902\n",
      "iteration 600 / 1500: loss 2.076071\n",
      "iteration 700 / 1500: loss 2.067474\n",
      "iteration 800 / 1500: loss 2.030588\n",
      "iteration 900 / 1500: loss 2.031292\n",
      "iteration 1000 / 1500: loss 2.068903\n",
      "iteration 1100 / 1500: loss 2.018728\n",
      "iteration 1200 / 1500: loss 2.117475\n",
      "iteration 1300 / 1500: loss 2.067686\n",
      "iteration 1400 / 1500: loss 2.036115\n",
      "That took 18.760750s\n",
      "iteration 0 / 1500: loss 604.518492\n",
      "iteration 100 / 1500: loss 49.849796\n",
      "iteration 200 / 1500: loss 5.941467\n",
      "iteration 300 / 1500: loss 2.346355\n",
      "iteration 400 / 1500: loss 2.107235\n",
      "iteration 500 / 1500: loss 2.053629\n",
      "iteration 600 / 1500: loss 2.042001\n",
      "iteration 700 / 1500: loss 2.003406\n",
      "iteration 800 / 1500: loss 2.083916\n",
      "iteration 900 / 1500: loss 2.109478\n",
      "iteration 1000 / 1500: loss 2.170844\n",
      "iteration 1100 / 1500: loss 2.056728\n",
      "iteration 1200 / 1500: loss 2.026441\n",
      "iteration 1300 / 1500: loss 2.100258\n",
      "iteration 1400 / 1500: loss 2.016337\n",
      "That took 18.213418s\n",
      "iteration 0 / 1500: loss 642.658847\n",
      "iteration 100 / 1500: loss 44.574346\n",
      "iteration 200 / 1500: loss 4.963399\n",
      "iteration 300 / 1500: loss 2.274135\n",
      "iteration 400 / 1500: loss 2.080134\n",
      "iteration 500 / 1500: loss 2.112160\n",
      "iteration 600 / 1500: loss 2.055182\n",
      "iteration 700 / 1500: loss 1.988508\n",
      "iteration 800 / 1500: loss 2.105044\n",
      "iteration 900 / 1500: loss 2.114147\n",
      "iteration 1000 / 1500: loss 2.044497\n",
      "iteration 1100 / 1500: loss 2.112079\n",
      "iteration 1200 / 1500: loss 2.082078\n",
      "iteration 1300 / 1500: loss 2.066465\n",
      "iteration 1400 / 1500: loss 2.076195\n",
      "That took 18.342960s\n",
      "iteration 0 / 1500: loss 693.478061\n",
      "iteration 100 / 1500: loss 40.266371\n",
      "iteration 200 / 1500: loss 4.250252\n",
      "iteration 300 / 1500: loss 2.228098\n",
      "iteration 400 / 1500: loss 2.099113\n",
      "iteration 500 / 1500: loss 2.115802\n",
      "iteration 600 / 1500: loss 2.055572\n",
      "iteration 700 / 1500: loss 2.065521\n",
      "iteration 800 / 1500: loss 2.025337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 2.068188\n",
      "iteration 1000 / 1500: loss 2.105841\n",
      "iteration 1100 / 1500: loss 2.066101\n",
      "iteration 1200 / 1500: loss 2.088961\n",
      "iteration 1300 / 1500: loss 2.051764\n",
      "iteration 1400 / 1500: loss 2.046973\n",
      "That took 18.013962s\n",
      "iteration 0 / 1500: loss 740.693444\n",
      "iteration 100 / 1500: loss 36.098473\n",
      "iteration 200 / 1500: loss 3.682207\n",
      "iteration 300 / 1500: loss 2.119058\n",
      "iteration 400 / 1500: loss 2.082859\n",
      "iteration 500 / 1500: loss 2.031579\n",
      "iteration 600 / 1500: loss 2.100320\n",
      "iteration 700 / 1500: loss 2.084345\n",
      "iteration 800 / 1500: loss 2.091012\n",
      "iteration 900 / 1500: loss 2.075582\n",
      "iteration 1000 / 1500: loss 2.115357\n",
      "iteration 1100 / 1500: loss 2.133858\n",
      "iteration 1200 / 1500: loss 2.087499\n",
      "iteration 1300 / 1500: loss 2.046360\n",
      "iteration 1400 / 1500: loss 2.095285\n",
      "That took 18.057697s\n",
      "iteration 0 / 1500: loss 777.583531\n",
      "iteration 100 / 1500: loss 31.973730\n",
      "iteration 200 / 1500: loss 3.283654\n",
      "iteration 300 / 1500: loss 2.132726\n",
      "iteration 400 / 1500: loss 2.109461\n",
      "iteration 500 / 1500: loss 2.089691\n",
      "iteration 600 / 1500: loss 2.099152\n",
      "iteration 700 / 1500: loss 2.002633\n",
      "iteration 800 / 1500: loss 2.047215\n",
      "iteration 900 / 1500: loss 2.104201\n",
      "iteration 1000 / 1500: loss 2.066442\n",
      "iteration 1100 / 1500: loss 2.068655\n",
      "iteration 1200 / 1500: loss 2.106320\n",
      "iteration 1300 / 1500: loss 2.098529\n",
      "iteration 1400 / 1500: loss 2.098423\n",
      "That took 18.124791s\n",
      "iteration 0 / 1500: loss 390.283973\n",
      "iteration 100 / 1500: loss 62.295852\n",
      "iteration 200 / 1500: loss 11.502008\n",
      "iteration 300 / 1500: loss 3.551338\n",
      "iteration 400 / 1500: loss 2.242623\n",
      "iteration 500 / 1500: loss 2.168373\n",
      "iteration 600 / 1500: loss 2.036463\n",
      "iteration 700 / 1500: loss 2.071223\n",
      "iteration 800 / 1500: loss 2.034556\n",
      "iteration 900 / 1500: loss 1.994475\n",
      "iteration 1000 / 1500: loss 1.983029\n",
      "iteration 1100 / 1500: loss 2.039081\n",
      "iteration 1200 / 1500: loss 1.947476\n",
      "iteration 1300 / 1500: loss 2.019017\n",
      "iteration 1400 / 1500: loss 1.988879\n",
      "That took 18.057378s\n",
      "iteration 0 / 1500: loss 432.461862\n",
      "iteration 100 / 1500: loss 56.813191\n",
      "iteration 200 / 1500: loss 9.015870\n",
      "iteration 300 / 1500: loss 2.907008\n",
      "iteration 400 / 1500: loss 2.204460\n",
      "iteration 500 / 1500: loss 2.048337\n",
      "iteration 600 / 1500: loss 2.072996\n",
      "iteration 700 / 1500: loss 2.071638\n",
      "iteration 800 / 1500: loss 2.073470\n",
      "iteration 900 / 1500: loss 2.025317\n",
      "iteration 1000 / 1500: loss 2.055511\n",
      "iteration 1100 / 1500: loss 2.020734\n",
      "iteration 1200 / 1500: loss 2.018924\n",
      "iteration 1300 / 1500: loss 2.016025\n",
      "iteration 1400 / 1500: loss 2.008859\n",
      "That took 18.011386s\n",
      "iteration 0 / 1500: loss 478.336680\n",
      "iteration 100 / 1500: loss 51.378080\n",
      "iteration 200 / 1500: loss 7.159737\n",
      "iteration 300 / 1500: loss 2.540827\n",
      "iteration 400 / 1500: loss 2.144403\n",
      "iteration 500 / 1500: loss 2.042584\n",
      "iteration 600 / 1500: loss 2.092959\n",
      "iteration 700 / 1500: loss 2.131733\n",
      "iteration 800 / 1500: loss 2.071598\n",
      "iteration 900 / 1500: loss 2.000497\n",
      "iteration 1000 / 1500: loss 2.046842\n",
      "iteration 1100 / 1500: loss 2.038692\n",
      "iteration 1200 / 1500: loss 1.999598\n",
      "iteration 1300 / 1500: loss 1.996582\n",
      "iteration 1400 / 1500: loss 2.018128\n",
      "That took 18.010938s\n",
      "iteration 0 / 1500: loss 517.679270\n",
      "iteration 100 / 1500: loss 45.629384\n",
      "iteration 200 / 1500: loss 5.710140\n",
      "iteration 300 / 1500: loss 2.389965\n",
      "iteration 400 / 1500: loss 2.058041\n",
      "iteration 500 / 1500: loss 2.069096\n",
      "iteration 600 / 1500: loss 2.017248\n",
      "iteration 700 / 1500: loss 2.082084\n",
      "iteration 800 / 1500: loss 2.077376\n",
      "iteration 900 / 1500: loss 2.078431\n",
      "iteration 1000 / 1500: loss 2.061693\n",
      "iteration 1100 / 1500: loss 1.979014\n",
      "iteration 1200 / 1500: loss 2.028097\n",
      "iteration 1300 / 1500: loss 2.081866\n",
      "iteration 1400 / 1500: loss 2.059676\n",
      "That took 18.008869s\n",
      "iteration 0 / 1500: loss 553.394074\n",
      "iteration 100 / 1500: loss 39.839077\n",
      "iteration 200 / 1500: loss 4.673377\n",
      "iteration 300 / 1500: loss 2.175161\n",
      "iteration 400 / 1500: loss 2.084440\n",
      "iteration 500 / 1500: loss 2.150499\n",
      "iteration 600 / 1500: loss 2.108018\n",
      "iteration 700 / 1500: loss 2.101174\n",
      "iteration 800 / 1500: loss 2.095811\n",
      "iteration 900 / 1500: loss 2.086276\n",
      "iteration 1000 / 1500: loss 2.052320\n",
      "iteration 1100 / 1500: loss 2.038187\n",
      "iteration 1200 / 1500: loss 2.001976\n",
      "iteration 1300 / 1500: loss 2.020254\n",
      "iteration 1400 / 1500: loss 2.033033\n",
      "That took 18.400617s\n",
      "iteration 0 / 1500: loss 594.757898\n",
      "iteration 100 / 1500: loss 35.142012\n",
      "iteration 200 / 1500: loss 3.885571\n",
      "iteration 300 / 1500: loss 2.183359\n",
      "iteration 400 / 1500: loss 2.069678\n",
      "iteration 500 / 1500: loss 2.086464\n",
      "iteration 600 / 1500: loss 2.079141\n",
      "iteration 700 / 1500: loss 2.016839\n",
      "iteration 800 / 1500: loss 2.094590\n",
      "iteration 900 / 1500: loss 2.086544\n",
      "iteration 1000 / 1500: loss 2.091855\n",
      "iteration 1100 / 1500: loss 2.048967\n",
      "iteration 1200 / 1500: loss 2.055797\n",
      "iteration 1300 / 1500: loss 2.084346\n",
      "iteration 1400 / 1500: loss 2.090391\n",
      "That took 18.203955s\n",
      "iteration 0 / 1500: loss 649.561158\n",
      "iteration 100 / 1500: loss 31.476671\n",
      "iteration 200 / 1500: loss 3.377648\n",
      "iteration 300 / 1500: loss 2.191798\n",
      "iteration 400 / 1500: loss 2.069030\n",
      "iteration 500 / 1500: loss 2.071342\n",
      "iteration 600 / 1500: loss 2.140271\n",
      "iteration 700 / 1500: loss 2.101535\n",
      "iteration 800 / 1500: loss 2.059021\n",
      "iteration 900 / 1500: loss 2.105264\n",
      "iteration 1000 / 1500: loss 2.122363\n",
      "iteration 1100 / 1500: loss 2.073664\n",
      "iteration 1200 / 1500: loss 2.062528\n",
      "iteration 1300 / 1500: loss 2.084673\n",
      "iteration 1400 / 1500: loss 2.084521\n",
      "That took 18.297557s\n",
      "iteration 0 / 1500: loss 691.896912\n",
      "iteration 100 / 1500: loss 27.510992\n",
      "iteration 200 / 1500: loss 3.004835\n",
      "iteration 300 / 1500: loss 2.128676\n",
      "iteration 400 / 1500: loss 2.112062\n",
      "iteration 500 / 1500: loss 2.094684\n",
      "iteration 600 / 1500: loss 2.045942\n",
      "iteration 700 / 1500: loss 2.077003\n",
      "iteration 800 / 1500: loss 2.098139\n",
      "iteration 900 / 1500: loss 2.061004\n",
      "iteration 1000 / 1500: loss 2.039673\n",
      "iteration 1100 / 1500: loss 2.070403\n",
      "iteration 1200 / 1500: loss 2.091213\n",
      "iteration 1300 / 1500: loss 2.101501\n",
      "iteration 1400 / 1500: loss 2.074616\n",
      "That took 18.066995s\n",
      "iteration 0 / 1500: loss 736.673483\n",
      "iteration 100 / 1500: loss 24.210683\n",
      "iteration 200 / 1500: loss 2.773449\n",
      "iteration 300 / 1500: loss 2.100971\n",
      "iteration 400 / 1500: loss 2.071004\n",
      "iteration 500 / 1500: loss 2.065111\n",
      "iteration 600 / 1500: loss 2.096136\n",
      "iteration 700 / 1500: loss 2.065188\n",
      "iteration 800 / 1500: loss 2.071655\n",
      "iteration 900 / 1500: loss 2.118656\n",
      "iteration 1000 / 1500: loss 2.029485\n",
      "iteration 1100 / 1500: loss 2.063171\n",
      "iteration 1200 / 1500: loss 2.198010\n",
      "iteration 1300 / 1500: loss 2.085795\n",
      "iteration 1400 / 1500: loss 2.065030\n",
      "That took 18.126648s\n",
      "iteration 0 / 1500: loss 779.334395\n",
      "iteration 100 / 1500: loss 21.022687\n",
      "iteration 200 / 1500: loss 2.557056\n",
      "iteration 300 / 1500: loss 2.098645\n",
      "iteration 400 / 1500: loss 2.081810\n",
      "iteration 500 / 1500: loss 2.078226\n",
      "iteration 600 / 1500: loss 2.109034\n",
      "iteration 700 / 1500: loss 2.116301\n",
      "iteration 800 / 1500: loss 2.069735\n",
      "iteration 900 / 1500: loss 2.036691\n",
      "iteration 1000 / 1500: loss 2.050064\n",
      "iteration 1100 / 1500: loss 2.140849\n",
      "iteration 1200 / 1500: loss 2.123688\n",
      "iteration 1300 / 1500: loss 2.137283\n",
      "iteration 1400 / 1500: loss 2.113110\n",
      "That took 18.136632s\n",
      "iteration 0 / 1500: loss 384.910000\n",
      "iteration 100 / 1500: loss 49.672135\n",
      "iteration 200 / 1500: loss 8.044596\n",
      "iteration 300 / 1500: loss 2.787487\n",
      "iteration 400 / 1500: loss 2.138329\n",
      "iteration 500 / 1500: loss 2.029575\n",
      "iteration 600 / 1500: loss 2.046342\n",
      "iteration 700 / 1500: loss 2.081007\n",
      "iteration 800 / 1500: loss 2.062965\n",
      "iteration 900 / 1500: loss 1.981311\n",
      "iteration 1000 / 1500: loss 2.018865\n",
      "iteration 1100 / 1500: loss 1.994042\n",
      "iteration 1200 / 1500: loss 2.117973\n",
      "iteration 1300 / 1500: loss 2.018871\n",
      "iteration 1400 / 1500: loss 2.013352\n",
      "That took 18.247171s\n",
      "iteration 0 / 1500: loss 433.566305\n",
      "iteration 100 / 1500: loss 44.847700\n",
      "iteration 200 / 1500: loss 6.309076\n",
      "iteration 300 / 1500: loss 2.436683\n",
      "iteration 400 / 1500: loss 2.066232\n",
      "iteration 500 / 1500: loss 2.019196\n",
      "iteration 600 / 1500: loss 2.016149\n",
      "iteration 700 / 1500: loss 2.082902\n",
      "iteration 800 / 1500: loss 2.058827\n",
      "iteration 900 / 1500: loss 2.002664\n",
      "iteration 1000 / 1500: loss 2.014543\n",
      "iteration 1100 / 1500: loss 2.075174\n",
      "iteration 1200 / 1500: loss 1.925411\n",
      "iteration 1300 / 1500: loss 1.991988\n",
      "iteration 1400 / 1500: loss 2.046223\n",
      "That took 19.136300s\n",
      "iteration 0 / 1500: loss 475.465297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 39.352417\n",
      "iteration 200 / 1500: loss 4.980557\n",
      "iteration 300 / 1500: loss 2.317362\n",
      "iteration 400 / 1500: loss 2.047441\n",
      "iteration 500 / 1500: loss 2.058840\n",
      "iteration 600 / 1500: loss 2.085186\n",
      "iteration 700 / 1500: loss 2.128061\n",
      "iteration 800 / 1500: loss 2.057478\n",
      "iteration 900 / 1500: loss 2.039407\n",
      "iteration 1000 / 1500: loss 2.065891\n",
      "iteration 1100 / 1500: loss 2.023885\n",
      "iteration 1200 / 1500: loss 2.044405\n",
      "iteration 1300 / 1500: loss 2.036888\n",
      "iteration 1400 / 1500: loss 2.048355\n",
      "That took 18.604017s\n",
      "iteration 0 / 1500: loss 516.914311\n",
      "iteration 100 / 1500: loss 34.202641\n",
      "iteration 200 / 1500: loss 4.065895\n",
      "iteration 300 / 1500: loss 2.194267\n",
      "iteration 400 / 1500: loss 2.076590\n",
      "iteration 500 / 1500: loss 2.055723\n",
      "iteration 600 / 1500: loss 2.068726\n",
      "iteration 700 / 1500: loss 1.972758\n",
      "iteration 800 / 1500: loss 2.039454\n",
      "iteration 900 / 1500: loss 2.013875\n",
      "iteration 1000 / 1500: loss 2.087644\n",
      "iteration 1100 / 1500: loss 2.017239\n",
      "iteration 1200 / 1500: loss 2.046807\n",
      "iteration 1300 / 1500: loss 2.017068\n",
      "iteration 1400 / 1500: loss 2.064986\n",
      "That took 18.276340s\n",
      "iteration 0 / 1500: loss 567.729128\n",
      "iteration 100 / 1500: loss 30.102637\n",
      "iteration 200 / 1500: loss 3.397497\n",
      "iteration 300 / 1500: loss 2.196138\n",
      "iteration 400 / 1500: loss 2.049218\n",
      "iteration 500 / 1500: loss 1.998934\n",
      "iteration 600 / 1500: loss 2.037123\n",
      "iteration 700 / 1500: loss 2.062008\n",
      "iteration 800 / 1500: loss 2.056260\n",
      "iteration 900 / 1500: loss 2.025991\n",
      "iteration 1000 / 1500: loss 2.055998\n",
      "iteration 1100 / 1500: loss 2.041086\n",
      "iteration 1200 / 1500: loss 2.064961\n",
      "iteration 1300 / 1500: loss 2.020429\n",
      "iteration 1400 / 1500: loss 2.099679\n",
      "That took 18.206122s\n",
      "iteration 0 / 1500: loss 607.195670\n",
      "iteration 100 / 1500: loss 25.844923\n",
      "iteration 200 / 1500: loss 3.085963\n",
      "iteration 300 / 1500: loss 2.087081\n",
      "iteration 400 / 1500: loss 2.087647\n",
      "iteration 500 / 1500: loss 2.109092\n",
      "iteration 600 / 1500: loss 2.058329\n",
      "iteration 700 / 1500: loss 2.008299\n",
      "iteration 800 / 1500: loss 2.100228\n",
      "iteration 900 / 1500: loss 2.087039\n",
      "iteration 1000 / 1500: loss 2.059963\n",
      "iteration 1100 / 1500: loss 2.116256\n",
      "iteration 1200 / 1500: loss 2.044469\n",
      "iteration 1300 / 1500: loss 2.074112\n",
      "iteration 1400 / 1500: loss 2.100719\n",
      "That took 18.131540s\n",
      "iteration 0 / 1500: loss 645.024271\n",
      "iteration 100 / 1500: loss 22.092743\n",
      "iteration 200 / 1500: loss 2.663707\n",
      "iteration 300 / 1500: loss 2.176203\n",
      "iteration 400 / 1500: loss 2.058367\n",
      "iteration 500 / 1500: loss 2.028408\n",
      "iteration 600 / 1500: loss 2.106777\n",
      "iteration 700 / 1500: loss 2.050482\n",
      "iteration 800 / 1500: loss 2.071112\n",
      "iteration 900 / 1500: loss 2.019650\n",
      "iteration 1000 / 1500: loss 2.139834\n",
      "iteration 1100 / 1500: loss 2.071401\n",
      "iteration 1200 / 1500: loss 2.056197\n",
      "iteration 1300 / 1500: loss 2.052091\n",
      "iteration 1400 / 1500: loss 2.079514\n",
      "That took 18.117592s\n",
      "iteration 0 / 1500: loss 679.148845\n",
      "iteration 100 / 1500: loss 18.892858\n",
      "iteration 200 / 1500: loss 2.621331\n",
      "iteration 300 / 1500: loss 2.135019\n",
      "iteration 400 / 1500: loss 2.080504\n",
      "iteration 500 / 1500: loss 2.090371\n",
      "iteration 600 / 1500: loss 2.129572\n",
      "iteration 700 / 1500: loss 2.034589\n",
      "iteration 800 / 1500: loss 2.139372\n",
      "iteration 900 / 1500: loss 2.108338\n",
      "iteration 1000 / 1500: loss 2.052411\n",
      "iteration 1100 / 1500: loss 2.114610\n",
      "iteration 1200 / 1500: loss 2.085423\n",
      "iteration 1300 / 1500: loss 2.116715\n",
      "iteration 1400 / 1500: loss 2.074347\n",
      "That took 18.064145s\n",
      "iteration 0 / 1500: loss 734.508455\n",
      "iteration 100 / 1500: loss 16.427196\n",
      "iteration 200 / 1500: loss 2.360491\n",
      "iteration 300 / 1500: loss 2.119705\n",
      "iteration 400 / 1500: loss 2.038794\n",
      "iteration 500 / 1500: loss 2.081093\n",
      "iteration 600 / 1500: loss 2.119849\n",
      "iteration 700 / 1500: loss 2.098004\n",
      "iteration 800 / 1500: loss 2.076226\n",
      "iteration 900 / 1500: loss 2.088019\n",
      "iteration 1000 / 1500: loss 2.150280\n",
      "iteration 1100 / 1500: loss 2.070253\n",
      "iteration 1200 / 1500: loss 2.105335\n",
      "iteration 1300 / 1500: loss 2.074805\n",
      "iteration 1400 / 1500: loss 2.053568\n",
      "That took 18.249859s\n",
      "iteration 0 / 1500: loss 788.625726\n",
      "iteration 100 / 1500: loss 14.318593\n",
      "iteration 200 / 1500: loss 2.245203\n",
      "iteration 300 / 1500: loss 1.986667\n",
      "iteration 400 / 1500: loss 2.075033\n",
      "iteration 500 / 1500: loss 2.109606\n",
      "iteration 600 / 1500: loss 2.149814\n",
      "iteration 700 / 1500: loss 2.122608\n",
      "iteration 800 / 1500: loss 2.099658\n",
      "iteration 900 / 1500: loss 2.116240\n",
      "iteration 1000 / 1500: loss 2.065693\n",
      "iteration 1100 / 1500: loss 2.065215\n",
      "iteration 1200 / 1500: loss 2.043546\n",
      "iteration 1300 / 1500: loss 2.059315\n",
      "iteration 1400 / 1500: loss 2.162808\n",
      "That took 18.263723s\n",
      "iteration 0 / 1500: loss 389.075105\n",
      "iteration 100 / 1500: loss 40.585383\n",
      "iteration 200 / 1500: loss 5.896405\n",
      "iteration 300 / 1500: loss 2.366870\n",
      "iteration 400 / 1500: loss 2.105681\n",
      "iteration 500 / 1500: loss 2.038558\n",
      "iteration 600 / 1500: loss 2.052188\n",
      "iteration 700 / 1500: loss 2.033141\n",
      "iteration 800 / 1500: loss 2.086960\n",
      "iteration 900 / 1500: loss 2.003668\n",
      "iteration 1000 / 1500: loss 1.994988\n",
      "iteration 1100 / 1500: loss 1.992912\n",
      "iteration 1200 / 1500: loss 2.017076\n",
      "iteration 1300 / 1500: loss 1.956008\n",
      "iteration 1400 / 1500: loss 2.036291\n",
      "That took 18.356296s\n",
      "iteration 0 / 1500: loss 435.529281\n",
      "iteration 100 / 1500: loss 35.387431\n",
      "iteration 200 / 1500: loss 4.565378\n",
      "iteration 300 / 1500: loss 2.253740\n",
      "iteration 400 / 1500: loss 2.110595\n",
      "iteration 500 / 1500: loss 2.029203\n",
      "iteration 600 / 1500: loss 2.033519\n",
      "iteration 700 / 1500: loss 2.041738\n",
      "iteration 800 / 1500: loss 2.089027\n",
      "iteration 900 / 1500: loss 2.062737\n",
      "iteration 1000 / 1500: loss 2.087562\n",
      "iteration 1100 / 1500: loss 2.063519\n",
      "iteration 1200 / 1500: loss 2.143628\n",
      "iteration 1300 / 1500: loss 2.025358\n",
      "iteration 1400 / 1500: loss 2.060657\n",
      "That took 18.110075s\n",
      "iteration 0 / 1500: loss 469.823416\n",
      "iteration 100 / 1500: loss 29.951227\n",
      "iteration 200 / 1500: loss 3.685039\n",
      "iteration 300 / 1500: loss 2.088183\n",
      "iteration 400 / 1500: loss 2.018339\n",
      "iteration 500 / 1500: loss 2.053732\n",
      "iteration 600 / 1500: loss 1.960229\n",
      "iteration 700 / 1500: loss 2.045835\n",
      "iteration 800 / 1500: loss 2.082517\n",
      "iteration 900 / 1500: loss 2.085672\n",
      "iteration 1000 / 1500: loss 2.083989\n",
      "iteration 1100 / 1500: loss 2.063350\n",
      "iteration 1200 / 1500: loss 2.074371\n",
      "iteration 1300 / 1500: loss 2.062733\n",
      "iteration 1400 / 1500: loss 2.022974\n",
      "That took 18.255784s\n",
      "iteration 0 / 1500: loss 516.843576\n",
      "iteration 100 / 1500: loss 25.818021\n",
      "iteration 200 / 1500: loss 3.158549\n",
      "iteration 300 / 1500: loss 2.061591\n",
      "iteration 400 / 1500: loss 2.075350\n",
      "iteration 500 / 1500: loss 2.071872\n",
      "iteration 600 / 1500: loss 1.984226\n",
      "iteration 700 / 1500: loss 2.036844\n",
      "iteration 800 / 1500: loss 1.995274\n",
      "iteration 900 / 1500: loss 2.076670\n",
      "iteration 1000 / 1500: loss 2.064481\n",
      "iteration 1100 / 1500: loss 2.106374\n",
      "iteration 1200 / 1500: loss 2.021931\n",
      "iteration 1300 / 1500: loss 2.082761\n",
      "iteration 1400 / 1500: loss 2.093108\n",
      "That took 18.080370s\n",
      "iteration 0 / 1500: loss 554.753330\n",
      "iteration 100 / 1500: loss 21.842653\n",
      "iteration 200 / 1500: loss 2.728495\n",
      "iteration 300 / 1500: loss 2.142940\n",
      "iteration 400 / 1500: loss 2.098575\n",
      "iteration 500 / 1500: loss 2.060179\n",
      "iteration 600 / 1500: loss 2.063026\n",
      "iteration 700 / 1500: loss 2.062109\n",
      "iteration 800 / 1500: loss 2.078446\n",
      "iteration 900 / 1500: loss 2.104118\n",
      "iteration 1000 / 1500: loss 2.086913\n",
      "iteration 1100 / 1500: loss 2.068307\n",
      "iteration 1200 / 1500: loss 2.056842\n",
      "iteration 1300 / 1500: loss 2.058960\n",
      "iteration 1400 / 1500: loss 2.036126\n",
      "That took 18.476702s\n",
      "iteration 0 / 1500: loss 605.671469\n",
      "iteration 100 / 1500: loss 18.735837\n",
      "iteration 200 / 1500: loss 2.521562\n",
      "iteration 300 / 1500: loss 2.081259\n",
      "iteration 400 / 1500: loss 2.044603\n",
      "iteration 500 / 1500: loss 2.091552\n",
      "iteration 600 / 1500: loss 1.977276\n",
      "iteration 700 / 1500: loss 2.064301\n",
      "iteration 800 / 1500: loss 2.099509\n",
      "iteration 900 / 1500: loss 2.028726\n",
      "iteration 1000 / 1500: loss 2.019461\n",
      "iteration 1100 / 1500: loss 2.074635\n",
      "iteration 1200 / 1500: loss 2.084155\n",
      "iteration 1300 / 1500: loss 2.046921\n",
      "iteration 1400 / 1500: loss 2.056325\n",
      "That took 18.164472s\n",
      "iteration 0 / 1500: loss 646.475644\n",
      "iteration 100 / 1500: loss 15.807742\n",
      "iteration 200 / 1500: loss 2.430254\n",
      "iteration 300 / 1500: loss 2.028677\n",
      "iteration 400 / 1500: loss 2.084179\n",
      "iteration 500 / 1500: loss 2.014956\n",
      "iteration 600 / 1500: loss 2.066208\n",
      "iteration 700 / 1500: loss 2.047849\n",
      "iteration 800 / 1500: loss 2.102547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 2.117952\n",
      "iteration 1000 / 1500: loss 2.016959\n",
      "iteration 1100 / 1500: loss 2.066049\n",
      "iteration 1200 / 1500: loss 2.089392\n",
      "iteration 1300 / 1500: loss 2.085707\n",
      "iteration 1400 / 1500: loss 2.086533\n",
      "That took 18.037640s\n",
      "iteration 0 / 1500: loss 693.340641\n",
      "iteration 100 / 1500: loss 13.476382\n",
      "iteration 200 / 1500: loss 2.296164\n",
      "iteration 300 / 1500: loss 2.105364\n",
      "iteration 400 / 1500: loss 2.075054\n",
      "iteration 500 / 1500: loss 2.064733\n",
      "iteration 600 / 1500: loss 2.075871\n",
      "iteration 700 / 1500: loss 2.130510\n",
      "iteration 800 / 1500: loss 2.075296\n",
      "iteration 900 / 1500: loss 2.055450\n",
      "iteration 1000 / 1500: loss 2.107999\n",
      "iteration 1100 / 1500: loss 2.140820\n",
      "iteration 1200 / 1500: loss 2.080864\n",
      "iteration 1300 / 1500: loss 2.089211\n",
      "iteration 1400 / 1500: loss 2.089307\n",
      "That took 17.981228s\n",
      "iteration 0 / 1500: loss 734.309732\n",
      "iteration 100 / 1500: loss 11.464299\n",
      "iteration 200 / 1500: loss 2.230642\n",
      "iteration 300 / 1500: loss 2.134540\n",
      "iteration 400 / 1500: loss 2.113691\n",
      "iteration 500 / 1500: loss 2.092636\n",
      "iteration 600 / 1500: loss 2.114666\n",
      "iteration 700 / 1500: loss 1.960066\n",
      "iteration 800 / 1500: loss 2.069706\n",
      "iteration 900 / 1500: loss 2.107467\n",
      "iteration 1000 / 1500: loss 2.068599\n",
      "iteration 1100 / 1500: loss 2.058983\n",
      "iteration 1200 / 1500: loss 2.143517\n",
      "iteration 1300 / 1500: loss 2.058669\n",
      "iteration 1400 / 1500: loss 2.125400\n",
      "That took 19.182486s\n",
      "iteration 0 / 1500: loss 778.086308\n",
      "iteration 100 / 1500: loss 9.776942\n",
      "iteration 200 / 1500: loss 2.149327\n",
      "iteration 300 / 1500: loss 2.108130\n",
      "iteration 400 / 1500: loss 2.066241\n",
      "iteration 500 / 1500: loss 2.061855\n",
      "iteration 600 / 1500: loss 2.061391\n",
      "iteration 700 / 1500: loss 2.077707\n",
      "iteration 800 / 1500: loss 2.099600\n",
      "iteration 900 / 1500: loss 2.105827\n",
      "iteration 1000 / 1500: loss 2.140824\n",
      "iteration 1100 / 1500: loss 2.101430\n",
      "iteration 1200 / 1500: loss 2.060425\n",
      "iteration 1300 / 1500: loss 2.093674\n",
      "iteration 1400 / 1500: loss 2.066046\n",
      "That took 17.929519s\n",
      "iteration 0 / 1500: loss 382.175070\n",
      "iteration 100 / 1500: loss 32.144436\n",
      "iteration 200 / 1500: loss 4.455796\n",
      "iteration 300 / 1500: loss 2.283812\n",
      "iteration 400 / 1500: loss 2.079266\n",
      "iteration 500 / 1500: loss 2.079964\n",
      "iteration 600 / 1500: loss 1.968329\n",
      "iteration 700 / 1500: loss 2.032930\n",
      "iteration 800 / 1500: loss 2.017573\n",
      "iteration 900 / 1500: loss 2.070064\n",
      "iteration 1000 / 1500: loss 2.078352\n",
      "iteration 1100 / 1500: loss 1.940811\n",
      "iteration 1200 / 1500: loss 1.998211\n",
      "iteration 1300 / 1500: loss 2.087777\n",
      "iteration 1400 / 1500: loss 2.049795\n",
      "That took 18.089646s\n",
      "iteration 0 / 1500: loss 433.254270\n",
      "iteration 100 / 1500: loss 27.964038\n",
      "iteration 200 / 1500: loss 3.591502\n",
      "iteration 300 / 1500: loss 2.092411\n",
      "iteration 400 / 1500: loss 2.036320\n",
      "iteration 500 / 1500: loss 2.060961\n",
      "iteration 600 / 1500: loss 2.077325\n",
      "iteration 700 / 1500: loss 2.065959\n",
      "iteration 800 / 1500: loss 2.020725\n",
      "iteration 900 / 1500: loss 2.040052\n",
      "iteration 1000 / 1500: loss 2.053507\n",
      "iteration 1100 / 1500: loss 2.013884\n",
      "iteration 1200 / 1500: loss 2.023872\n",
      "iteration 1300 / 1500: loss 2.104131\n",
      "iteration 1400 / 1500: loss 2.020894\n",
      "That took 18.016771s\n",
      "iteration 0 / 1500: loss 478.609762\n",
      "iteration 100 / 1500: loss 23.788262\n",
      "iteration 200 / 1500: loss 3.011299\n",
      "iteration 300 / 1500: loss 2.118776\n",
      "iteration 400 / 1500: loss 2.057879\n",
      "iteration 500 / 1500: loss 1.993277\n",
      "iteration 600 / 1500: loss 2.035854\n",
      "iteration 700 / 1500: loss 2.015371\n",
      "iteration 800 / 1500: loss 2.022850\n",
      "iteration 900 / 1500: loss 2.057298\n",
      "iteration 1000 / 1500: loss 2.077729\n",
      "iteration 1100 / 1500: loss 2.059944\n",
      "iteration 1200 / 1500: loss 2.042494\n",
      "iteration 1300 / 1500: loss 2.054391\n",
      "iteration 1400 / 1500: loss 2.027392\n",
      "That took 18.097489s\n",
      "iteration 0 / 1500: loss 519.444709\n",
      "iteration 100 / 1500: loss 19.678405\n",
      "iteration 200 / 1500: loss 2.716762\n",
      "iteration 300 / 1500: loss 2.099927\n",
      "iteration 400 / 1500: loss 2.002203\n",
      "iteration 500 / 1500: loss 2.125395\n",
      "iteration 600 / 1500: loss 2.086783\n",
      "iteration 700 / 1500: loss 2.033774\n",
      "iteration 800 / 1500: loss 2.099714\n",
      "iteration 900 / 1500: loss 2.016062\n",
      "iteration 1000 / 1500: loss 2.037005\n",
      "iteration 1100 / 1500: loss 2.069365\n",
      "iteration 1200 / 1500: loss 2.118002\n",
      "iteration 1300 / 1500: loss 2.037172\n",
      "iteration 1400 / 1500: loss 2.056274\n",
      "That took 17.924057s\n",
      "iteration 0 / 1500: loss 555.950346\n",
      "iteration 100 / 1500: loss 16.275949\n",
      "iteration 200 / 1500: loss 2.412582\n",
      "iteration 300 / 1500: loss 2.095065\n",
      "iteration 400 / 1500: loss 2.101793\n",
      "iteration 500 / 1500: loss 2.061186\n",
      "iteration 600 / 1500: loss 2.021283\n",
      "iteration 700 / 1500: loss 2.088774\n",
      "iteration 800 / 1500: loss 2.013759\n",
      "iteration 900 / 1500: loss 2.137682\n",
      "iteration 1000 / 1500: loss 2.016877\n",
      "iteration 1100 / 1500: loss 2.011803\n",
      "iteration 1200 / 1500: loss 2.086664\n",
      "iteration 1300 / 1500: loss 2.019279\n",
      "iteration 1400 / 1500: loss 2.111724\n",
      "That took 18.057109s\n",
      "iteration 0 / 1500: loss 609.026863\n",
      "iteration 100 / 1500: loss 13.848378\n",
      "iteration 200 / 1500: loss 2.295449\n",
      "iteration 300 / 1500: loss 2.094491\n",
      "iteration 400 / 1500: loss 2.093358\n",
      "iteration 500 / 1500: loss 2.073080\n",
      "iteration 600 / 1500: loss 2.033523\n",
      "iteration 700 / 1500: loss 2.056133\n",
      "iteration 800 / 1500: loss 2.068869\n",
      "iteration 900 / 1500: loss 2.028152\n",
      "iteration 1000 / 1500: loss 2.005453\n",
      "iteration 1100 / 1500: loss 2.070420\n",
      "iteration 1200 / 1500: loss 2.025435\n",
      "iteration 1300 / 1500: loss 2.043740\n",
      "iteration 1400 / 1500: loss 2.070026\n",
      "That took 18.200163s\n",
      "iteration 0 / 1500: loss 642.085771\n",
      "iteration 100 / 1500: loss 11.403803\n",
      "iteration 200 / 1500: loss 2.193784\n",
      "iteration 300 / 1500: loss 2.084594\n",
      "iteration 400 / 1500: loss 2.069073\n",
      "iteration 500 / 1500: loss 2.092485\n",
      "iteration 600 / 1500: loss 2.062130\n",
      "iteration 700 / 1500: loss 2.118996\n",
      "iteration 800 / 1500: loss 2.074578\n",
      "iteration 900 / 1500: loss 2.085136\n",
      "iteration 1000 / 1500: loss 2.028218\n",
      "iteration 1100 / 1500: loss 2.061884\n",
      "iteration 1200 / 1500: loss 2.013123\n",
      "iteration 1300 / 1500: loss 2.010614\n",
      "iteration 1400 / 1500: loss 2.108559\n",
      "That took 18.170993s\n",
      "iteration 0 / 1500: loss 682.297373\n",
      "iteration 100 / 1500: loss 9.530140\n",
      "iteration 200 / 1500: loss 2.150005\n",
      "iteration 300 / 1500: loss 2.031659\n",
      "iteration 400 / 1500: loss 2.098222\n",
      "iteration 500 / 1500: loss 2.029566\n",
      "iteration 600 / 1500: loss 2.142707\n",
      "iteration 700 / 1500: loss 2.105137\n",
      "iteration 800 / 1500: loss 2.077928\n",
      "iteration 900 / 1500: loss 2.036621\n",
      "iteration 1000 / 1500: loss 2.093936\n",
      "iteration 1100 / 1500: loss 2.057241\n",
      "iteration 1200 / 1500: loss 2.066090\n",
      "iteration 1300 / 1500: loss 2.014240\n",
      "iteration 1400 / 1500: loss 2.070742\n",
      "That took 18.256458s\n",
      "iteration 0 / 1500: loss 733.253946\n",
      "iteration 100 / 1500: loss 8.183606\n",
      "iteration 200 / 1500: loss 2.043942\n",
      "iteration 300 / 1500: loss 2.064413\n",
      "iteration 400 / 1500: loss 2.089185\n",
      "iteration 500 / 1500: loss 2.154862\n",
      "iteration 600 / 1500: loss 2.080880\n",
      "iteration 700 / 1500: loss 2.109290\n",
      "iteration 800 / 1500: loss 2.116239\n",
      "iteration 900 / 1500: loss 2.094475\n",
      "iteration 1000 / 1500: loss 2.161810\n",
      "iteration 1100 / 1500: loss 2.071896\n",
      "iteration 1200 / 1500: loss 2.133575\n",
      "iteration 1300 / 1500: loss 2.021291\n",
      "iteration 1400 / 1500: loss 2.061169\n",
      "That took 18.071119s\n",
      "iteration 0 / 1500: loss 770.443325\n",
      "iteration 100 / 1500: loss 6.927947\n",
      "iteration 200 / 1500: loss 2.090089\n",
      "iteration 300 / 1500: loss 2.050672\n",
      "iteration 400 / 1500: loss 2.092092\n",
      "iteration 500 / 1500: loss 2.094806\n",
      "iteration 600 / 1500: loss 2.113600\n",
      "iteration 700 / 1500: loss 2.122080\n",
      "iteration 800 / 1500: loss 2.050056\n",
      "iteration 900 / 1500: loss 2.089973\n",
      "iteration 1000 / 1500: loss 2.148510\n",
      "iteration 1100 / 1500: loss 2.089711\n",
      "iteration 1200 / 1500: loss 2.069639\n",
      "iteration 1300 / 1500: loss 2.127442\n",
      "iteration 1400 / 1500: loss 2.081937\n",
      "That took 18.063687s\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.350408 val accuracy: 0.358000\n",
      "lr 1.000000e-07 reg 2.777778e+04 train accuracy: 0.346388 val accuracy: 0.353000\n",
      "lr 1.000000e-07 reg 3.055556e+04 train accuracy: 0.342898 val accuracy: 0.359000\n",
      "lr 1.000000e-07 reg 3.333333e+04 train accuracy: 0.339000 val accuracy: 0.359000\n",
      "lr 1.000000e-07 reg 3.611111e+04 train accuracy: 0.339347 val accuracy: 0.353000\n",
      "lr 1.000000e-07 reg 3.888889e+04 train accuracy: 0.331469 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 4.166667e+04 train accuracy: 0.335898 val accuracy: 0.357000\n",
      "lr 1.000000e-07 reg 4.444444e+04 train accuracy: 0.333531 val accuracy: 0.348000\n",
      "lr 1.000000e-07 reg 4.722222e+04 train accuracy: 0.328735 val accuracy: 0.352000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.328612 val accuracy: 0.348000\n",
      "lr 1.444444e-07 reg 2.500000e+04 train accuracy: 0.350551 val accuracy: 0.367000\n",
      "lr 1.444444e-07 reg 2.777778e+04 train accuracy: 0.345204 val accuracy: 0.359000\n",
      "lr 1.444444e-07 reg 3.055556e+04 train accuracy: 0.342163 val accuracy: 0.343000\n",
      "lr 1.444444e-07 reg 3.333333e+04 train accuracy: 0.337449 val accuracy: 0.349000\n",
      "lr 1.444444e-07 reg 3.611111e+04 train accuracy: 0.340714 val accuracy: 0.357000\n",
      "lr 1.444444e-07 reg 3.888889e+04 train accuracy: 0.332959 val accuracy: 0.348000\n",
      "lr 1.444444e-07 reg 4.166667e+04 train accuracy: 0.333204 val accuracy: 0.352000\n",
      "lr 1.444444e-07 reg 4.444444e+04 train accuracy: 0.332673 val accuracy: 0.346000\n",
      "lr 1.444444e-07 reg 4.722222e+04 train accuracy: 0.332551 val accuracy: 0.351000\n",
      "lr 1.444444e-07 reg 5.000000e+04 train accuracy: 0.326653 val accuracy: 0.339000\n",
      "lr 1.888889e-07 reg 2.500000e+04 train accuracy: 0.350000 val accuracy: 0.360000\n",
      "lr 1.888889e-07 reg 2.777778e+04 train accuracy: 0.346143 val accuracy: 0.350000\n",
      "lr 1.888889e-07 reg 3.055556e+04 train accuracy: 0.345653 val accuracy: 0.369000\n",
      "lr 1.888889e-07 reg 3.333333e+04 train accuracy: 0.341878 val accuracy: 0.348000\n",
      "lr 1.888889e-07 reg 3.611111e+04 train accuracy: 0.336122 val accuracy: 0.354000\n",
      "lr 1.888889e-07 reg 3.888889e+04 train accuracy: 0.339878 val accuracy: 0.345000\n",
      "lr 1.888889e-07 reg 4.166667e+04 train accuracy: 0.331673 val accuracy: 0.350000\n",
      "lr 1.888889e-07 reg 4.444444e+04 train accuracy: 0.333531 val accuracy: 0.347000\n",
      "lr 1.888889e-07 reg 4.722222e+04 train accuracy: 0.332245 val accuracy: 0.349000\n",
      "lr 1.888889e-07 reg 5.000000e+04 train accuracy: 0.330163 val accuracy: 0.349000\n",
      "lr 2.333333e-07 reg 2.500000e+04 train accuracy: 0.352816 val accuracy: 0.370000\n",
      "lr 2.333333e-07 reg 2.777778e+04 train accuracy: 0.337837 val accuracy: 0.357000\n",
      "lr 2.333333e-07 reg 3.055556e+04 train accuracy: 0.345959 val accuracy: 0.367000\n",
      "lr 2.333333e-07 reg 3.333333e+04 train accuracy: 0.344245 val accuracy: 0.355000\n",
      "lr 2.333333e-07 reg 3.611111e+04 train accuracy: 0.339592 val accuracy: 0.355000\n",
      "lr 2.333333e-07 reg 3.888889e+04 train accuracy: 0.333041 val accuracy: 0.347000\n",
      "lr 2.333333e-07 reg 4.166667e+04 train accuracy: 0.337224 val accuracy: 0.348000\n",
      "lr 2.333333e-07 reg 4.444444e+04 train accuracy: 0.336531 val accuracy: 0.340000\n",
      "lr 2.333333e-07 reg 4.722222e+04 train accuracy: 0.326837 val accuracy: 0.340000\n",
      "lr 2.333333e-07 reg 5.000000e+04 train accuracy: 0.324306 val accuracy: 0.338000\n",
      "lr 2.777778e-07 reg 2.500000e+04 train accuracy: 0.351286 val accuracy: 0.368000\n",
      "lr 2.777778e-07 reg 2.777778e+04 train accuracy: 0.340939 val accuracy: 0.354000\n",
      "lr 2.777778e-07 reg 3.055556e+04 train accuracy: 0.344306 val accuracy: 0.356000\n",
      "lr 2.777778e-07 reg 3.333333e+04 train accuracy: 0.343673 val accuracy: 0.352000\n",
      "lr 2.777778e-07 reg 3.611111e+04 train accuracy: 0.338449 val accuracy: 0.356000\n",
      "lr 2.777778e-07 reg 3.888889e+04 train accuracy: 0.336551 val accuracy: 0.343000\n",
      "lr 2.777778e-07 reg 4.166667e+04 train accuracy: 0.332082 val accuracy: 0.342000\n",
      "lr 2.777778e-07 reg 4.444444e+04 train accuracy: 0.337633 val accuracy: 0.346000\n",
      "lr 2.777778e-07 reg 4.722222e+04 train accuracy: 0.325694 val accuracy: 0.349000\n",
      "lr 2.777778e-07 reg 5.000000e+04 train accuracy: 0.330857 val accuracy: 0.352000\n",
      "lr 3.222222e-07 reg 2.500000e+04 train accuracy: 0.344408 val accuracy: 0.363000\n",
      "lr 3.222222e-07 reg 2.777778e+04 train accuracy: 0.345531 val accuracy: 0.352000\n",
      "lr 3.222222e-07 reg 3.055556e+04 train accuracy: 0.340367 val accuracy: 0.351000\n",
      "lr 3.222222e-07 reg 3.333333e+04 train accuracy: 0.338735 val accuracy: 0.344000\n",
      "lr 3.222222e-07 reg 3.611111e+04 train accuracy: 0.339571 val accuracy: 0.334000\n",
      "lr 3.222222e-07 reg 3.888889e+04 train accuracy: 0.338224 val accuracy: 0.355000\n",
      "lr 3.222222e-07 reg 4.166667e+04 train accuracy: 0.330510 val accuracy: 0.348000\n",
      "lr 3.222222e-07 reg 4.444444e+04 train accuracy: 0.327306 val accuracy: 0.345000\n",
      "lr 3.222222e-07 reg 4.722222e+04 train accuracy: 0.326837 val accuracy: 0.339000\n",
      "lr 3.222222e-07 reg 5.000000e+04 train accuracy: 0.329429 val accuracy: 0.342000\n",
      "lr 3.666667e-07 reg 2.500000e+04 train accuracy: 0.346898 val accuracy: 0.362000\n",
      "lr 3.666667e-07 reg 2.777778e+04 train accuracy: 0.342735 val accuracy: 0.354000\n",
      "lr 3.666667e-07 reg 3.055556e+04 train accuracy: 0.348490 val accuracy: 0.363000\n",
      "lr 3.666667e-07 reg 3.333333e+04 train accuracy: 0.338510 val accuracy: 0.358000\n",
      "lr 3.666667e-07 reg 3.611111e+04 train accuracy: 0.333878 val accuracy: 0.345000\n",
      "lr 3.666667e-07 reg 3.888889e+04 train accuracy: 0.331265 val accuracy: 0.350000\n",
      "lr 3.666667e-07 reg 4.166667e+04 train accuracy: 0.334551 val accuracy: 0.351000\n",
      "lr 3.666667e-07 reg 4.444444e+04 train accuracy: 0.333224 val accuracy: 0.345000\n",
      "lr 3.666667e-07 reg 4.722222e+04 train accuracy: 0.328755 val accuracy: 0.343000\n",
      "lr 3.666667e-07 reg 5.000000e+04 train accuracy: 0.324347 val accuracy: 0.328000\n",
      "lr 4.111111e-07 reg 2.500000e+04 train accuracy: 0.344694 val accuracy: 0.352000\n",
      "lr 4.111111e-07 reg 2.777778e+04 train accuracy: 0.343633 val accuracy: 0.360000\n",
      "lr 4.111111e-07 reg 3.055556e+04 train accuracy: 0.346776 val accuracy: 0.360000\n",
      "lr 4.111111e-07 reg 3.333333e+04 train accuracy: 0.340816 val accuracy: 0.356000\n",
      "lr 4.111111e-07 reg 3.611111e+04 train accuracy: 0.330122 val accuracy: 0.338000\n",
      "lr 4.111111e-07 reg 3.888889e+04 train accuracy: 0.336020 val accuracy: 0.348000\n",
      "lr 4.111111e-07 reg 4.166667e+04 train accuracy: 0.334531 val accuracy: 0.347000\n",
      "lr 4.111111e-07 reg 4.444444e+04 train accuracy: 0.339531 val accuracy: 0.357000\n",
      "lr 4.111111e-07 reg 4.722222e+04 train accuracy: 0.326857 val accuracy: 0.338000\n",
      "lr 4.111111e-07 reg 5.000000e+04 train accuracy: 0.330898 val accuracy: 0.340000\n",
      "lr 4.555556e-07 reg 2.500000e+04 train accuracy: 0.343204 val accuracy: 0.354000\n",
      "lr 4.555556e-07 reg 2.777778e+04 train accuracy: 0.348327 val accuracy: 0.367000\n",
      "lr 4.555556e-07 reg 3.055556e+04 train accuracy: 0.338857 val accuracy: 0.357000\n",
      "lr 4.555556e-07 reg 3.333333e+04 train accuracy: 0.343918 val accuracy: 0.368000\n",
      "lr 4.555556e-07 reg 3.611111e+04 train accuracy: 0.326837 val accuracy: 0.349000\n",
      "lr 4.555556e-07 reg 3.888889e+04 train accuracy: 0.339306 val accuracy: 0.358000\n",
      "lr 4.555556e-07 reg 4.166667e+04 train accuracy: 0.324469 val accuracy: 0.339000\n",
      "lr 4.555556e-07 reg 4.444444e+04 train accuracy: 0.332612 val accuracy: 0.335000\n",
      "lr 4.555556e-07 reg 4.722222e+04 train accuracy: 0.333408 val accuracy: 0.355000\n",
      "lr 4.555556e-07 reg 5.000000e+04 train accuracy: 0.322816 val accuracy: 0.343000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.346959 val accuracy: 0.356000\n",
      "lr 5.000000e-07 reg 2.777778e+04 train accuracy: 0.336327 val accuracy: 0.348000\n",
      "lr 5.000000e-07 reg 3.055556e+04 train accuracy: 0.338449 val accuracy: 0.351000\n",
      "lr 5.000000e-07 reg 3.333333e+04 train accuracy: 0.326469 val accuracy: 0.343000\n",
      "lr 5.000000e-07 reg 3.611111e+04 train accuracy: 0.335939 val accuracy: 0.358000\n",
      "lr 5.000000e-07 reg 3.888889e+04 train accuracy: 0.328020 val accuracy: 0.339000\n",
      "lr 5.000000e-07 reg 4.166667e+04 train accuracy: 0.325592 val accuracy: 0.342000\n",
      "lr 5.000000e-07 reg 4.444444e+04 train accuracy: 0.335286 val accuracy: 0.351000\n",
      "lr 5.000000e-07 reg 4.722222e+04 train accuracy: 0.329571 val accuracy: 0.348000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.321224 val accuracy: 0.337000\n",
      "best validation accuracy achieved during cross-validation: 0.370000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "range_lern_rate = np.linspace(learning_rates[0],learning_rates[1],10)\n",
    "range_reg = np.linspace(regularization_strengths[0],regularization_strengths[1],10)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "for i in range_lern_rate:\n",
    "    for j in range_reg:\n",
    "        softmax = Softmax()\n",
    "        tic = time.time()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=i, reg=j,num_iters=1500, verbose=True)\n",
    "        toc = time.time()\n",
    "        print('That took %fs' % (toc - tic))\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_accur = np.mean(y_train == y_train_pred)   \n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_accur = np.mean(y_val == y_val_pred)\n",
    "        results[(i, j)]=(train_accur, val_accur)\n",
    "        \n",
    "        if val_accur>best_val:\n",
    "            best_val = val_accur\n",
    "            best_softmax = softmax    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.346000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:Да\n",
    "\n",
    "*Your explanation*:В SVM если добавить точку, score которой будет за пределами margin от правильного класса, она никак не повлияет на loss, однако в SVM даже точка, очень сильно удалённая по score от правильного класса, повлияет на loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
